{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook contains a data cleaning pipeline for preparing a dataset for land cover data overlaid on digital elevation data. \n",
    "\n",
    "The expected outputs are:\n",
    "- Train and test files for image data, with shape (N, 2, H, W) with the first channel for LULC and second channel for DEM\n",
    "- Train and test geopackage files containing the bounding boxes for each image\n",
    "\n",
    "The main steps in this notebook are:\n",
    "- Splitting up the domain into adjacent grid cells and extracting land cover data\n",
    "- Assigning portions of the domain into train and test splits using Sobol sequences\n",
    "- Downloading and merging DEM files for the same domain\n",
    "- Extracting DEM data for each land cover image\n",
    "- Saving these data to disk\n",
    "- Creating an animation of the final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyproj    : 3.7.1\n",
      "cv2       : 4.10.0\n",
      "rasterio  : 1.3.11\n",
      "dotenv    : 0.9.9\n",
      "IPython   : 8.37.0\n",
      "shapely   : 2.1.1\n",
      "tqdm      : 4.66.5\n",
      "elevation : 1.1.3\n",
      "logging   : 0.5.1.2\n",
      "psutil    : 7.0.0\n",
      "numpy     : 1.26.4\n",
      "geopandas : 1.1.0\n",
      "pandas    : 2.2.3\n",
      "scipy     : 1.14.1\n",
      "matplotlib: 3.9.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import elevation\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import rasterio\n",
    "import pickle\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "from pyproj import CRS, Transformer\n",
    "from scipy.stats import mode\n",
    "from shapely.geometry import box\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple\n",
    "from dotenv import load_dotenv\n",
    "from shapely.geometry import mapping\n",
    "from rasterio.mask import mask\n",
    "from rasterio.warp import transform_bounds, calculate_default_transform, reproject, Resampling\n",
    "from rasterio.features import geometry_mask\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "import logging\n",
    "\n",
    "BUCKET_NAME = \"lc-inpaint\"\n",
    "logging.getLogger('boto3').setLevel(logging.WARNING)\n",
    "logging.getLogger('botocore').setLevel(logging.WARNING)\n",
    "logging.getLogger('s3transfer').setLevel(logging.WARNING)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -iv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setting up the config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Setting project data directory to /mnt/m2ssd/data/Dropbox/research/lc-gpt/data\n"
     ]
    }
   ],
   "source": [
    "current_path = Path.cwd()\n",
    "parent_path = current_path.parent\n",
    "\n",
    "@dataclass\n",
    "class DataPrepConfig:\n",
    "    logging_level = logging.INFO\n",
    "\n",
    "    # Geographic bounds (WGS84 coordinates)\n",
    "    bbox_west: float = -119.0\n",
    "    bbox_east: float = -64.0\n",
    "    bbox_south: float = 22.0\n",
    "    bbox_north: float = 49.0\n",
    "\n",
    "    # Sampling parameters\n",
    "    image_size: int = 64  # Size of output images\n",
    "    meters_per_raw_pixel: int = 240  # Meters per pixel in the raw data\n",
    "    downsample_ratio: int = meters_per_raw_pixel // 30  # Downsampling factor\n",
    "    max_fraction_reject_class: float = 0.9  # Maximum fraction of pixels allowed in a reject-eligible class\n",
    "    area_fraction_test: float = 0.05  # Fraction of area to reserve for testing\n",
    "    n_grid_unit: int = 50  # discrete units in each dimension for gridding the domain into discrete units\n",
    "    n_samples_per_cell: int = 1000  # Maximum of samples to extract per cell; only use for testing since it does not randomize over position\n",
    "\n",
    "    # Data paths\n",
    "    data_dir: Path = parent_path / 'data'\n",
    "    dem_dir = Path(data_dir) / 'dem'\n",
    "\n",
    "    nlcd_path: Path = data_dir / 'nlcd_2021_land_cover_l48_20230630.img' # Make sure you have this file before you start\n",
    "    output_path: Path  = data_dir / f'data_size{image_size}_ratio{downsample_ratio}.npz'\n",
    "    output_path_test_gpkg: Path = data_dir / f\"test_{image_size}_ratio{downsample_ratio}.gpkg\"\n",
    "    output_path_train_gpkg: Path = data_dir / f\"train_{image_size}_ratio{downsample_ratio}.gpkg\"\n",
    "    split_save_path: Path = data_dir / f\"split_{image_size}_ratio{downsample_ratio}.gpkg\"\n",
    "    merged_dem_path: Path = dem_dir / f\"merged_conus_dem.tif\"\n",
    "    output_path_animation: Path = data_dir / f\"animation_{image_size}_ratio{downsample_ratio}.gif\"\n",
    "    \n",
    "    # Processing parameters\n",
    "    random_seed: int = 827  # Random seed for reproducibility\n",
    "    recompute_counts: bool = False  # Whether to recompute class counts\n",
    "    show_plots: bool = False  # Whether to display plots\n",
    "    \n",
    "    # CRS parameters\n",
    "    working_crs: str = 'EPSG:4326'  # CRS for geographic operations (WGS84)         \n",
    "    \n",
    "    nlcd_original_classes_for_reject = {11}\n",
    "    nlcd_original_unknown_class = 0\n",
    "\n",
    "    # Parameters for DEM processing\n",
    "    dem_nodata_threshold: float = 0.25\n",
    "    dem_product: str = 'SRTM1' # Choices are 'SRTM1' or 'SRTM3', lower resolution\n",
    "    download_dem: bool = False \n",
    "    merge_dem: bool = False\n",
    "    dem_elev_max: float = 4430.0 # Threshold for nodata values in DEM. Highest point in CONUS is 4421 m.\n",
    "\n",
    "    # Create a tokenizer which allows us to represent images in patches instead of single pixels\n",
    "    tokenizer_downsample_ratio: int = 2\n",
    "    tokenizer_path = data_dir / f'tokenizer.pkl'\n",
    "\n",
    "    upload_to_s3: bool = True\n",
    "\n",
    "    # Mapping from raw NLCD classes to RGB colors for visualization\n",
    "    nlcd_to_rgb  = {\n",
    "            11: (0.278, 0.420, 0.627),\n",
    "            12: (0.820, 0.867, 0.976),\n",
    "            21: (0.867, 0.788, 0.788),\n",
    "            22: (0.847, 0.576, 0.510),\n",
    "            23: (0.929, 0.0, 0.0),\n",
    "            24: (0.667, 0.0, 0.0),\n",
    "            31: (0.698, 0.678, 0.639),\n",
    "            41: (0.408, 0.667, 0.388),\n",
    "            42: (0.110, 0.388, 0.188),\n",
    "            43: (0.710, 0.788, 0.557),\n",
    "            51: (0.647, 0.549, 0.188),\n",
    "            52: (0.800, 0.729, 0.486),\n",
    "            71: (0.886, 0.886, 0.757),\n",
    "            72: (0.788, 0.788, 0.467),\n",
    "            73: (0.600, 0.757, 0.278),\n",
    "            74: (0.467, 0.678, 0.576),\n",
    "            81: (0.859, 0.847, 0.239),\n",
    "            82: (0.667, 0.439, 0.157),\n",
    "            90: (0.729, 0.847, 0.918),\n",
    "            95: (0.439, 0.639, 0.729),  \n",
    "        }\n",
    "    nlcd_to_name = {\n",
    "        11: \"Open Water\",\n",
    "        12: \"Perennial Ice/Snow\",\n",
    "        21: \"Developed, Open Space\",\n",
    "        22: \"Developed, Low Intensity\",\n",
    "        23: \"Developed, Medium Intensity\",\n",
    "        24: \"Developed, High Intensity\",\n",
    "        31: \"Barren Land (Rock/Sand/Clay)\",\n",
    "        41: \"Deciduous Forest\",\n",
    "        42: \"Evergreen Forest\",\n",
    "        43: \"Mixed Forest\",\n",
    "        51: \"Dwarf Scrub\",\n",
    "        52: \"Shrub/Scrub\",\n",
    "        71: \"Grassland/Herbaceous\",\n",
    "        72: \"Sedge/Herbaceous\",\n",
    "        73: \"Lichens\",\n",
    "        74: \"Moss\",\n",
    "        81: \"Pasture/Hay\",\n",
    "        82: \"Cultivated Crops\",\n",
    "        90: \"Woody Wetlands\",\n",
    "        95: \"Emergent Herbaceous Wetlands\"\n",
    "    }\n",
    "    \n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Validate bbox coordinates\n",
    "        if not (self.bbox_west < self.bbox_east):\n",
    "            raise ValueError(f\"Invalid bbox coordinates: bbox_west ({self.bbox_west}) should be less than bbox_east ({self.bbox_east})\")\n",
    "        if not (self.bbox_south < self.bbox_north):\n",
    "            raise ValueError(f\"Invalid bbox coordinates: bbox_south ({self.bbox_south}) should be less than bbox_north ({self.bbox_north})\")\n",
    "        \n",
    "        # Validate file paths\n",
    "        if not self.nlcd_path.is_file():\n",
    "            raise FileNotFoundError(f\"NLCD file not found at {self.nlcd_path}\")\n",
    "        if not self.data_dir.is_dir():\n",
    "            raise FileNotFoundError(f\"Data directory not found at {self.data_dir}\")\n",
    "            \n",
    "\n",
    "config = DataPrepConfig()\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='[%(levelname)s] %(message)s',\n",
    "    level=config.logging_level,\n",
    ")\n",
    "\n",
    "np.random.seed(827)\n",
    "\n",
    "logging.info(f\"Setting project data directory to {config.data_dir}\")\n",
    "\n",
    "def downsample_patch(patch: np.ndarray, ratio: int, force_divisible=False) -> np.ndarray:\n",
    "    \"\"\"Downsample a patch by taking the mode of each ratio x ratio window.\"\"\"\n",
    "    if ratio == 1:\n",
    "        return patch\n",
    "    \n",
    "    if force_divisible:\n",
    "        h, w = patch.shape\n",
    "        new_h = (h // ratio) * ratio\n",
    "        new_w = (w // ratio) * ratio\n",
    "        patch = patch[:new_h, :new_w]\n",
    "    \n",
    "    # Reshape into blocks of size ratio x ratio\n",
    "    h, w = patch.shape\n",
    "    new_h, new_w = h // ratio, w // ratio\n",
    "    \n",
    "    reshaped = patch.reshape(new_h, ratio, new_w, ratio)\n",
    "    \n",
    "    # Move the two ratio axes adjacent so each block becomes one dimension\n",
    "    # resulting shape: (new_h * new_w, ratio * ratio)\n",
    "    reshaped = reshaped.swapaxes(1, 2).reshape(new_h * new_w, ratio * ratio)\n",
    "    \n",
    "    # mode(..., axis=1) finds the most frequent value in each row\n",
    "    block_modes, _ = mode(reshaped, axis=1)\n",
    "    \n",
    "    # Reshape back to (new_h, new_w)\n",
    "    downsampled = block_modes.reshape(new_h, new_w)\n",
    "    \n",
    "    return downsampled\n",
    "\n",
    "def get_pixel_bounds(src, x: float, y: float, size_pixels: int) -> Tuple[slice, slice]:\n",
    "    \"\"\"Convert geographic coordinates to pixel bounds for image extraction.\"\"\"\n",
    "    # Convert from working CRS to data CRS\n",
    "    x_data, y_data = from_working_crs.transform(x, y)\n",
    "    \n",
    "    # Convert to pixel coordinates\n",
    "    row, col = src.index(x_data, y_data)\n",
    "    \n",
    "    # Calculate pixel bounds\n",
    "    half_size = size_pixels // 2\n",
    "    row_start = row - half_size\n",
    "    row_end = row + half_size\n",
    "    col_start = col - half_size\n",
    "    col_end = col + half_size\n",
    "    \n",
    "    return (slice(row_start, row_end), slice(col_start, col_end))\n",
    "\n",
    "def check_overlap(point_coords: Tuple[float, float], image_size_meters: float, \n",
    "                 grid_gdf: gpd.GeoDataFrame, split: str) -> bool:\n",
    "    \"\"\"Check if an image centered at point_coords overlaps with the specified split area.\"\"\"\n",
    "    x, y = point_coords\n",
    "    half_size = image_size_meters / 2\n",
    "    \n",
    "    # Create a box representing the image extent in working CRS\n",
    "    image_box = box(x - half_size, y - half_size,\n",
    "                   x + half_size, y + half_size)\n",
    "    \n",
    "    # Check intersection with grid cells of the opposite split\n",
    "    opposite_split = 'test' if split == 'train' else 'train'\n",
    "    opposite_cells = grid_gdf[grid_gdf['split'] == opposite_split]\n",
    "    \n",
    "    return not any(image_box.intersects(cell) for cell in opposite_cells.geometry)\n",
    "\n",
    "def is_within_bbox(x: float, y: float) -> bool:\n",
    "    \"\"\"Check if a point is within the specified bbox.\"\"\"\n",
    "    return (config.bbox_west <= x <= config.bbox_east and\n",
    "            config.bbox_south <= y <= config.bbox_north)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Information and CRS Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset CRS: PROJCS[\"Albers_Conical_Equal_Area\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"meters\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n",
      "[INFO] Dataset bounds: BoundingBox(left=-2493045.0, bottom=177285.0, right=2342655.0, top=3310005.0)\n",
      "[INFO] Dataset shape: (104424, 161190)\n",
      "[INFO] Dataset resolution: (30.0, 30.0)\n",
      "[INFO] Dataset transform: | 30.00, 0.00,-2493045.00|\n",
      "| 0.00,-30.00, 3310005.00|\n",
      "| 0.00, 0.00, 1.00|\n",
      "[INFO] \n",
      "Bounding box validation:\n",
      "[INFO] Dataset bounds (lon/lat): -119.7861, 21.7423, -63.6722, 49.1771\n",
      "[INFO] Selected bbox (lon/lat): -119.0, 22.0, -64.0, 49.0\n",
      "[INFO] Maximum number of sampled images from full dataset: 64209\n"
     ]
    }
   ],
   "source": [
    "# Open the NLCD dataset and print basic information\n",
    "with rasterio.open(config.nlcd_path) as src:\n",
    "    logging.info(f\"Dataset CRS: {src.crs}\")\n",
    "    logging.info(f\"Dataset bounds: {src.bounds}\")\n",
    "    logging.info(f\"Dataset shape: {src.shape}\")\n",
    "    logging.info(f\"Dataset resolution: {src.res}\")\n",
    "    logging.info(f\"Dataset transform: {src.transform}\")\n",
    "    \n",
    "    # Set up CRS transformers\n",
    "    data_crs = src.crs\n",
    "    working_crs = CRS.from_string(config.working_crs)\n",
    "    \n",
    "    # Create transformers for converting between CRS\n",
    "    to_working_crs = Transformer.from_crs(data_crs, working_crs, always_xy=True)\n",
    "    from_working_crs = Transformer.from_crs(working_crs, data_crs, always_xy=True)\n",
    "    \n",
    "    # Convert dataset bounds to working CRS for validation\n",
    "    bounds = src.bounds\n",
    "    ds_left, ds_bottom = to_working_crs.transform(bounds.left, bounds.bottom)\n",
    "    ds_right, ds_top = to_working_crs.transform(bounds.right, bounds.top)\n",
    "    \n",
    "    \n",
    "    logging.info(\"\\nBounding box validation:\")\n",
    "    logging.info(f\"Dataset bounds (lon/lat): {ds_left:.4f}, {ds_bottom:.4f}, {ds_right:.4f}, {ds_top:.4f}\")\n",
    "    logging.info(f\"Selected bbox (lon/lat): {config.bbox_west}, {config.bbox_south}, {config.bbox_east}, {config.bbox_north}\")    \n",
    "    samples_x = src.shape[1] / config.downsample_ratio / config.image_size\n",
    "    samples_y = src.shape[0] / config.downsample_ratio / config.image_size\n",
    "\n",
    "    logging.info(f\"Maximum number of sampled images from full dataset: {samples_x * samples_y:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Class Counting and Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping class counts computation from raster; loading from file\n"
     ]
    }
   ],
   "source": [
    "# Function to compute class counts in a block\n",
    "def compute_block_counts(data):\n",
    "    unique, counts = np.unique(data, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "# Calculate available memory\n",
    "available_memory = psutil.virtual_memory().available\n",
    "dtype_size = np.dtype('uint8').itemsize\n",
    "max_elements = available_memory // (2 * dtype_size)  # Use half of available memory\n",
    "\n",
    "if not config.recompute_counts:\n",
    "    logging.info(f\"Skipping class counts computation from raster; loading from file\")\n",
    "else:\n",
    "    with rasterio.open(config.nlcd_path) as src:\n",
    "        # Convert bbox to pixel coordinates\n",
    "        bbox_left, bbox_bottom = from_working_crs.transform(config.bbox_west, config.bbox_south)\n",
    "        bbox_right, bbox_top = from_working_crs.transform(config.bbox_east, config.bbox_north)\n",
    "        \n",
    "        # Get pixel bounds\n",
    "        row_start, col_start = src.index(bbox_left, bbox_top)\n",
    "        row_end, col_end = src.index(bbox_right, bbox_bottom)\n",
    "        \n",
    "        # Ensure correct order\n",
    "        row_start, row_end = min(row_start, row_end), max(row_start, row_end)\n",
    "        col_start, col_end = min(col_start, col_end), max(col_start, col_end)\n",
    "        \n",
    "        # Calculate block size for the bbox region\n",
    "        bbox_height = row_end - row_start\n",
    "        bbox_width = col_end - col_start\n",
    "        total_pixels = bbox_height * bbox_width\n",
    "        n_blocks = max(1, total_pixels // max_elements)\n",
    "        block_height = bbox_height // n_blocks\n",
    "        \n",
    "        # Initialize counts dictionary\n",
    "        total_counts = {}\n",
    "        \n",
    "        # Process data in blocks within the bbox\n",
    "        for i in tqdm(range(row_start, row_end, block_height), desc='Computing class counts'):\n",
    "            # Read a block of data\n",
    "            window = rasterio.windows.Window(\n",
    "                col_start, i, \n",
    "                col_end - col_start,\n",
    "                min(block_height, row_end - i)\n",
    "            )\n",
    "            data = src.read(1, window=window)\n",
    "            \n",
    "            # Update counts\n",
    "            block_counts = compute_block_counts(data)\n",
    "            for k, v in block_counts.items():\n",
    "                total_counts[k] = total_counts.get(k, 0) + v\n",
    "\n",
    "    logging.info(f\"Unique values present in the bbox: {len(total_counts)}: {total_counts.keys()}\")\n",
    "\n",
    "    # Drop the counts which are in class 0 (Unknown)\n",
    "    _ = total_counts.pop(config.nlcd_original_unknown_class, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping class counts computation from raster; loading from file\n",
      "[INFO] Prepare lookup table for plotting with shape (100, 3)\n"
     ]
    }
   ],
   "source": [
    "if config.recompute_counts:\n",
    "\n",
    "    # Convert to DataFrame for better visualization\n",
    "    classes_df = pd.DataFrame([\n",
    "        {'class_value': k, 'count': v, 'name': config.nlcd_to_name.get(k, 'Unknown')} \n",
    "        for k, v in total_counts.items()\n",
    "    ])\n",
    "    classes_df['percentage'] = classes_df['count'] / classes_df['count'].sum() * 100\n",
    "    classes_df = classes_df.sort_values('count', ascending=False)\n",
    "\n",
    "    # Rename the index (currently unnamed) to \"class\"\n",
    "    classes_df.index.name = 'class'\n",
    "    classes_df = classes_df.sort_index()\n",
    "\n",
    "    # Load the mapping from original class codes to RGB for plotting and add to the dataframe\n",
    "    # We will use these for plotting later\n",
    "    classes_df['RGB'] = classes_df['class_value'].map(config.nlcd_to_rgb)\n",
    "    classes_df.to_parquet(Path(config.data_dir) / 'class_distribution.parquet')\n",
    "    present_classes = sorted(total_counts.keys())\n",
    "\n",
    "\n",
    "else:\n",
    "    logging.info(f\"Skipping class counts computation from raster; loading from file\")\n",
    "    classes_df = pd.read_parquet(Path(config.data_dir) / 'class_distribution.parquet')\n",
    "    total_counts = classes_df.set_index('class_value')['count'].to_dict()\n",
    "    classes_df['RGB'] = classes_df['class_value'].map(config.nlcd_to_rgb)\n",
    "\n",
    "\n",
    "palette_series = classes_df['RGB']\n",
    "lut = np.zeros((100, 3))\n",
    "for code, color in config.nlcd_to_rgb.items():\n",
    "    lut[code] = color\n",
    "\n",
    "logging.info(f\"Prepare lookup table for plotting with shape {lut.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel counts by class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Grid Creation for Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/m2ssd/data/Dropbox/research/lc-gpt/.venv/lib/python3.10/site-packages/scipy/stats/_qmc.py:958: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  sample = self._random(n, workers=workers)\n",
      "[INFO] Generated 125 test cells out of 2500 total cells and saved to grid_gdf\n",
      "[INFO] Created 2,500 records\n",
      "[INFO] Saved geodataframe for grid of train/test cells to /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/split_64_ratio8.gpkg\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import qmc  # Built into scipy, no extra installation needed\n",
    "\n",
    "# Set up grid for train/test split\n",
    "with rasterio.open(config.nlcd_path) as src:\n",
    "    # Create grid cells in working CRS using bbox\n",
    "    x_edges = np.linspace(config.bbox_west, config.bbox_east, config.n_grid_unit + 1)\n",
    "    y_edges = np.linspace(config.bbox_south, config.bbox_north, config.n_grid_unit + 1)\n",
    "    \n",
    "    # Create grid cell polygons\n",
    "    grid_cells = []\n",
    "    for i in range(len(x_edges)-1):\n",
    "        for j in range(len(y_edges)-1):\n",
    "            # Create polygon in working CRS\n",
    "            polygon = {\n",
    "                'geometry': {\n",
    "                    'type': 'Polygon',\n",
    "                    'coordinates': [[\n",
    "                        [x_edges[i], y_edges[j]],\n",
    "                        [x_edges[i+1], y_edges[j]],\n",
    "                        [x_edges[i+1], y_edges[j+1]],\n",
    "                        [x_edges[i], y_edges[j+1]],\n",
    "                        [x_edges[i], y_edges[j]]\n",
    "                    ]]\n",
    "                },\n",
    "                'properties': {'id': len(grid_cells)}\n",
    "            }\n",
    "            grid_cells.append(polygon)\n",
    "    \n",
    "    # Create GeoDataFrame in working CRS\n",
    "    grid_gdf = gpd.GeoDataFrame.from_features(grid_cells, crs=working_crs)\n",
    "    \n",
    "    # Set up Sobol sequence generator\n",
    "    n_cells = len(grid_gdf)\n",
    "    n_test = int(n_cells * config.area_fraction_test)\n",
    "    \n",
    "    # Generate Sobol sequence and scale to unique grid indices\n",
    "    sobol_points = qmc.Sobol(d=1, seed=config.random_seed).random(n=n_test)\n",
    "    sobol_indices = (sobol_points.flatten() * (n_cells - 1)).astype(int)\n",
    "    sobol_indices = np.unique(sobol_indices)\n",
    "    \n",
    "    # If we got fewer unique indices than needed, add random ones\n",
    "    if len(sobol_indices) < n_test:\n",
    "        additional_indices = np.random.choice(\n",
    "            np.setdiff1d(np.arange(n_cells), sobol_indices),\n",
    "            size=n_test - len(sobol_indices),\n",
    "            replace=False\n",
    "        )\n",
    "        sobol_indices = np.concatenate([sobol_indices, additional_indices])\n",
    "    \n",
    "    # Assign splits\n",
    "    grid_gdf['split'] = 'train'\n",
    "    grid_gdf.loc[sobol_indices, 'split'] = 'test'\n",
    "    logging.info(f\"Generated {n_test} test cells out of {n_cells} total cells and saved to grid_gdf\")\n",
    "\n",
    "# Save the grid to a GeoPackage file\n",
    "grid_gdf.to_file(config.split_save_path, driver='GPKG')\n",
    "logging.info(f\"Saved geodataframe for grid of train/test cells to {config.split_save_path}\")\n",
    "\n",
    "if config.show_plots:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    grid_gdf[grid_gdf['split'] == 'train'].plot(ax=ax, color='c', alpha=0.3)\n",
    "    grid_gdf[grid_gdf['split'] == 'test'].plot(ax=ax, color='m', alpha=0.3)\n",
    "\n",
    "    for x in x_edges:\n",
    "        ax.axvline(x, color='black', linestyle='--', alpha=0.5)\n",
    "    for y in y_edges:\n",
    "        ax.axhline(y, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Manually create legend elements\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor='c', alpha=0.3, label='Train unit'),\n",
    "        mpatches.Patch(facecolor='m', alpha=0.3, label='Test unit')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "    ax.set_title('Train/Test Grid Split')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sampling Land Cover Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Processing cells using 15 processes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabd416fedac44cf8349dac45fa7234b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing grid cells:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def process_cell(cell_data, args):\n",
    "    \"\"\"Process a single cell of data.\n",
    "    \n",
    "    Args:\n",
    "        cell_data: tuple of (cell, src_bounds, src_res, transforms)\n",
    "        args: dict containing configuration parameters\n",
    "    \"\"\"\n",
    "    cell, src_bounds, src_res, transforms = cell_data\n",
    "    to_working_crs, from_working_crs = transforms\n",
    "    \n",
    "    # Unpack configuration\n",
    "    config = args['config']\n",
    "    full_size_pixels = config.image_size * config.downsample_ratio\n",
    "    \n",
    "    bounds = cell.geometry.bounds\n",
    "    split = cell['split']\n",
    "    \n",
    "    # Convert bounds to pixel coordinates\n",
    "    bbox_left, bbox_bottom = from_working_crs.transform(bounds[0], bounds[1])\n",
    "    bbox_right, bbox_top = from_working_crs.transform(bounds[2], bounds[3])\n",
    "    \n",
    "    with rasterio.open(config.nlcd_path) as src:\n",
    "        row_start, col_start = src.index(bbox_left, bbox_top)\n",
    "        row_end, col_end = src.index(bbox_right, bbox_bottom)\n",
    "        \n",
    "        # Ensure correct order\n",
    "        row_start, row_end = min(row_start, row_end), max(row_start, row_end)\n",
    "        col_start, col_end = min(col_start, col_end), max(col_start, col_end)\n",
    "        \n",
    "        # Read the entire cell into memory\n",
    "        cell_data = src.read(1, window=rasterio.windows.Window(\n",
    "            col_start, row_start, \n",
    "            col_end - col_start, \n",
    "            row_end - row_start\n",
    "        ))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    done = False\n",
    "    for i in range(0, cell_data.shape[0] - full_size_pixels + 1, full_size_pixels // 2 ):\n",
    "        if done:\n",
    "            break\n",
    "        for j in range(0, cell_data.shape[1] - full_size_pixels + 1, full_size_pixels // 2):\n",
    "            patch = cell_data[i:i + full_size_pixels, j:j + full_size_pixels]\n",
    "            \n",
    "            # Reject if any pixels are unknown\n",
    "            if np.any(patch == config.nlcd_original_unknown_class):\n",
    "                continue\n",
    "            \n",
    "            # Check water fraction\n",
    "            reject = False\n",
    "            for c in config.nlcd_original_classes_for_reject:\n",
    "                class_fraction = np.mean(patch == c)\n",
    "                if class_fraction > config.max_fraction_reject_class:\n",
    "                    reject = True\n",
    "                    break\n",
    "            if reject:\n",
    "                continue\n",
    "            \n",
    "            # Downsample the patch\n",
    "            downsampled = downsample_patch(patch, config.downsample_ratio)\n",
    "            \n",
    "        \n",
    "            x_ul, y_ul = to_working_crs.transform(*src.xy(row_start + i, col_start + j))\n",
    "            x_lr, y_lr = to_working_crs.transform(*src.xy(\n",
    "                row_start + i + full_size_pixels, \n",
    "                col_start + j + full_size_pixels\n",
    "            ))\n",
    "            \n",
    "            bbox = box(x_ul, y_ul, x_lr, y_lr)\n",
    "            results.append((downsampled, bbox, split))\n",
    "            if config.n_samples_per_cell and len(results) >= config.n_samples_per_cell:\n",
    "                done = True\n",
    "                break\n",
    "    \n",
    "    return results\n",
    "\n",
    "def sample_images_parallel(grid_gdf, config, n_processes=None) -> tuple:\n",
    "    \"\"\"Sample and process images in parallel for either train or test set.\n",
    "    \n",
    "    Args:\n",
    "        grid_gdf: GeoDataFrame containing grid cells\n",
    "        config: Configuration object\n",
    "        n_processes: Number of processes to use (defaults to CPU count - 1)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_images, train_gdf, test_images, test_gdf)\n",
    "    \"\"\"\n",
    "    if n_processes is None:\n",
    "        n_processes = max(1, mp.cpu_count() - 1)\n",
    "    \n",
    "    # Get source metadata once\n",
    "    with rasterio.open(config.nlcd_path) as src:\n",
    "        src_bounds = src.bounds\n",
    "        src_res = src.res\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    transforms = (to_working_crs, from_working_crs)  # Assuming these are defined\n",
    "    cell_data = [(cell, src_bounds, src_res, transforms) for _, cell in grid_gdf.iterrows()]\n",
    "    \n",
    "    # Prepare static arguments\n",
    "    process_args = {\n",
    "        'config': config,\n",
    "    }\n",
    "    \n",
    "    # Create process pool and process cells in parallel\n",
    "    logging.info(f\"Processing cells using {n_processes} processes...\")\n",
    "    with mp.Pool(n_processes) as pool:\n",
    "        process_func = partial(process_cell, args=process_args)\n",
    "        results = list(tqdm(\n",
    "            pool.imap(process_func, cell_data),\n",
    "            total=len(cell_data),\n",
    "            desc=\"Processing grid cells\"\n",
    "        ))\n",
    "    \n",
    "    # Flatten results and separate train/test\n",
    "    train_images = []\n",
    "    test_images = []\n",
    "    train_bboxes = []\n",
    "    test_bboxes = []\n",
    "    \n",
    "    for cell_results in results:\n",
    "        for remapped, bbox, split in cell_results:\n",
    "            if split == 'train':\n",
    "                train_images.append(remapped)\n",
    "                train_bboxes.append(bbox)\n",
    "            else:\n",
    "                test_images.append(remapped)\n",
    "                test_bboxes.append(bbox)\n",
    "    \n",
    "    # Create GeoDataFrames for train and test bounding boxes\n",
    "    working_crs = grid_gdf.crs  # Get CRS from input GeoDataFrame\n",
    "    train_gdf = gpd.GeoDataFrame(geometry=train_bboxes, crs=working_crs)\n",
    "    test_gdf = gpd.GeoDataFrame(geometry=test_bboxes, crs=working_crs)\n",
    "    \n",
    "    return (np.array(train_images, dtype=np.uint8), train_gdf, np.array(test_images, dtype=np.uint8), test_gdf)\n",
    "\n",
    "\n",
    "train_images, train_gdf, test_images, test_gdf = sample_images_parallel(\n",
    "    grid_gdf,\n",
    "    config,\n",
    ")\n",
    "\n",
    "print(f\"Created {len(train_images)} train images and {len(test_images)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] All images are free of null / NaN values.\n",
      "[INFO] All images are the correct size.\n"
     ]
    }
   ],
   "source": [
    "# Make sure all images are in the valid integer range with no NaNs\n",
    "assert np.all(np.isfinite(train_images))\n",
    "assert np.all(np.isfinite(test_images))\n",
    "logging.info(\"All images are free of null / NaN values.\")\n",
    "\n",
    "# Check that the images are the correct size\n",
    "assert train_images.shape[1:] == (config.image_size, config.image_size), f\"Train shape: {train_images.shape} should be {(config.image_size, config.image_size)}\"\n",
    "assert test_images.shape[1:] == (config.image_size, config.image_size), f\"Test shape: {test_images.shape}, should be {(config.image_size, config.image_size)}\"\n",
    "logging.info(\"All images are the correct size.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.show_plots:\n",
    "    train_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping display of sample NLCD images. Set `show_plots` to True to display.\n"
     ]
    }
   ],
   "source": [
    "n_images = 3\n",
    "\n",
    "seen_classes = set()\n",
    "\n",
    "if config.show_plots:\n",
    "    fig, ax = plt.subplots(2, n_images, figsize=(8, 6))\n",
    "\n",
    "    for i in range(n_images):\n",
    "        for j, (images, geom, title) in enumerate(zip([train_images, test_images], [train_gdf.iloc[i].geometry, test_gdf.iloc[i].geometry], [\"Train\", \"Test\"])):\n",
    "            ax[j, i].imshow(lut[images[i]])\n",
    "            ax[j, i].set_title(f\"{title} Image {i+1}\")\n",
    "            ax[j, i].axis('off')\n",
    "\n",
    "            lat, lon = geom.centroid.xy\n",
    "            ax[j, i].text(1.5, 3, f\"{lat[0]:.3f}, {lon[0]:.3f}\", color='black', fontsize=8,\n",
    "                          bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "            seen_classes.update(np.unique(images[i]))\n",
    "\n",
    "    legend_handles = [mpatches.Patch(color=classes_df.loc[idx, \"RGB\"], label=classes_df.loc[idx, \"name\"]) for idx in seen_classes]\n",
    "\n",
    "    fig.legend(handles=legend_handles, loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.15))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.info(\"Skipping display of sample NLCD images. Set `show_plots` to True to display.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Class distribution across sampled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] \n",
      "Final class distribution (original class ID: percentage):\n",
      "[INFO] \n",
      "Training set:\n",
      "[INFO] 11 (Open Water): 2.58%\n",
      "[INFO] 12 (Perennial Ice/Snow): 0.00%\n",
      "[INFO] 21 (Developed, Open Space): 1.10%\n",
      "[INFO] 22 (Developed, Low Intensity): 1.24%\n",
      "[INFO] 23 (Developed, Medium Intensity): 0.94%\n",
      "[INFO] 24 (Developed, High Intensity): 0.33%\n",
      "[INFO] 31 (Barren Land (Rock/Sand/Clay)): 0.72%\n",
      "[INFO] 41 (Deciduous Forest): 12.34%\n",
      "[INFO] 42 (Evergreen Forest): 10.30%\n",
      "[INFO] 43 (Mixed Forest): 3.32%\n",
      "[INFO] 52 (Shrub/Scrub): 19.23%\n",
      "[INFO] 71 (Grassland/Herbaceous): 13.13%\n",
      "[INFO] 81 (Pasture/Hay): 8.05%\n",
      "[INFO] 82 (Cultivated Crops): 19.62%\n",
      "[INFO] 90 (Woody Wetlands): 5.67%\n",
      "[INFO] 95 (Emergent Herbaceous Wetlands): 1.43%\n"
     ]
    }
   ],
   "source": [
    "def compute_class_distribution(images):\n",
    "    unique, counts = np.unique(images, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    return {cls: count/total for cls, count in zip(unique, counts)}\n",
    "\n",
    "train_dist = compute_class_distribution(train_images)\n",
    "test_dist = compute_class_distribution(test_images)\n",
    "\n",
    "logging.info(\"\\nFinal class distribution (original class ID: percentage):\")\n",
    "logging.info(\"\\nTraining set:\")\n",
    "for cls_id, pct in train_dist.items():\n",
    "    logging.info(f\"{cls_id} ({config.nlcd_to_name[cls_id]}): {pct*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Downloading and matching with DEM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data using `elevation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping DEM download. Set `download_dem` to True to download.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if config.download_dem:\n",
    "    n_dem_downloads, bounds = 625, (config.bbox_west, config.bbox_south, config.bbox_east, config.bbox_north)  # should be a square number\n",
    "    # Define the bounding box for continental USA (approximate)\n",
    "    # For testing, use a sample pair of values like below:\n",
    "    # n_dem_downloads, bounds = 4, (-100.0, 28.0, -99.0, 29.0)  # should be a square number\n",
    "\n",
    "    os.makedirs(config.dem_dir, exist_ok=True)\n",
    "\n",
    "    # Calculate the number of splits in each dimension\n",
    "    n_splits = int(n_dem_downloads ** 0.5)\n",
    "\n",
    "    # Remove all files from the DEM directory\n",
    "    for file in config.dem_dir.glob('*.tif'):\n",
    "        file.unlink()\n",
    "\n",
    "    # Split bounds into a grid and download DEM data\n",
    "    with tqdm(total=n_dem_downloads, desc=\"Downloading DEM data\") as pbar:\n",
    "        for i in range(n_splits):\n",
    "            for j in range(n_splits):\n",
    "                west = bounds[0] + (bounds[2] - bounds[0]) * i / n_splits\n",
    "                east = bounds[0] + (bounds[2] - bounds[0]) * (i + 1) / n_splits\n",
    "                south = bounds[1] + (bounds[3] - bounds[1]) * j / n_splits\n",
    "                north = bounds[1] + (bounds[3] - bounds[1]) * (j + 1) / n_splits\n",
    "\n",
    "                assert west < east, f\"West {west} should be less than east {east}\"\n",
    "                assert south < north, f\"South {south} should be less than north {north}\"\n",
    "                \n",
    "                dem_save_path = config.dem_dir / f'conus_dem_{i}_{j}.tif'\n",
    "                elevation.clip(bounds=(west, south, east, north), output=dem_save_path, product=config.dem_product)\n",
    "\n",
    "                # Check the statistics on the DEM\n",
    "                with rasterio.open(dem_save_path) as src:\n",
    "                    dem_data = src.read(1)\n",
    "                    dem_nodata = src.nodata\n",
    "                    dem_stats = {\n",
    "                        'min': dem_data.min(),\n",
    "                        'max': dem_data.max(),\n",
    "                        'mean': dem_data.mean(),\n",
    "                        'nodata': dem_nodata,\n",
    "                        'nodata_fraction': np.mean(dem_data == dem_nodata)\n",
    "                    }\n",
    "                    logging.info(f\"DEM statistics for {dem_save_path}: {dem_stats}\")\n",
    "\n",
    "                pbar.update(1)\n",
    "else:\n",
    "    logging.info(\"Skipping DEM download. Set `download_dem` to True to download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge into single contiguous DEM raster file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from rasterio import merge\n",
    "\n",
    "if config.merge_dem:\n",
    "    # Create a list of all the GeoTIFF files\n",
    "    search_pattern = os.path.join(config.dem_dir, \"conus_dem_*.tif\")\n",
    "    dem_files = glob.glob(search_pattern)\n",
    "\n",
    "    src_files_to_mosaic = []\n",
    "    for file in dem_files:\n",
    "        src = rasterio.open(file)\n",
    "        src_files_to_mosaic.append(src)\n",
    "\n",
    "    mosaic, out_trans = merge.merge(src_files_to_mosaic)\n",
    "\n",
    "    # Copy the metadata from one of the input files\n",
    "    out_meta = src_files_to_mosaic[0].meta.copy()\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": mosaic.shape[1],\n",
    "        \"width\": mosaic.shape[2],\n",
    "        \"transform\": out_trans\n",
    "    })\n",
    "\n",
    "    with rasterio.open(config.merged_dem_path, \"w\", **out_meta) as dest:\n",
    "        dest.write(mosaic)\n",
    "        logging.info(f\"Merged DEM saved to {config.merged_dem_path}\")\n",
    "\n",
    "    logging.info(f\"Proportion of missing data in merged DEM: {np.mean(mosaic < config.dem_nodata_threshold):.2%}\")\n",
    "\n",
    "    for src in src_files_to_mosaic:\n",
    "        src.close()\n",
    "\n",
    "    # Delete variables to save on memory\n",
    "    del mosaic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check merged file metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset CRS: EPSG:4326\n",
      "[INFO] Dataset bounds: BoundingBox(left=-119.00013888888888, bottom=22.000138888888905, right=-64.00013888888893, top=49.00013888888889)\n",
      "[INFO] Dataset shape: (97200, 198000)\n",
      "[INFO] Dataset resolution: (0.0002777777777777776, 0.0002777777777777776)\n",
      "[INFO] Dataset transform: | 0.00, 0.00,-119.00|\n",
      "| 0.00,-0.00, 49.00|\n",
      "| 0.00, 0.00, 1.00|\n",
      "[INFO] Missing data value: -32768.0\n",
      "[INFO] Data type: ('int16',)\n"
     ]
    }
   ],
   "source": [
    "# Print basic information about the merged GeoTIFF file\n",
    "with rasterio.open(config.merged_dem_path) as merged_src:\n",
    "    logging.info(f\"Dataset CRS: {merged_src.crs}\")\n",
    "    logging.info(f\"Dataset bounds: {merged_src.bounds}\")\n",
    "    logging.info(f\"Dataset shape: {merged_src.shape}\")\n",
    "    logging.info(f\"Dataset resolution: {merged_src.res}\")\n",
    "    logging.info(f\"Dataset transform: {merged_src.transform}\")\n",
    "    logging.info(f\"Missing data value: {merged_src.nodata}\")\n",
    "    logging.info(f\"Data type: {merged_src.dtypes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show merged file as elevation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping display of merged DEM data. Set `show_plots` to True to display.\n"
     ]
    }
   ],
   "source": [
    "# Load the image and run imshow\n",
    "with rasterio.open(config.merged_dem_path) as src:\n",
    "        downsample_stride = 100\n",
    "        dem_data = src.read(1,\n",
    "            out_shape=(\n",
    "                src.count,\n",
    "                int(src.height / downsample_stride),\n",
    "                int(src.width / downsample_stride)\n",
    "            ),\n",
    "        resampling=rasterio.enums.Resampling.nearest\n",
    "    )\n",
    "if config.show_plots:\n",
    "    \n",
    "    # Calculate slope\n",
    "    x, y = np.gradient(dem_data, src.res[0], src.res[1])\n",
    "    slope = np.sqrt(x**2 + y**2)\n",
    "    log_slope = np.log10(slope + 1)  # Adding 1 to avoid log(0)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # Plot elevation\n",
    "    im1 = axes[0].imshow(dem_data, cmap='terrain', extent=(bounds[0], bounds[2], bounds[1], bounds[3]), vmin=0)\n",
    "    axes[0].set_title('Merged DEM Data')\n",
    "    axes[0].set_xlabel('Longitude')\n",
    "    axes[0].set_ylabel('Latitude')\n",
    "    cbar1 = fig.colorbar(im1, ax=axes[0], orientation='vertical', label='Elevation (meters)')\n",
    "    \n",
    "    # Plot log10 slope\n",
    "    im2 = axes[1].imshow(log_slope, cmap='viridis', extent=(bounds[0], bounds[2], bounds[1], bounds[3]))\n",
    "    axes[1].set_title('Log10 Slope')\n",
    "    axes[1].set_xlabel('Longitude')\n",
    "    axes[1].set_ylabel('Latitude')\n",
    "    cbar2 = fig.colorbar(im2, ax=axes[1], orientation='vertical', label='Log10 Slope')\n",
    "    \n",
    "    # Set the ticks to match the bounds\n",
    "    for ax in axes:\n",
    "        ax.set_xticks(np.linspace(bounds[0], bounds[2], num=3))\n",
    "        ax.set_yticks(np.linspace(bounds[1], bounds[3], num=3))\n",
    "        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1f}'))\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1f}'))\n",
    "        ax.grid(True, linestyle='--', alpha=0.8, color='k')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.info(\"Skipping display of merged DEM data. Set `show_plots` to True to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32767"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Join elevation data with land cover data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DEM images for 377539 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3674bbe62a0143aa90eff330ca1bed84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting DEM images:   0%|          | 0/377539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Number of images dropped due to nodata proportion exceeding threshold: 0 / 377539\n",
      "[INFO] Number of images with interpolation of missing values: 906 / 377539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DEM images for 18865 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de61222dc16c4da18a6c760657fb88a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting DEM images:   0%|          | 0/18865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Number of images dropped due to nodata proportion exceeding threshold: 0 / 18865\n",
      "[INFO] Number of images with interpolation of missing values: 56 / 18865\n",
      "/tmp/ipykernel_996135/3713914763.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  train_dem_images = train_dem_images / train_dem_images.max(axis=(1, 2), keepdims=True)\n",
      "/tmp/ipykernel_996135/3713914763.py:61: RuntimeWarning: invalid value encountered in cast\n",
      "  train_dem_images = (train_dem_images * 255).astype(np.uint8)\n"
     ]
    }
   ],
   "source": [
    "def extract_dem_images(gdf: gpd.GeoDataFrame, dem_src: rasterio.DatasetReader) -> np.ndarray[np.float32]:\n",
    "    dem_images = []\n",
    "    nodata_count = 0\n",
    "    interpolate_count = 0\n",
    "    print(f\"Extracting DEM images for {len(gdf)} samples\")\n",
    "    for _, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extracting DEM images\"):\n",
    "        \n",
    "        window = rasterio.windows.from_bounds(*row.geometry.bounds, transform=dem_src.transform) \n",
    "        \n",
    "        dem_data = dem_src.read(1, window=window)\n",
    "\n",
    "        # If the data is NaN or it's above the threshold, we set it to NaN\n",
    "        is_nodata = np.logical_or(dem_data == dem_src.nodata, dem_data>config.dem_elev_max)\n",
    "        nodata_fraction = np.mean(is_nodata)\n",
    "        dem_data = dem_data.astype(np.float32)\n",
    "\n",
    "        # If the read failed, the shape will be empty so we raise an alarm\n",
    "        # If any failure cases occur, we want the resulting DEM array to be all NaNs\n",
    "        # and have all dims with nonzero size.\n",
    "        if len(dem_data.shape) == 0:\n",
    "            dem_data  = np.empty((config.image_size, config.image_size)) * np.nan\n",
    "            logging.debug(f\"Failed to read window for row {row} with window {window}\")\n",
    "            nodata_count += 1\n",
    "        elif any([dim == 0 for dim in dem_data.shape]):\n",
    "            dem_data = np.empty((config.image_size, config.image_size)) * np.nan\n",
    "            logging.debug(f\"Window read for row {row} with bbox {row.geometry.bounds} has a zero dimension with shape {dem_data.shape}\")\n",
    "            nodata_count += 1\n",
    "        elif nodata_fraction > config.dem_nodata_threshold:            \n",
    "            dem_data *= np.nan\n",
    "            nodata_count += 1\n",
    "        elif np.any(is_nodata):\n",
    "            # Interpolate NaN values using a spatially informed method\n",
    "            dem_data = cv2.inpaint(dem_data, is_nodata.astype(np.uint8), inpaintRadius=3, flags=cv2.INPAINT_TELEA)\n",
    "            interpolate_count += 1\n",
    "\n",
    "        assert np.all(np.isnan(dem_data)) or np.nanmax(dem_data) < config.dem_elev_max, f\"DEM data contains values above nodata threshold: {dem_data.max()}\"\n",
    "\n",
    "        # Resize using cv2 to the desired image size\n",
    "        if not np.any(np.isnan(dem_data)):\n",
    "            dem_data = cv2.resize(dem_data, (config.image_size, config.image_size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        dem_images.append(dem_data)\n",
    "\n",
    "    logging.info(f\"Number of images dropped due to nodata proportion exceeding threshold: {nodata_count} / {len(gdf)}\")\n",
    "    logging.info(f\"Number of images with interpolation of missing values: {interpolate_count} / {len(gdf)}\")\n",
    "    return np.array(dem_images).astype(np.float32)\n",
    "\n",
    "with rasterio.open(config.merged_dem_path) as dem_src:\n",
    "    train_dem_images = extract_dem_images(train_gdf, dem_src)\n",
    "    test_dem_images = extract_dem_images(test_gdf, dem_src)\n",
    "\n",
    "# Offset all images to have a minimum of zero\n",
    "train_dem_images -= train_dem_images.min(axis=(1, 2), keepdims=True)\n",
    "test_dem_images -= test_dem_images.min(axis=(1, 2), keepdims=True)\n",
    "\n",
    "train_dem_images = train_dem_images / train_dem_images.max(axis=(1, 2), keepdims=True)\n",
    "test_dem_images  = test_dem_images / test_dem_images.max(axis=(1, 2), keepdims=True)\n",
    "\n",
    "\n",
    "# Case to uint8\n",
    "train_dem_images = (train_dem_images * 255).astype(np.uint8)\n",
    "test_dem_images = (test_dem_images * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of DEM values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7314977., 3056575., 2198493., 3471591., 2493582., 3912007.,\n",
       "        2692822., 4224856., 2889282., 4490232., 4583372., 3091568.,\n",
       "        4741877., 3223243., 4976500., 3337914., 5114291., 3397029.,\n",
       "        5182809., 3438414., 5463494., 5416825., 3618319., 5438494.,\n",
       "        3902094., 5501771., 3763519., 5668468., 3853560., 5785934.,\n",
       "        5791240., 3881322., 5721366., 4300789., 5872077., 3976467.,\n",
       "        5942263., 4026717., 5963341., 3920948., 6174278., 6017717.,\n",
       "        4077930., 5961567., 4028869., 5987049., 3932056., 5922141.,\n",
       "        3902485., 6581463., 5711381., 3827286., 5760815., 3865128.,\n",
       "        5713086., 3803393., 5723774., 3663956., 5509946., 3564875.,\n",
       "        5578992., 5278715., 3547027., 5214321., 3363952., 4946960.,\n",
       "        3657875., 4757686., 3157765., 4687678., 4478639., 3059739.,\n",
       "        4310082., 2838925., 4265876., 2594295., 3854688., 2537710.,\n",
       "        3616254., 2261799., 3473333., 3194681., 2025166., 2963693.,\n",
       "        1853138., 2651719., 1645946., 2320290., 1474252., 2000266.,\n",
       "        1798888., 1088238., 1463030.,  872587., 1134791.,  626829.,\n",
       "         773360.,  405216.,  431976., 1023922.]),\n",
       " array([  0.  ,   2.55,   5.1 ,   7.65,  10.2 ,  12.75,  15.3 ,  17.85,\n",
       "         20.4 ,  22.95,  25.5 ,  28.05,  30.6 ,  33.15,  35.7 ,  38.25,\n",
       "         40.8 ,  43.35,  45.9 ,  48.45,  51.  ,  53.55,  56.1 ,  58.65,\n",
       "         61.2 ,  63.75,  66.3 ,  68.85,  71.4 ,  73.95,  76.5 ,  79.05,\n",
       "         81.6 ,  84.15,  86.7 ,  89.25,  91.8 ,  94.35,  96.9 ,  99.45,\n",
       "        102.  , 104.55, 107.1 , 109.65, 112.2 , 114.75, 117.3 , 119.85,\n",
       "        122.4 , 124.95, 127.5 , 130.05, 132.6 , 135.15, 137.7 , 140.25,\n",
       "        142.8 , 145.35, 147.9 , 150.45, 153.  , 155.55, 158.1 , 160.65,\n",
       "        163.2 , 165.75, 168.3 , 170.85, 173.4 , 175.95, 178.5 , 181.05,\n",
       "        183.6 , 186.15, 188.7 , 191.25, 193.8 , 196.35, 198.9 , 201.45,\n",
       "        204.  , 206.55, 209.1 , 211.65, 214.2 , 216.75, 219.3 , 221.85,\n",
       "        224.4 , 226.95, 229.5 , 232.05, 234.6 , 237.15, 239.7 , 242.25,\n",
       "        244.8 , 247.35, 249.9 , 252.45, 255.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGsCAYAAACB/u5dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAeTElEQVR4nO3df5CV1X0/8M8i7NpsLgqKgIhERDQtSAohSBMBf0Uyo0HrFGPaiZq0jSaZqS1tldYOmE5kjBNMimgcU9FE4zQ/SpMRQcWoIxSx/gSVQCI/qsuygBAWFFjB8/0jX29n5Yd7l3O59y6v18wZuc997j6fPd65973nnOd56iIiBQBABt0qXQAA0HUIFgBANoIFAJCNYAEAZCNYAADZCBYAQDaCBQCQjWABAGQjWAAA2QgWAEA2FQsWZ599dvzyl7+MpqamSCnFpEmTOvVzpkyZEitXroxdu3bFm2++Gf/0T/+UuVIAoKO6V+rAjY2N8fLLL8c999wTc+fO7dTP+N73vhef/exn4+///u9j+fLl0bt37+jdu3fmSgGAUqRKt5RSmjRpUrtt9fX16dZbb01vvvlm2rFjR3rmmWfS+PHji8+fccYZqa2tLQ0dOrTi9Wuapmma9vtWtWssbr/99hg7dmx84QtfiDPPPDN++tOfxoIFC2LIkCEREXHxxRfH6tWr46KLLorVq1fHmjVr4u67745evXpVuHIAOLJVPN18cMRi4MCB6d133039+/dvt99jjz2WvvWtb6WISHfeeWfauXNnWrJkSfrMZz6Txo8fn1544YX0+OOPV/z30TRN07QjtVVsjcXBDB8+PLp37x6rVq1qt72hoSHeeuutiIjo1q1bHH300fGlL30pfvOb30RExFe+8pV44YUXYujQofu8FgAov6oMFh/96Edjz549MWrUqNi7d2+753bs2BEREc3NzfHuu+8WQ0VExIoVKyIi4uSTTxYsAKACqjJYvPjii9G9e/c44YQTYtGiRfvdZ/HixdGjR48YPHhwrF69OiIihg4dGhER69atO2y1AgD/py5+Pydy2DU2NhYXYr700kvxt3/7t/HEE0/Eli1b4o033ogf/ehH8elPfzqmTJkSL774YvTp0yfOO++8WLZsWTz88MNRV1cX//M//xM7duyI6667Lrp16xazZ8+O1tbWuPDCCyvxKwEAUaHFHePHj0/7M2fOnN8v/ujePU2fPj2tXr067d69OzU1NaWf//znadiwYcWf0b9///Szn/0stba2pubm5nTPPfekXr16VXzhiqZpmqYdqa1iIxYAQNdTtdexAABqj2ABAGRTkbNCTjzxxNi+fXslDg0AdFKhUIj169cfdJ/DHixOPPHEaGpqOtyHBQAyGDBgwEHDxWEPFu+PVAwYMMCoBQDUiEKhEE1NTR/63V2xC2Rt375dsACALsbiTQAgG8ECAMhGsAAAshEsAIBsBAsAIBvBAgDIRrAAALIRLACAbAQLACAbwQIAyEawAACyESwAgGwECwAgG8ECAMimYrdNL4fvLF+yz7Ypw8dWoBIAODIZsQAAshEsAIBsBAsAIBvBAgDIRrAAALIRLACAbAQLACAbwQIAyEawAACyESwAgGxKChZr1qyJlNI+7fbbby9XfQBADSnpXiGjR4+Oo446qvh42LBhsXDhwvjpT3+avTAAoPaUFCw2b97c7vENN9wQv/3tb+Opp57KWhQAUJs6fXfTHj16xF/8xV/EzJkzD7pffX19NDQ0FB8XCoXOHhIAqHKdXrx5ySWXxLHHHhv33nvvQfebOnVqtLa2FltTU1NnDwkAVLlOB4uvfOUrMX/+/Ghubj7ofjNmzIiePXsW24ABAzp7SACgynVqKuTkk0+O888/P/70T//0Q/dta2uLtra2zhwGAKgxnRqxuPrqq2Pjxo0xb9683PUAADWs5GBRV1cXV199ddx3332xd+/ectQEANSokoPF+eefH4MGDYp77rmnHPUAADWs5DUWjz32WNTV1ZWjFgCgxrlXCACQjWABAGQjWAAA2QgWAEA2ggUAkI1gAQBkI1gAANkIFgBANoIFAJCNYAEAZCNYAADZCBYAQDaCBQCQjWABAGRT8m3TAd73neVL9tk2ZfjYClQCVAsjFgBANoIFAJCNYAEAZCNYAADZCBYAQDaCBQCQjWABAGQjWAAA2QgWAEA2ggUAkI1gAQBkI1gAANkIFgBANoIFAJCNYAEAZCNYAADZCBYAQDaCBQCQTfdKFwAc2HeWL2n3eMrwsRWqBKBjjFgAANkIFgBANoIFAJCNYAEAZFNysDjxxBPjRz/6UWzevDneeeedWLZsWYwaNaoctQEANaaks0KOPfbYWLx4cTzxxBPxuc99LjZt2hSnnXZabN26tVz1AQA1pKRgcf3118cbb7wRX/7yl4vb1q5dm7smAKBGlRQsPv/5z8cjjzwSP/nJT2L8+PHR1NQUd9xxR/zgBz844Gvq6+ujoaGh+LhQKHS+WjjCffC6FhGubQFUl5KCxeDBg+Paa6+NmTNnxs033xyjR4+Of/u3f4u2trb44Q9/uN/XTJ06NaZPn56jViATAQUol5KCRbdu3eK5556Lf/7nf46IiJdeeimGDRsW11xzzQGDxYwZM2LmzJnFx4VCIZqamg6hZKh95fxiFxqASiopWDQ3N8drr73WbtuKFSvisssuO+Br2traoq2trXPVQQ3yxQ4cyUoKFosXL47TTz+93bahQ4fGunXrshYFVJ6ABHRGSdexuO222+Kss86KqVOnxqmnnhpXXHFF/PVf/3XMnj27XPUBADWkpGDx3HPPxaWXXhpXXHFFvPLKK/Ev//Ivcd1118WPf/zjctUHANSQkm+bPm/evJg3b145aoGqZ3oA4ODcKwQAyEawAACyESwAgGxKXmMBtWB/ayE+yNoIgPwEC+Cw+2DwE/Kg6zAVAgBkY8QC/j+nkuaRazSiIz/HyAdUHyMWAEA2ggUAkI2pEGqO4W+A6mXEAgDIRrAAALIRLACAbKyxAI541u1APkYsAIBsBAsAIBtTIUCXZpoDDi/BgqriSwCgtgkWlMw9NQA4EGssAIBsjFhw2JjmAOj6BAuyEBoAiDAVAgBkJFgAANkIFgBANoIFAJCNYAEAZOOsEIAPcBE46DwjFgBANoIFAJCNqRCATjBdAvtnxAIAyMaIBe34KwyAQ2HEAgDIRrAAALIxFXIEMc0BQLkZsQAAsikpWEybNi1SSu3aihUrylUbAFBjSp4KeeWVV+L8888vPt6zZ0/WggCA2lVysNizZ0+0tLSUoxYAoMaVvMbitNNOi6ampnj99dfj/vvvj4EDBx50//r6+igUCu0aANA1lRQsli5dGldddVVMnDgxrr322jjllFPi6aefjo9+9KMHfM3UqVOjtbW12Jqamg65aACgOpU0FbJgwYLiv5cvXx5Lly6NdevWxeTJk+Oee+7Z72tmzJgRM2fOLD4uFArCRRk4lRSAanBI17HYtm1brFq1KoYMGXLAfdra2qKtre1QDgNQkwR+jkSHdB2LxsbGOPXUU6O5uTlXPQBADSspWNx6660xbty4GDRoUIwdOzbmzp0be/fujQcffLBc9QEANaSkqZCTTjopHnzwwTjuuONi06ZNsWjRojjrrLNi8+bN5aoPAKghJQWLK664olx1AABdgHuFAADZCBYAQDaCBQCQjWABAGRzSBfIAuDQuIgWXY1gUQN88ABQK0yFAADZCBYAQDamQgCqnOlQaokRCwAgG8ECAMhGsAAAshEsAIBsLN6sMIuyAOhKjFgAANkIFgBANoIFAJCNNRaH2f7WVABAVyFYANSgD/6RYtE31cJUCACQjWABAGQjWAAA2QgWAEA2ggUAkI1gAQBkI1gAANm4jgUchAuaUctc64JKMGIBAGRjxALgCGZUg9yMWAAA2RixyEjyB+BIZ8QCAMhGsAAAshEsAIBsBAsAIBvBAgDIRrAAALI5pGBx/fXXR0opbrvttlz1ACX6zvIl7RpAJXU6WHzyk5+Mr371q/Hyyy/nrAcAqGGdukBWY2NjPPDAA/FXf/VXceONN+auCcjMSAZwuHQqWMyePTvmzZsXjz/++IcGi/r6+mhoaCg+LhQKnTkk1DRf7MCRouRgcfnll8fIkSNj9OjRHdp/6tSpMX369FIPAwDUoJKCxUknnRTf+9734oILLojdu3d36DUzZsyImTNnFh8XCoVoamoqrUqoYkYjAP5PScFi1KhR0bdv33jhhRf+7wd07x7jxo2Lb3zjG9HQ0BDvvfdeu9e0tbVFW1tbnmoBgKpWUrB4/PHHY9iwYe22zZkzJ37961/HLbfcsk+oAGqHkRcgh5KCxY4dO+LVV19tt+3tt9+Ot956a5/tXY1bolNutfDFXgs1ApXVqbNCIAdfUgBdzyEHi3POOSdHHXDYCTbVw/8L6DqMWFDzfClBeZkKphSCBVBWgh8cWQQLoCYIKFAb3DYdAMjGiAVVzV+pALXFiAUAkI1gAQBkI1gAANkIFgBANhZvAl1GLSz2rYUa4VAYsQAAsjFiQVn4qwyOLC77zfuMWAAA2QgWAEA2pkKAI4ppOigvwQKgAwQS6BhTIQBANoIFAJCNqZBwmhTA4eCz9shgxAIAyMaIBR/KojUAOsqIBQCQjRGLI5zRCAByEiwAMhHUwVQIAJCREYsuzF9PABxuRiwAgGwECwAgG8ECAMhGsAAAsrF4s0ZZmAlANTJiAQBkY8QCgIrY38irO57WPsECoMqY6qSWmQoBALIRLACAbAQLACCbktZYXHPNNXHttdfGxz72sYiIePXVV+Ob3/xmLFiwoBy1HbHMrwJQq0oKFm+++WbccMMN8Zvf/Cbq6uriyiuvjF/84hfxx3/8x/Haa6+Vq8ZD8sEvaSuOga7IHyRUi5KCxUMPPdTu8Y033hjXXnttnHXWWVUbLACAw6fTp5t269Yt/uzP/iwaGxtjyZIDJ+X6+vpoaGgoPi4UCp09JABQ5UpevDls2LDYvn177N69O77//e/HpZdeGitWrDjg/lOnTo3W1tZia2pqOqSCAYDqVXKwWLlyZXziE5+IMWPGxJ133hn33XdffPzjHz/g/jNmzIiePXsW24ABAw6pYACgepU8FfLuu+/G66+/HhERL7zwQowePTr+5m/+Jq655pr97t/W1hZtbW2HViUARySX/a49h3wdi27durVbQwEAHLlKGrG4+eabY/78+fG///u/USgU4otf/GJMmDAhLrzwwnLVBwDUkJKCxQknnBA//OEPo3///rFt27ZYtmxZXHjhhbFw4cJy1VfTnFcOwJGmpGDxl3/5l+WqAwDoAtwrBADIptMXyAKgtpie5XAwYgEAZCNYAADZCBYAQDaCBQCQjWABAGQjWAAA2TjdtJOctgUA+zJiAQBkY8QCgJrm1urVxYgFAJCNEQsAiqwf41AZsQAAshEsAIBsTIUAUBLTJRyMEQsAIBsjFvshjQNA5xixAACyESwAgGwECwAgG8ECAMhGsAAAshEsAIBsBAsAIBvBAgDIxgWyAOjy9nfhwynDx1agkq7PiAUAkI0RCwCyc2uEI5cRCwAgG8ECAMjmiJsKMTwHAOVjxAIAyEawAACyESwAgGwECwAgG8ECAMimpGBxww03xLPPPhutra3R0tISc+fOjaFDh5arNgCgxpQULMaPHx+zZ8+Os846Ky644ILo0aNHPProo/GRj3ykXPUBADWkpOtYfO5zn2v3+KqrropNmzbFqFGj4umnn85aGABQew7pAlnHHHNMRERs2bLlgPvU19dHQ0ND8XGhUDiUQwLQhbmIYe3rdLCoq6uL7373u7Fo0aJ49dVXD7jf1KlTY/r06Z09DAAcFm6tnkenzwqZPXt2DBs2LL7whS8cdL8ZM2ZEz549i23AgAGdPSQAUOU6NWIxa9asuOiii2LcuHHR1NR00H3b2tqira2tU8UBALWl5GAxa9asuPTSS2PChAmxdu3aMpQEANSqkoLF7Nmz44tf/GJMmjQptm/fHn379o2IiG3btsWuXbvKUiAAUDtKWmPxta99LY499th46qmnYsOGDcV2+eWXl6s+AKCGlDRiUVdXV646AIAu4JCuYwEAh5trXVQ3NyEDALIRLACAbEyFAMABfHDaxZU4P5wRCwAgG8ECAMhGsAAAshEsAIBsBAsAIBvBAgDIxummAHQ5rs5ZOUYsAIBsBAsAIBvBAgDIRrAAALIRLACAbAQLACAbwQIAyMZ1LAA4IrnWRXkIFgBwCD4YUKYMH1uhSqqDqRAAIBvBAgDIRrAAALIRLACAbAQLACAbwQIAyEawAACyESwAgGwECwAgG8ECAMhGsAAAshEsAIBsBAsAIBvBAgDIRrAAALLpXukCAKCr+87yJftsmzJ8bAUqKT8jFgBANiWPWJx99tnxD//wDzFq1Kg48cQT45JLLolf/OIX5agNAKrK/kYeaK/kEYvGxsZ4+eWX4+tf/3o56gEAaljJIxYLFiyIBQsWlKMWAKDGlX3xZn19fTQ0NBQfFwqFch8SAI4YH5yeqfSi0LIv3pw6dWq0trYWW1NTU7kPCQBUSNmDxYwZM6Jnz57FNmDAgHIfEgCokLJPhbS1tUVbW1u5DwMAVAHXsQAAsil5xKKxsTGGDBlSfHzKKafEiBEjYsuWLfHGG29kLQ4AqC0lB4tPfvKT8eSTTxYf33bbbRERce+998bVV1+drTAAoPaUHCyeeuqpqKurK0ctAFDzjvSrc1pjAQBkI1gAANkIFgBANoIFAJBN2S+QBQB8uP0t+qz0fT86w4gFAJCNYAEAZCNYAADZCBYAQDaCBQCQjWABAGTjdFMAqICuek8RIxYAQDaCBQCQjWABAGQjWAAA2QgWAEA2ggUAkI1gAQBkI1gAANm4QBYA1IhauKiWEQsAIBsjFgBQpWphhOKDjFgAANkIFgBANoIFAJCNYAEAZCNYAADZCBYAQDaCBQCQjWABAGQjWAAA2QgWAEA2ggUAkI1gAQBkI1gAANkIFgBANp0KFl/72tdizZo1sXPnznjmmWdi9OjRuesCAGpQycFi8uTJMXPmzLjpppti5MiR8fLLL8cjjzwSffr0KUd9AEANKTlY/N3f/V3cfffdce+998aKFSvimmuuiXfeeSe+/OUvl6M+AKCGdC9l5x49esSoUaNixowZxW0ppVi4cGGMHTt2v6+pr6+PhoaG4uNCodDuvznVdzsq+88EgFpSju/XUn5uScHi+OOPj+7du0dLS0u77S0tLXHGGWfs9zVTp06N6dOn77O9qamplEMDAB3wjdbWsv78QqEQ27dvP+DzJQWLzpgxY0bMnDmz3bbevXvHli1bsh6nUChEU1NTDBgw4KC/MJ2nj8tL/5aX/i0/fVxe1dC/hUIh1q9ff9B9SgoWmzdvjj179kTfvn3bbe/bt29s2LBhv69pa2uLtra2dtvK2SHbt2/3hi4zfVxe+re89G/56ePyqmT/duS4JS3efPfdd+P555+P8847r7itrq4uzjvvvFiyZEnpFQIAXUrJUyEzZ86M++67L5577rl49tln47rrrovGxsaYM2dOOeoDAGpIycHiJz/5SfTp0ye++c1vRr9+/eKll16KiRMnxsaNG8tRX4ft3r07pk+fHrt3765oHV2ZPi4v/Vte+rf89HF51Ur/1kVEqnQRAEDX4F4hAEA2ggUAkI1gAQBkI1gAANl0mWDhVu55TJs2LVJK7dqKFSuKzzc0NMTtt98emzdvju3bt8fPfvazOOGEEypYcXU7++yz45e//GU0NTVFSikmTZq0zz433XRTrF+/Pt5555147LHHYsiQIe2e79WrV9x///2xbdu22Lp1a/zgBz+IxsbGw/UrVL0P6+M5c+bs856eP39+u3308YHdcMMN8eyzz0Zra2u0tLTE3LlzY+jQoe326cjnwsCBA+Ohhx6Kt99+O1paWuLb3/52HHWU+zt1pH+feOKJfd7Dd955Z7t9qq1/U623yZMnp127dqWrrroqffzjH0933XVX2rJlS+rTp0/Fa6u1Nm3atLR8+fLUt2/fYjvuuOOKz99xxx1p3bp16ZxzzkkjR45M//3f/50WLVpU8bqrtU2cODH967/+a7rkkktSSilNmjSp3fP/+I//mLZu3Zo+//nPp+HDh6f/+q//Sq+//npqaGgo7vPwww+nF198MX3qU59Kn/70p9OqVavSAw88UPHfrVrah/XxnDlz0sMPP9zuPX3ssce220cfH7jNnz8/XXnllekP//AP05lnnpkeeuihtHbt2vSRj3ykuM+HfS5069YtLVu2LD366KNpxIgRaeLEiWnjxo3pW9/6VsV/v0q3jvTvE088ke6666527+FCoVDN/Vv5jj3U9swzz6RZs2YVH9fV1aU333wzXX/99RWvrdbatGnT0osvvrjf53r27Jl2796dLrvssuK2008/PaWU0pgxYypee7W3/X3prV+/Pk2ZMqVdH+/cuTNdfvnlKSLSGWeckVJKadSoUcV9LrzwwrR3797Uv3//iv9O1dYOFCzmzp17wNfo49La8ccfn1JK6eyzz04RHftcmDhxYtqzZ0864YQTivt89atfTb/73e9Sjx49Kv47VVP7YP9G/D5Y3HbbbQd8TbX1b81Phbx/K/eFCxcWt33Yrdw5uNNOOy2ampri9ddfj/vvvz8GDhwYERGjRo2K+vr6dn29cuXKWLdunb7uhFNOOSX69+/frj9bW1tj6dKlxf4cO3ZsbN26NZ5//vniPgsXLoz33nsvxowZc9hrrlUTJkyIlpaW+PWvfx133HFH9O7du/icPi7NMcccExFRvJFkRz4Xxo4dG8uXL293IcVHHnkkjjnmmPijP/qjw1h99ftg/77vz//8z2PTpk2xfPnyuPnmm+MP/uAPis9VW/+W/e6m5daZW7lzYEuXLo2rrroqVq5cGf37949p06bF008/HcOGDYt+/frF7t27Y9u2be1e09LSEv369atQxbXr/T7b33v3/ef69eu3z1Vt9+7dG1u2bNHnHbRgwYL4z//8z1izZk2ceuqpcfPNN8f8+fNj7Nix8d577+njEtTV1cV3v/vdWLRoUbz66qsRER36XOjXr99+3+fvP8fv7a9/IyJ+/OMfx7p162L9+vVx5plnxi233BKnn356XHbZZRFRff1b88GCvBYsWFD89/Lly2Pp0qWxbt26mDx5cuzcubOClUHn/Md//Efx36+88kosW7YsVq9eHRMmTIhf/epXFays9syePTuGDRsWn/nMZypdSpd0oP69++67i/9+5ZVXorm5OX71q1/F4MGDY/Xq1Ye7zA9V81MhnbmVOx23bdu2WLVqVQwZMiQ2bNgQDQ0NxaG69+nrznm/zw723t2wYcM+q+uPOuqo6N27tz7vpDVr1sSmTZuKZ9/o446ZNWtWXHTRRXHOOedEU1NTcXtHPhc2bNiw3/f5+89x4P7dn6VLl0ZEtHsPV1P/1nywcCv38mpsbIxTTz01mpub4/nnn4+2trZ2fT106NAYNGiQvu6ENWvWRHNzc7v+LBQKMWbMmGJ/LlmyJHr16hUjR44s7nPuuedGt27dih8ulGbAgAFx3HHHRXNzc0To446YNWtWXHrppXHuuefG2rVr2z3Xkc+FJUuWxPDhw6NPnz7FfS644ILYtm1bvPbaa4fld6hmB+vf/fnEJz4REdHuPVxt/VvxVbCH2iZPnpx27tyZvvSlL6Uzzjgjff/7309btmxpt0JW61i79dZb07hx49KgQYPS2LFj06OPPpo2btyYjj/++BTx+9PK1q5dmyZMmJBGjhyZFi9enBYvXlzxuqu1NTY2phEjRqQRI0aklFK67rrr0ogRI9LAgQNTxO9PN92yZUu6+OKL07Bhw9LcuXP3e7rp888/n0aPHp3+5E/+JK1cudKpkB3s48bGxvTtb387jRkzJg0aNCide+656bnnnksrV65M9fX1+rgDbfbs2Wnr1q1p3Lhx7U53PProo4v7fNjnwvunQy5YsCCdeeaZ6bOf/WxqaWlxumkH+nfw4MHpxhtvTCNHjkyDBg1KF198cfrtb3+bnnzyyWru38p3bI729a9/Pa1duzbt2rUrPfPMM+lTn/pUxWuqxfbggw+mpqamtGvXrvTGG2+kBx98MA0ePLj4fENDQ7r99tvTW2+9lXbs2JF+/vOfp759+1a87mpt48ePT/szZ86c4j433XRTam5uTjt37kyPPfZYOu2009r9jF69eqUHHnggtba2pt/97nfp3//931NjY2PFf7dqaQfr46OPPjotWLAgtbS0pN27d6c1a9aku+66a58/OvTxgduBXHnllcV9OvK5cPLJJ6d58+alt99+O23cuDHdeuut6aijjqr471fp9mH9e9JJJ6Unn3wybd68Oe3cuTOtWrUq3XLLLe2uY1Ft/eu26QBANjW/xgIAqB6CBQCQjWABAGQjWAAA2QgWAEA2ggUAkI1gAQBkI1gAANkIFgBANoIFAJCNYAEAZCNYAADZ/D9WAHyaWBumgQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_dem_images.flatten(), bins=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show DEM images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping DEM image visualization; set show_plots to True to display images.\n"
     ]
    }
   ],
   "source": [
    "# Plot several train and test images using elevation colormap\n",
    "if config.show_plots:\n",
    "    n_images = 3\n",
    "    fig, ax = plt.subplots(2, n_images, figsize=(8, 5))\n",
    "\n",
    "    # Pick random sample of train and test images to show\n",
    "    sampled_train_indices = np.random.choice(len(train_dem_images), n_images, replace=False)\n",
    "    sampled_test_indices = np.random.choice(len(test_dem_images), n_images, replace=False)\n",
    "\n",
    "    for i in range(n_images):\n",
    "        for j, (images, gdf, title, sampled_indices) in enumerate(zip(\n",
    "                [train_dem_images, test_dem_images], \n",
    "                [train_gdf, test_gdf], \n",
    "                [\"Train DEM\", \"Test DEM\"],\n",
    "                [sampled_train_indices, sampled_test_indices])):\n",
    "            \n",
    "            im = ax[j, i].imshow(images[sampled_indices[i]], cmap='terrain')\n",
    "            ax[j, i].set_title(f\"{title} Image {i+1}\")\n",
    "            ax[j, i].axis('off')\n",
    "            centroid = gdf.iloc[sampled_indices[i]].geometry.centroid\n",
    "            ax[j, i].text(1.5, 3, f\"{centroid.y:.3f}, {centroid.x:.3f}\",color='black', \n",
    "                          bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "            # Add gridlines and lat/long overlay\n",
    "            ax[j, i].grid(True, linestyle='--', alpha=0.8, color='k')\n",
    "            ax[j, i].set_xticks(np.linspace(0, config.image_size, num=3))\n",
    "            ax[j, i].set_yticks(np.linspace(0, config.image_size, num=3))\n",
    "            ax[j, i].set_xticklabels(np.linspace(centroid.x - config.image_size // 2, centroid.x + config.image_size // 2, num=3).round(2))\n",
    "            ax[j, i].set_yticklabels(np.linspace(centroid.y - config.image_size // 2, centroid.y + config.image_size // 2, num=3).round(2))\n",
    "            \n",
    "    # Add a common colorbar on the right-hand side\n",
    "    cbar_ax = fig.add_axes([1.0, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax, orientation='vertical', label='Elevation (meters)')\n",
    "\n",
    "    plt.tight_layout(rect=[0.1, 0, 1, 1])\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.info(\"Skipping DEM image visualization; set show_plots to True to display images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping visualization of NLCD and DEM data; set show_plots=True to enable.\n"
     ]
    }
   ],
   "source": [
    "# Create visualization of NLCD and DEM data\n",
    "n_images = 3\n",
    "seen_classes = set()\n",
    "if config.show_plots:\n",
    "    fig, axes = plt.subplots(2, n_images, figsize=(n_images*2.5, 8))\n",
    "\n",
    "    # Pick random sample of train images to show\n",
    "    \n",
    "    sampled_indices = np.random.choice(len(train_images), n_images, replace=False)\n",
    "\n",
    "    # Plot first n_images from training set\n",
    "    for i, sample_idx in enumerate(sampled_indices):\n",
    "\n",
    "        # Get DEM data for this image\n",
    "        dem = train_dem_images[sample_idx]\n",
    "        dem_min = dem.min()\n",
    "        dem_relative = dem - dem_min\n",
    "        \n",
    "        # Calculate contours (relative to minimum elevation)\n",
    "        levels = np.linspace(0, dem_relative.max(), 10)\n",
    "        \n",
    "        # Plot NLCD with contours\n",
    "        axes[0, i].imshow(lut[train_images[sample_idx]])\n",
    "        cs = axes[0, i].contour(dem_relative, levels=levels, colors='k', alpha=0.7, linewidths=0.5)\n",
    "        axes[0, i].clabel(cs, inline=True, fontsize=8, fmt='%.0f')\n",
    "        axes[0, i].set_title(f'Training Image {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Add lat/lon labels to image\n",
    "        centroid = train_gdf.iloc[sample_idx].geometry.centroid\n",
    "        axes[0, i].text(1.5, 3, f\"{centroid.y:.3f}, {centroid.x:.3f}\", color='black',\n",
    "                        bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "        axes[0, i].grid(True, linestyle='--', alpha=0.8, color='k')\n",
    "\n",
    "        # Plot DEM\n",
    "        im = axes[1, i].imshow(dem, cmap='terrain')\n",
    "        axes[1, i].set_title(f'Elevation Image {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "        seen_classes.update(np.unique(train_images[sample_idx]))\n",
    "        \n",
    "    # Add colorbar for elevation below the subplots\n",
    "    cbar_ax = fig.add_axes([0.15, 0.12, 0.7, 0.02])\n",
    "    fig.colorbar(im, cax=cbar_ax, orientation='horizontal', label='Elevation, relative to minimum (m)')\n",
    "\n",
    "    # Add legend for NLCD classes above the subplots\n",
    "    legend_handles = [mpatches.Patch(color=classes_df.loc[idx, \"RGB\"], \n",
    "                                     label=classes_df.loc[idx, \"name\"]) \n",
    "                      for idx in seen_classes]\n",
    "    fig.legend(handles=legend_handles, loc='upper center', \n",
    "               bbox_to_anchor=(0.5, 0.99), ncol=4)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.95])\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.info(\"Skipping visualization of NLCD and DEM data; set show_plots=True to enable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Create tokenized arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding verified successfully! There are 38755 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "At this stage, we want to ge the unique DxD patches from both `train_images` and `test_images` which have shape (Ntrain, H, W) and (Ntest, H, W) respectively with integer datatype.\n",
    "D is the token downsampling ratio here, so a ratio of D means that each patch is DxD and the shape of the tokenized image is (H // D, W // D). At the end, we\n",
    "need the `train_images_tokenized`, `test_images_tokenized` and the `decode_table`(shape (K, D, D) where K is the number of unique tokens)\n",
    "'''\n",
    "\n",
    "D = config.tokenizer_downsample_ratio\n",
    "\n",
    "# Extract all DxD patches\n",
    "def extract_patches(images, D):\n",
    "    N, H, W = images.shape\n",
    "    patches = images.reshape(N, H//D, D, W//D, D).transpose(0,1,3,2,4).reshape(-1, D, D)\n",
    "    return patches\n",
    "\n",
    "# Get unique patches and create decode table\n",
    "all_patches = np.vstack([extract_patches(train_images, D), extract_patches(test_images, D)])\n",
    "decode_table, inverse = np.unique(all_patches.reshape(len(all_patches), -1), \n",
    "                                  axis=0, return_inverse=True)\n",
    "decode_table = decode_table.reshape(-1, D, D)\n",
    "\n",
    "# Tokenize images\n",
    "n_train_patches = (train_images.shape[0] * train_images.shape[1] * train_images.shape[2]) // (D * D)\n",
    "train_tokens = inverse[:n_train_patches]\n",
    "test_tokens = inverse[n_train_patches:]\n",
    "\n",
    "# Reshape to tokenized images\n",
    "train_images_tokenized = train_tokens.reshape(train_images.shape[0], train_images.shape[1]//D, train_images.shape[2]//D)\n",
    "test_images_tokenized = test_tokens.reshape(test_images.shape[0], test_images.shape[1]//D, test_images.shape[2]//D)\n",
    "\n",
    "# Decode and verify first 3 images\n",
    "def decode_images(tokenized, decode_table, D):\n",
    "    N, th, tw = tokenized.shape\n",
    "    decoded = decode_table[tokenized.flatten()].reshape(N, th, tw, D, D)\n",
    "    return decoded.transpose(0,1,3,2,4).reshape(N, th*D, tw*D)\n",
    "\n",
    "train_decoded = decode_images(train_images_tokenized[:3], decode_table, D)\n",
    "test_decoded = decode_images(test_images_tokenized[:3], decode_table, D)\n",
    "\n",
    "assert np.array_equal(train_decoded, train_images[:3]), \"Train decoding mismatch!\"\n",
    "assert np.array_equal(test_decoded, test_images[:3]), \"Test decoding mismatch!\"\n",
    "print(f\"Decoding verified successfully! There are {decode_table.shape[0]} unique tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Concatenate data and save to disk + s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Removed 0 images with missing DEM data from training set.\n",
      "[INFO] Removed 0 images with missing DEM data from test set.\n",
      "[INFO] Created 377,539 records\n",
      "[INFO] Training sample location GeoDataFrame saved to /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/train_32_ratio8.gpkg (Size: 75.54 MB)\n",
      "[INFO] Created 18,865 records\n",
      "[INFO] Test sample location GeoDataFrame saved to /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/test_32_ratio8.gpkg (Size: 3.87 MB)\n",
      "/tmp/ipykernel_996135/87776154.py:33: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  [train_gdf_final.centroid.x.values,\n",
      "/tmp/ipykernel_996135/87776154.py:34: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_gdf_final.centroid.y.values,],\n",
      "/tmp/ipykernel_996135/87776154.py:38: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  [test_gdf_final.centroid.x.values,\n",
      "/tmp/ipykernel_996135/87776154.py:39: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  test_gdf_final.centroid.y.values],\n",
      "[INFO] Saved training and test data to /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/data_size32_ratio8.npz (Size: 493.11 MB)\n",
      "[INFO] Uploading /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/data_size32_ratio8.npz to S3 bucket lc-inpaint...\n",
      "[ERROR] Failed to upload /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/data_size32_ratio8.npz to S3 bucket lc-inpaint: Failed to upload /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/data_size32_ratio8.npz to lc-inpaint/data_size32_ratio8.npz: An error occurred (InvalidAccessKeyId) when calling the CreateMultipartUpload operation: The AWS Access Key Id you provided does not exist in our records.\n",
      "[INFO] Uploading /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/train_32_ratio8.gpkg to S3 bucket lc-inpaint...\n",
      "[ERROR] Failed to upload /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/train_32_ratio8.gpkg to S3 bucket lc-inpaint: Failed to upload /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/train_32_ratio8.gpkg to lc-inpaint/train_32_ratio8.gpkg: An error occurred (InvalidAccessKeyId) when calling the CreateMultipartUpload operation: The AWS Access Key Id you provided does not exist in our records.\n",
      "[INFO] Uploading /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/test_32_ratio8.gpkg to S3 bucket lc-inpaint...\n",
      "[ERROR] Failed to upload /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/test_32_ratio8.gpkg to S3 bucket lc-inpaint: Failed to upload /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/test_32_ratio8.gpkg to lc-inpaint/test_32_ratio8.gpkg: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\n"
     ]
    }
   ],
   "source": [
    "if save_data:\n",
    "    is_image_bad_train = np.any(np.isnan(train_dem_images), axis=(1, 2))\n",
    "    is_image_kept_train = ~is_image_bad_train\n",
    "\n",
    "    train_gdf_final = train_gdf[is_image_kept_train]\n",
    "    logging.info(f\"Removed {is_image_bad_train.sum()} images with missing DEM data from training set.\")\n",
    "\n",
    "    is_image_bad_test = np.any(np.isnan(test_dem_images), axis=(1, 2))\n",
    "    is_image_kept_test = ~is_image_bad_test\n",
    "    test_gdf_final = test_gdf[is_image_kept_test]\n",
    "    logging.info(f\"Removed {is_image_bad_test.sum()} images with missing DEM data from test set.\")\n",
    "\n",
    "    train_gdf_final.to_crs('EPSG:4326', inplace=True)\n",
    "    test_gdf_final.to_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "    # Take arrays of shape (N, H, W) and stack them along the channel axis\n",
    "    # which needs to be created for both data sets\n",
    "    train_combined = np.stack([train_images[is_image_kept_train], train_dem_images[is_image_kept_train]], axis=1)\n",
    "    test_combined = np.stack([test_images[is_image_kept_test],  test_dem_images[is_image_kept_test]], axis=1)\n",
    "\n",
    "    train_gdf_path = config.output_path_train_gpkg\n",
    "    train_gdf_final.to_file(train_gdf_path, driver='GPKG')\n",
    "    train_gpkg_size = os.path.getsize(train_gdf_path)\n",
    "    logging.info(f\"Training sample location GeoDataFrame saved to {train_gdf_path} (Size: {train_gpkg_size / (1024 * 1024):.2f} MB)\")\n",
    "\n",
    "    test_gdf_path = config.output_path_test_gpkg\n",
    "    test_gdf_final.to_file(test_gdf_path, driver='GPKG')\n",
    "    test_gpkg_size = os.path.getsize(test_gdf_path)\n",
    "    logging.info(f\"Test sample location GeoDataFrame saved to {test_gdf_path} (Size: {test_gpkg_size / (1024 * 1024):.2f} MB)\")\n",
    "\n",
    "    # Save the lat-long coordinates of the training and test samples\n",
    "    train_coords = np.stack(\n",
    "        [train_gdf_final.centroid.x.values,\n",
    "        train_gdf_final.centroid.y.values,],\n",
    "    axis=1)\n",
    "\n",
    "    test_coords = np.stack(\n",
    "        [test_gdf_final.centroid.x.values,\n",
    "        test_gdf_final.centroid.y.values],\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "    np.savez_compressed(config.output_path, train_data=train_combined, test_data=test_combined, train_coords=train_coords, test_coords=test_coords,\n",
    "    train_data_tokenized=train_images_tokenized, test_data_tokenized=test_images_tokenized, decode_table=decode_table)\n",
    "    logging.info(f\"Saved training and test data to {config.output_path} (Size: {os.path.getsize(config.output_path) / (1024 * 1024):.2f} MB)\")\n",
    "    if config.upload_to_s3:\n",
    "        import boto3\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket_name = BUCKET_NAME\n",
    "        files_to_upload = [\n",
    "            config.output_path,\n",
    "            train_gdf_path,\n",
    "            test_gdf_path, \n",
    "        ]\n",
    "\n",
    "        for file in files_to_upload:\n",
    "            logging.info(f\"Uploading {file} to S3 bucket {bucket_name}...\")\n",
    "            try:\n",
    "                s3.upload_file(str(file), bucket_name, file.name)\n",
    "                logging.info(f\"Uploaded {file} to S3 bucket {bucket_name} as {file.name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to upload {file} to S3 bucket {bucket_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show large, random sample of LULC images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lulc_sample = False\n",
    "\n",
    "if plot_lulc_sample:\n",
    "    fig, axes = plt.subplots(10, 16, figsize=(48, 30))\n",
    "    axes = axes.flatten()\n",
    "    for i in range(16*10):\n",
    "        ax = axes[i]\n",
    "        random_index = np.random.randint(0, train_combined.shape[0])\n",
    "        image = train_combined[random_index, 0].astype(np.uint8)\n",
    "        ax.imshow(lut[image])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Create animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'train_images_final' in locals():\n",
    "    train_images_final = np.load(config.output_path)['train_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_animation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_animation:\n",
    "    class TerrainAnimator:\n",
    "        def __init__(self, train_images, train_dem_images, lut, n_rows=4, n_cols=8):\n",
    "            self.train_images = train_images\n",
    "            self.train_dem_images = train_dem_images\n",
    "            self.lut = lut\n",
    "            self.n_rows = n_rows\n",
    "            self.n_cols = n_cols\n",
    "            self.exaggeration = 1\n",
    "            \n",
    "            # Pre-calculate mesh grid\n",
    "            self.h, self.w = train_images[0].shape\n",
    "            x = np.arange(self.w)\n",
    "            y = np.arange(self.h)\n",
    "            self.X, self.Y = np.meshgrid(x, y)\n",
    "            \n",
    "            # Initialize figure\n",
    "            self.setup_figure()\n",
    "            \n",
    "        def setup_figure(self):\n",
    "            plt.rcParams['figure.dpi'] = 300\n",
    "            self.fig, self.axes = plt.subplots(\n",
    "                self.n_rows, \n",
    "                self.n_cols, \n",
    "                figsize=(self.n_cols*1.4, self.n_rows*1.6),  # Reduced figure size\n",
    "                subplot_kw={'projection': '3d'},\n",
    "                constrained_layout=True  # Use constrained layout\n",
    "            )\n",
    "            self.fig.set_facecolor('black')\n",
    "            self.fig.patch.set_alpha(1.0)\n",
    "            # Reduce margins\n",
    "            plt.subplots_adjust(left=0.02, right=0.98, bottom=0.02, top=0.98)\n",
    "            \n",
    "            # Select random indices once\n",
    "            self.indices = np.random.choice(\n",
    "                len(self.train_images), \n",
    "                self.n_rows * self.n_cols, \n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "        def process_elevation(self, elevation):\n",
    "            \"\"\"Pre-process elevation data with Gaussian smoothing\"\"\"\n",
    "            return cv2.GaussianBlur(elevation, (3, 3), 0)\n",
    "            \n",
    "        def create_surface(self, ax, idx):\n",
    "            \"\"\"Create a single surface plot\"\"\"\n",
    "            land_cover = self.train_images[idx]\n",
    "            elevation = self.process_elevation(self.train_dem_images[idx])\n",
    "            \n",
    "            surf = ax.plot_surface(\n",
    "                self.X, self.Y,\n",
    "                elevation * self.exaggeration,\n",
    "                facecolors=self.lut[land_cover],\n",
    "                shade=False,\n",
    "                antialiased=False,\n",
    "                rstride=1,\n",
    "                cstride=1\n",
    "            )\n",
    "\n",
    "            ax.set_facecolor('black')\n",
    "            \n",
    "            # Configure view\n",
    "            ax.view_init(elev=30, azim=45)\n",
    "            ax.set_box_aspect([1, 1, 0.5])\n",
    "            \n",
    "            # Remove unnecessary elements\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_zticks([])\n",
    "            ax.grid(False)\n",
    "            ax.axis('off')\n",
    "\n",
    "            ele_max = elevation.max()\n",
    "\n",
    "            if ele_max < 100:\n",
    "                zlim = 150\n",
    "            elif ele_max < 200:\n",
    "                zlim = 250\n",
    "            else:\n",
    "                zlim = max(300, ele_max * 3)\n",
    "\n",
    "            ax.set_zlim(0, zlim)\n",
    "            \n",
    "            return surf\n",
    "            \n",
    "        def setup_plots(self):\n",
    "            \"\"\"Initialize all surface plots in parallel\"\"\"\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                self.surfaces = list(executor.map(\n",
    "                    lambda args: self.create_surface(*args),\n",
    "                    zip(self.axes.flatten(), self.indices)\n",
    "                ))\n",
    "            \n",
    "            plt.subplots_adjust(hspace=-0.6, wspace=-0.2)  # Increased overlap between subplots\n",
    "            \n",
    "        def update(self, frame):\n",
    "            \"\"\"Animation update function\"\"\"\n",
    "            for ax in self.axes.flatten():\n",
    "                ax.view_init(elev=30, azim=frame)\n",
    "            return self.surfaces\n",
    "            \n",
    "        def create_animation(self, frames=360, fps=30, out_path=config.output_path_animation):\n",
    "            \"\"\"Create and save the animation\"\"\"\n",
    "            self.setup_plots()\n",
    "            \n",
    "            anim = FuncAnimation(\n",
    "                self.fig,\n",
    "                self.update,\n",
    "                frames=frames,\n",
    "                interval=1000/fps,\n",
    "                blit=True\n",
    "            )\n",
    "            \n",
    "            # Save with optimized settings\n",
    "            anim.save(\n",
    "                out_path,\n",
    "                writer='pillow',\n",
    "                fps=fps,\n",
    "                savefig_kwargs={'facecolor': 'black'},\n",
    "                progress_callback=lambda i, n: print(f'Saving frame {i}/{n}', end='\\r')\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    # Usage\n",
    "    train_images_final[:,0]\n",
    "    animator = TerrainAnimator(\n",
    "        train_images_final[:,0], train_images_final[:,1], lut,\n",
    "        n_rows=4, n_cols=4\n",
    "    )\n",
    "    animator.create_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"terrain_rotation.gif\" width=\"1500\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                      Type              Data/Info\n",
      "---------------------------------------------------------\n",
      "BUCKET_NAME                   str               lc-inpaint\n",
      "CRS                           type              <class 'pyproj.crs.crs.CRS'>\n",
      "D                             int               2\n",
      "DataPrepConfig                type              <class '__main__.DataPrepConfig'>\n",
      "FuncAnimation                 type              <class 'matplotlib.animation.FuncAnimation'>\n",
      "HTML                          type              <class 'IPython.core.display.HTML'>\n",
      "N_test                        int               660\n",
      "N_train                       int               13293\n",
      "Path                          type              <class 'pathlib.Path'>\n",
      "Resampling                    EnumMeta          <enum 'Resampling'>\n",
      "ThreadPoolExecutor            type              <class 'concurrent.future<...>read.ThreadPoolExecutor'>\n",
      "Transformer                   type              <class 'pyproj.transformer.Transformer'>\n",
      "Tuple                         _TupleType        typing.Tuple\n",
      "all_patches                   ndarray           101479424x2x2: 405917696 elems, type `uint8`, 405917696 bytes (387.11328125 Mb)\n",
      "available_memory              int               64614408192\n",
      "boto3                         module            <module 'boto3' from '/mn<...>kages/boto3/__init__.py'>\n",
      "bounds                        BoundingBox       BoundingBox(left=-2493045<...>2342655.0, top=3310005.0)\n",
      "box                           function          <function box at 0x7f79e71b7520>\n",
      "bucket_name                   str               lc-inpaint\n",
      "calculate_default_transform   function          <function calculate_defau<...>nsform at 0x7f79b37e7880>\n",
      "check_overlap                 function          <function check_overlap at 0x7f7a36a73e20>\n",
      "classes_df                    DataFrame                class_value       <...>  (0.439, 0.639, 0.729)  \n",
      "cls_id                        uint8             95\n",
      "code                          int               95\n",
      "color                         tuple             n=3\n",
      "compute_block_counts          function          <function compute_block_counts at 0x7f7a34b49cf0>\n",
      "compute_class_distribution    function          <function compute_class_d<...>bution at 0x7f79b944d120>\n",
      "config                        DataPrepConfig    DataPrepConfig(bbox_west=<...>tio=2, upload_to_s3=True)\n",
      "current_path                  PosixPath         /mnt/m2ssd/data/Dropbox/research/lc-gpt/scripts\n",
      "cv2                           module            <module 'cv2' from '/mnt/<...>ackages/cv2/__init__.py'>\n",
      "data_crs                      CRS               PROJCS[\"Albers_Conical_Eq<...>],AXIS[\"Northing\",NORTH]]\n",
      "dataclass                     function          <function dataclass at 0x7f7a51750e50>\n",
      "decode_images                 function          <function decode_images at 0x7f79b98aa0e0>\n",
      "decode_table                  ndarray           38755x2x2: 155020 elems, type `uint8`, 155020 bytes (151.38671875 kb)\n",
      "dem_data                      ndarray           972x1980: 1924560 elems, type `int16`, 3849120 bytes (3.670806884765625 Mb)\n",
      "dem_src                       DatasetReader     <closed DatasetReader nam<...>_conus_dem.tif' mode='r'>\n",
      "display                       function          <function display at 0x7f7a50fd3e20>\n",
      "downsample_patch              function          <function downsample_patch at 0x7f7a21d5ea70>\n",
      "downsample_stride             int               100\n",
      "ds_bottom                     float             21.742307778016638\n",
      "ds_left                       float             -119.78610533604079\n",
      "ds_right                      float             -63.672191850655295\n",
      "ds_top                        float             49.177063191389536\n",
      "dtype_size                    int               1\n",
      "elevation                     module            <module 'elevation' from <...>s/elevation/__init__.py'>\n",
      "extract_dem_images            function          <function extract_dem_images at 0x7f79b7ac5e10>\n",
      "extract_patches               function          <function extract_patches at 0x7f7a21db3910>\n",
      "extract_patches_vectorized    function          <function extract_patches<...>orized at 0x7f79b95b7d00>\n",
      "fig                           Figure            Figure(2000x400)\n",
      "file                          PosixPath         /mnt/m2ssd/data/Dropbox/r<...>/data/test_32_ratio8.gpkg\n",
      "files_to_upload               list              n=3\n",
      "from_working_crs              Transformer       proj=pipeline step proj=u<...>5 x_0=0 y_0=0 ellps=WGS84\n",
      "geometry_mask                 function          <function geometry_mask at 0x7f79b37e7c70>\n",
      "get_pixel_bounds              function          <function get_pixel_bounds at 0x7f79b944dc60>\n",
      "glob                          module            <module 'glob' from '/usr<...>/lib/python3.10/glob.py'>\n",
      "gpd                           module            <module 'geopandas' from <...>s/geopandas/__init__.py'>\n",
      "grid_cells                    list              n=2500\n",
      "grid_gdf                      GeoDataFrame                               <...>\\n[2500 rows x 3 columns]\n",
      "i                             int               49\n",
      "inverse                       ndarray           101479424: 101479424 elems, type `int64`, 811835392 bytes (774.2265625 Mb)\n",
      "inverse_indices               ndarray           223248: 223248 elems, type `int64`, 1785984 bytes (1.7032470703125 Mb)\n",
      "is_image_bad_test             ndarray           18865: 18865 elems, type `bool`, 18865 bytes\n",
      "is_image_bad_train            ndarray           377539: 377539 elems, type `bool`, 377539 bytes (368.6904296875 kb)\n",
      "is_image_kept_test            ndarray           18865: 18865 elems, type `bool`, 18865 bytes\n",
      "is_image_kept_train           ndarray           377539: 377539 elems, type `bool`, 377539 bytes (368.6904296875 kb)\n",
      "is_within_bbox                function          <function is_within_bbox at 0x7f79b3801870>\n",
      "j                             int               49\n",
      "load_dotenv                   function          <function load_dotenv at 0x7f79b37e65f0>\n",
      "logging                       module            <module 'logging' from '/<...>.10/logging/__init__.py'>\n",
      "lut                           ndarray           100x3: 300 elems, type `float64`, 2400 bytes\n",
      "make_animation                bool              False\n",
      "mapping                       function          <function mapping at 0x7f79e71b7640>\n",
      "mask                          function          <function mask at 0x7f79b37e6d40>\n",
      "max_elements                  int               32307204096\n",
      "merge                         module            <module 'rasterio.merge' <...>kages/rasterio/merge.py'>\n",
      "merged_src                    DatasetReader     <closed DatasetReader nam<...>_conus_dem.tif' mode='r'>\n",
      "mode                          function          <function mode at 0x7f79b3ec09d0>\n",
      "mp                            module            <module 'multiprocessing'<...>iprocessing/__init__.py'>\n",
      "mpatches                      module            <module 'matplotlib.patch<...>s/matplotlib/patches.py'>\n",
      "n_cells                       int               2500\n",
      "n_images                      int               3\n",
      "n_test                        int               125\n",
      "n_tokens                      int               5807\n",
      "n_train_patches               int               96649984\n",
      "np                            module            <module 'numpy' from '/mn<...>kages/numpy/__init__.py'>\n",
      "num_train_patches             int               212688\n",
      "os                            module            <module 'os' from '/usr/lib/python3.10/os.py'>\n",
      "palette_series                Series            class\\n0      (0.278, 0.4<...>nName: RGB, dtype: object\n",
      "parent_path                   PosixPath         /mnt/m2ssd/data/Dropbox/research/lc-gpt\n",
      "partial                       type              <class 'functools.partial'>\n",
      "patch_dims                    tuple             n=2\n",
      "patch_size                    int               2\n",
      "patch_to_token                dict              n=5807\n",
      "patches_per_dim               int               4\n",
      "pct                           float64           0.014275540387052728\n",
      "pd                            module            <module 'pandas' from '/m<...>ages/pandas/__init__.py'>\n",
      "pickle                        module            <module 'pickle' from '/u<...>ib/python3.10/pickle.py'>\n",
      "plot_lulc_sample              bool              False\n",
      "plt                           module            <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "polygon                       dict              n=2\n",
      "process_cell                  function          <function process_cell at 0x7f7a34b49000>\n",
      "psutil                        module            <module 'psutil' from '/m<...>ages/psutil/__init__.py'>\n",
      "qmc                           module            <module 'scipy.stats.qmc'<...>ages/scipy/stats/qmc.py'>\n",
      "rasterio                      module            <module 'rasterio' from '<...>es/rasterio/__init__.py'>\n",
      "reconstruct_image             function          <function reconstruct_image at 0x7f79b9519990>\n",
      "reproject                     function          <function reproject at 0x7f79b37e76d0>\n",
      "s3                            S3                <botocore.client.S3 object at 0x7f7957ae6800>\n",
      "sample_images_parallel        function          <function sample_images_p<...>rallel at 0x7f79a2dd2e60>\n",
      "samples_x                     float             629.6484375\n",
      "samples_y                     float             407.90625\n",
      "save_data                     bool              True\n",
      "seen_classes                  set               set()\n",
      "show_encode_decode_examples   function          <function show_encode_dec<...>amples at 0x7f79b9518d30>\n",
      "sobol_indices                 ndarray           125: 125 elems, type `int64`, 1000 bytes\n",
      "sobol_points                  ndarray           125x1: 125 elems, type `float64`, 1000 bytes\n",
      "src                           DatasetReader     <closed DatasetReader nam<...>_conus_dem.tif' mode='r'>\n",
      "subprocess                    module            <module 'subprocess' from<...>ython3.10/subprocess.py'>\n",
      "tempfile                      module            <module 'tempfile' from '<...>/python3.10/tempfile.py'>\n",
      "test_combined                 ndarray           18865x2x32x32: 38635520 elems, type `uint8`, 38635520 bytes (36.845703125 Mb)\n",
      "test_coords                   ndarray           18865x2: 37730 elems, type `float64`, 301840 bytes (294.765625 kb)\n",
      "test_decoded                  ndarray           3x32x32: 3072 elems, type `uint8`, 3072 bytes\n",
      "test_dem_images               ndarray           18865x32x32: 19317760 elems, type `uint8`, 19317760 bytes (18.4228515625 Mb)\n",
      "test_dist                     dict              n=16\n",
      "test_gdf                      GeoDataFrame                               <...>n[18865 rows x 1 columns]\n",
      "test_gdf_final                GeoDataFrame                               <...>n[18865 rows x 1 columns]\n",
      "test_gdf_path                 PosixPath         /mnt/m2ssd/data/Dropbox/r<...>/data/test_32_ratio8.gpkg\n",
      "test_gpkg_size                int               4059136\n",
      "test_images                   ndarray           18865x32x32: 19317760 elems, type `uint8`, 19317760 bytes (18.4228515625 Mb)\n",
      "test_images_tokenized         ndarray           18865x16x16: 4829440 elems, type `int64`, 38635520 bytes (36.845703125 Mb)\n",
      "test_patches                  ndarray           10560x4: 42240 elems, type `uint8`, 42240 bytes\n",
      "test_tokens                   ndarray           4829440: 4829440 elems, type `int64`, 38635520 bytes (36.845703125 Mb)\n",
      "to_working_crs                Transformer       proj=pipeline step inv pr<...>vert xy_in=rad xy_out=deg\n",
      "total_counts                  dict              n=16\n",
      "tqdm                          type              <class 'tqdm.notebook.tqdm_notebook'>\n",
      "train_combined                ndarray           377539x2x32x32: 773199872 elems, type `uint8`, 773199872 bytes (737.380859375 Mb)\n",
      "train_coords                  ndarray           377539x2: 755078 elems, type `float64`, 6040624 bytes (5.7607879638671875 Mb)\n",
      "train_decoded                 ndarray           3x32x32: 3072 elems, type `uint8`, 3072 bytes\n",
      "train_dem_images              ndarray           377539x32x32: 386599936 elems, type `uint8`, 386599936 bytes (368.6904296875 Mb)\n",
      "train_dist                    dict              n=16\n",
      "train_gdf                     GeoDataFrame                               <...>[377539 rows x 1 columns]\n",
      "train_gdf_final               GeoDataFrame                               <...>[377539 rows x 1 columns]\n",
      "train_gdf_path                PosixPath         /mnt/m2ssd/data/Dropbox/r<...>data/train_32_ratio8.gpkg\n",
      "train_gpkg_size               int               79212544\n",
      "train_images                  ndarray           377539x32x32: 386599936 elems, type `uint8`, 386599936 bytes (368.6904296875 Mb)\n",
      "train_images_final            ndarray           1295560x2x8x8: 165831680 elems, type `uint8`, 165831680 bytes (158.1494140625 Mb)\n",
      "train_images_tokenized        ndarray           377539x16x16: 96649984 elems, type `int64`, 773199872 bytes (737.380859375 Mb)\n",
      "train_patches                 ndarray           212688x4: 850752 elems, type `uint8`, 850752 bytes (830.8125 kb)\n",
      "train_tokens                  ndarray           96649984: 96649984 elems, type `int64`, 773199872 bytes (737.380859375 Mb)\n",
      "transform_bounds              function          <function transform_bounds at 0x7f79b37e75b0>\n",
      "unique_patches                ndarray           5807x4: 23228 elems, type `uint8`, 23228 bytes\n",
      "working_crs                   CRS               EPSG:4326\n",
      "x_edges                       ndarray           51: 51 elems, type `float64`, 408 bytes\n",
      "y_edges                       ndarray           51: 51 elems, type `float64`, 408 bytes\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 12. Base Footprint Extraction\n",
    "\n",
    "Extract NLCD data around base borders with configurable buffer radius.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
