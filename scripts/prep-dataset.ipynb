{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook contains a data cleaning pipeline for preparing a dataset for land cover data overlaid on digital elevation data. \n",
    "\n",
    "The expected outputs are:\n",
    "- Train and test files for image data, with shape (N, 2, H, W) with the first channel for LULC and second channel for DEM\n",
    "- Train and test geopackage files containing the bounding boxes for each image\n",
    "\n",
    "The main steps in this notebook are:\n",
    "- Splitting up the domain into adjacent grid cells and extracting land cover data\n",
    "- Assigning portions of the domain into train and test splits using Sobol sequences\n",
    "- Downloading and merging DEM files for the same domain\n",
    "- Extracting DEM data for each land cover image\n",
    "- Saving these data to disk\n",
    "- Creating an animation of the final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elevation : 1.1.3\n",
      "rasterio  : 1.3.11\n",
      "scipy     : 1.14.1\n",
      "pandas    : 2.2.3\n",
      "geopandas : 1.1.0\n",
      "matplotlib: 3.9.2\n",
      "pyproj    : 3.7.1\n",
      "tqdm      : 4.66.5\n",
      "cv2       : 4.10.0\n",
      "dotenv    : 0.9.9\n",
      "logging   : 0.5.1.2\n",
      "psutil    : 7.0.0\n",
      "shapely   : 2.1.1\n",
      "numpy     : 1.26.4\n",
      "IPython   : 8.37.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import elevation\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import rasterio\n",
    "import pickle\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "from pyproj import CRS, Transformer\n",
    "from scipy.stats import mode\n",
    "from shapely.geometry import box\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple\n",
    "from dotenv import load_dotenv\n",
    "from shapely.geometry import mapping\n",
    "from rasterio.mask import mask\n",
    "from rasterio.warp import transform_bounds, calculate_default_transform, reproject, Resampling\n",
    "from rasterio.features import geometry_mask\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "import logging\n",
    "\n",
    "BUCKET_NAME = \"lc-inpaint\"\n",
    "logging.getLogger('boto3').setLevel(logging.WARNING)\n",
    "logging.getLogger('botocore').setLevel(logging.WARNING)\n",
    "logging.getLogger('s3transfer').setLevel(logging.WARNING)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -iv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setting up the config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# These can be manipulated by Papermill when running\n",
    "# from the command line\n",
    "image_size = 64\n",
    "downsample_ratio = 2\n",
    "n_samples = 10_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Setting project data directory to /mnt/m2ssd/data/Dropbox/research/lc-gpt/data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_path = Path.cwd()\n",
    "parent_path = current_path.parent\n",
    "\n",
    "@dataclass\n",
    "class DataPrepConfig:\n",
    "    logging_level = logging.INFO\n",
    "\n",
    "    # Geographic bounds (WGS84 coordinates)\n",
    "    bbox_west: float = -119.0\n",
    "    bbox_east: float = -64.0\n",
    "    bbox_south: float = 22.0\n",
    "    bbox_north: float = 49.0\n",
    "\n",
    "    # Sampling parameters\n",
    "    image_size: int = 64  # Size of output images\n",
    "    downsample_ratio: int = downsample_ratio\n",
    "    max_fraction_reject_class: float = 0.9  # Maximum fraction of pixels allowed in a reject-eligible class\n",
    "    area_fraction_test: float = 0.05  # Fraction of area to reserve for testing\n",
    "    n_samples: int = n_samples\n",
    "    n_grid_unit: int = 50  # discrete units in each dimension for gridding the domain into discrete units\n",
    "    n_samples_per_cell: int = n_samples // (n_grid_unit ** 2)\n",
    "\n",
    "    # Data paths\n",
    "    data_dir: Path = parent_path / 'data'\n",
    "    dem_dir = Path(data_dir) / 'dem'\n",
    "\n",
    "    nlcd_path: Path = data_dir / 'nlcd_2021_land_cover_l48_20230630.img' # Make sure you have this file before you start\n",
    "    output_path: Path  = data_dir / f'data_size{image_size}_ratio{downsample_ratio}.npz'\n",
    "    output_path_test_gpkg: Path = data_dir / f\"test_{image_size}_ratio{downsample_ratio}.gpkg\"\n",
    "    output_path_train_gpkg: Path = data_dir / f\"train_{image_size}_ratio{downsample_ratio}.gpkg\"\n",
    "    split_save_path: Path = data_dir / f\"split_{image_size}_ratio{downsample_ratio}.gpkg\"\n",
    "    merged_dem_path: Path = dem_dir / f\"merged_conus_dem.tif\"\n",
    "    output_path_animation: Path = data_dir / f\"animation_{image_size}_ratio{downsample_ratio}.gif\"\n",
    "    \n",
    "    # Processing parameters\n",
    "    random_seed: int = 827  # Random seed for reproducibility\n",
    "    recompute_counts: bool = False  # Whether to recompute class counts\n",
    "    show_plots: bool = False  # Whether to display plots\n",
    "    \n",
    "    # CRS parameters\n",
    "    working_crs: str = 'EPSG:4326'  # CRS for geographic operations (WGS84)         \n",
    "    \n",
    "    nlcd_original_classes_for_reject = {11}\n",
    "    nlcd_original_unknown_class = 0\n",
    "\n",
    "    # Parameters for DEM processing\n",
    "    dem_nodata_threshold: float = 0.25\n",
    "    dem_product: str = 'SRTM1' # Choices are 'SRTM1' or 'SRTM3', lower resolution\n",
    "    download_dem: bool = False \n",
    "    merge_dem: bool = False\n",
    "    dem_elev_max: float = 4430.0 # Threshold for nodata values in DEM. Highest point in CONUS is 4421 m.\n",
    "\n",
    "    # Create a tokenizer which allows us to represent images in patches instead of single pixels\n",
    "    tokenizer_downsample_ratio: int = 2\n",
    "    tokenizer_path = data_dir / f'tokenizer.pkl'\n",
    "\n",
    "    upload_to_s3: bool = True\n",
    "\n",
    "    # Mapping from raw NLCD classes to RGB colors for visualization\n",
    "    nlcd_to_rgb  = {\n",
    "            11: (0.278, 0.420, 0.627),\n",
    "            12: (0.820, 0.867, 0.976),\n",
    "            21: (0.867, 0.788, 0.788),\n",
    "            22: (0.847, 0.576, 0.510),\n",
    "            23: (0.929, 0.0, 0.0),\n",
    "            24: (0.667, 0.0, 0.0),\n",
    "            31: (0.698, 0.678, 0.639),\n",
    "            41: (0.408, 0.667, 0.388),\n",
    "            42: (0.110, 0.388, 0.188),\n",
    "            43: (0.710, 0.788, 0.557),\n",
    "            51: (0.647, 0.549, 0.188),\n",
    "            52: (0.800, 0.729, 0.486),\n",
    "            71: (0.886, 0.886, 0.757),\n",
    "            72: (0.788, 0.788, 0.467),\n",
    "            73: (0.600, 0.757, 0.278),\n",
    "            74: (0.467, 0.678, 0.576),\n",
    "            81: (0.859, 0.847, 0.239),\n",
    "            82: (0.667, 0.439, 0.157),\n",
    "            90: (0.729, 0.847, 0.918),\n",
    "            95: (0.439, 0.639, 0.729),  \n",
    "        }\n",
    "    nlcd_to_name = {\n",
    "        11: \"Open Water\",\n",
    "        12: \"Perennial Ice/Snow\",\n",
    "        21: \"Developed, Open Space\",\n",
    "        22: \"Developed, Low Intensity\",\n",
    "        23: \"Developed, Medium Intensity\",\n",
    "        24: \"Developed, High Intensity\",\n",
    "        31: \"Barren Land (Rock/Sand/Clay)\",\n",
    "        41: \"Deciduous Forest\",\n",
    "        42: \"Evergreen Forest\",\n",
    "        43: \"Mixed Forest\",\n",
    "        51: \"Dwarf Scrub\",\n",
    "        52: \"Shrub/Scrub\",\n",
    "        71: \"Grassland/Herbaceous\",\n",
    "        72: \"Sedge/Herbaceous\",\n",
    "        73: \"Lichens\",\n",
    "        74: \"Moss\",\n",
    "        81: \"Pasture/Hay\",\n",
    "        82: \"Cultivated Crops\",\n",
    "        90: \"Woody Wetlands\",\n",
    "        95: \"Emergent Herbaceous Wetlands\"\n",
    "    }\n",
    "    \n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Validate bbox coordinates\n",
    "        if not (self.bbox_west < self.bbox_east):\n",
    "            raise ValueError(f\"Invalid bbox coordinates: bbox_west ({self.bbox_west}) should be less than bbox_east ({self.bbox_east})\")\n",
    "        if not (self.bbox_south < self.bbox_north):\n",
    "            raise ValueError(f\"Invalid bbox coordinates: bbox_south ({self.bbox_south}) should be less than bbox_north ({self.bbox_north})\")\n",
    "        \n",
    "        # Validate file paths\n",
    "        if not self.nlcd_path.is_file():\n",
    "            raise FileNotFoundError(f\"NLCD file not found at {self.nlcd_path}\")\n",
    "        if not self.data_dir.is_dir():\n",
    "            raise FileNotFoundError(f\"Data directory not found at {self.data_dir}\")\n",
    "            \n",
    "\n",
    "config = DataPrepConfig()\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='[%(levelname)s] %(message)s',\n",
    "    level=config.logging_level,\n",
    ")\n",
    "\n",
    "np.random.seed(827)\n",
    "\n",
    "logging.info(f\"Setting project data directory to {config.data_dir}\")\n",
    "\n",
    "def downsample_patch(patch: np.ndarray, ratio: int, force_divisible=False) -> np.ndarray:\n",
    "    \"\"\"Downsample a patch by taking the mode of each ratio x ratio window.\"\"\"\n",
    "    if ratio == 1:\n",
    "        return patch\n",
    "    \n",
    "    if force_divisible:\n",
    "        h, w = patch.shape\n",
    "        new_h = (h // ratio) * ratio\n",
    "        new_w = (w // ratio) * ratio\n",
    "        patch = patch[:new_h, :new_w]\n",
    "    \n",
    "    # Reshape into blocks of size ratio x ratio\n",
    "    h, w = patch.shape\n",
    "    new_h, new_w = h // ratio, w // ratio\n",
    "    \n",
    "    reshaped = patch.reshape(new_h, ratio, new_w, ratio)\n",
    "    \n",
    "    # Move the two ratio axes adjacent so each block becomes one dimension\n",
    "    # resulting shape: (new_h * new_w, ratio * ratio)\n",
    "    reshaped = reshaped.swapaxes(1, 2).reshape(new_h * new_w, ratio * ratio)\n",
    "    \n",
    "    # mode(..., axis=1) finds the most frequent value in each row\n",
    "    block_modes, _ = mode(reshaped, axis=1)\n",
    "    \n",
    "    # Reshape back to (new_h, new_w)\n",
    "    downsampled = block_modes.reshape(new_h, new_w)\n",
    "    \n",
    "    return downsampled\n",
    "\n",
    "def check_overlap(point_coords: Tuple[float, float], image_size_meters: float, \n",
    "                 grid_gdf: gpd.GeoDataFrame, split: str) -> bool:\n",
    "    \"\"\"Check if an image centered at point_coords overlaps with the specified split area.\"\"\"\n",
    "    x, y = point_coords\n",
    "    half_size = image_size_meters / 2\n",
    "    \n",
    "    # Create a box representing the image extent in working CRS\n",
    "    image_box = box(x - half_size, y - half_size,\n",
    "                   x + half_size, y + half_size)\n",
    "    \n",
    "    # Check intersection with grid cells of the opposite split\n",
    "    opposite_split = 'test' if split == 'train' else 'train'\n",
    "    opposite_cells = grid_gdf[grid_gdf['split'] == opposite_split]\n",
    "    \n",
    "    return not any(image_box.intersects(cell) for cell in opposite_cells.geometry)\n",
    "\n",
    "def is_within_bbox(x: float, y: float) -> bool:\n",
    "    \"\"\"Check if a point is within the specified bbox.\"\"\"\n",
    "    return (config.bbox_west <= x <= config.bbox_east and\n",
    "            config.bbox_south <= y <= config.bbox_north)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Information and CRS Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset CRS: PROJCS[\"Albers_Conical_Equal_Area\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"meters\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n",
      "[INFO] Dataset bounds: BoundingBox(left=-2493045.0, bottom=177285.0, right=2342655.0, top=3310005.0)\n",
      "[INFO] Dataset shape: (104424, 161190)\n",
      "[INFO] Dataset resolution: (30.0, 30.0)\n",
      "[INFO] Dataset transform: | 30.00, 0.00,-2493045.00|\n",
      "| 0.00,-30.00, 3310005.00|\n",
      "| 0.00, 0.00, 1.00|\n",
      "[INFO] \n",
      "Bounding box validation:\n",
      "[INFO] Dataset bounds (lon/lat): -119.7861, 21.7423, -63.6722, 49.1771\n",
      "[INFO] Selected bbox (lon/lat): -119.0, 22.0, -64.0, 49.0\n",
      "[INFO] Maximum number of sampled images from full dataset: 1027350\n"
     ]
    }
   ],
   "source": [
    "# Open the NLCD dataset and print basic information\n",
    "with rasterio.open(config.nlcd_path) as src:\n",
    "    logging.info(f\"Dataset CRS: {src.crs}\")\n",
    "    logging.info(f\"Dataset bounds: {src.bounds}\")\n",
    "    logging.info(f\"Dataset shape: {src.shape}\")\n",
    "    logging.info(f\"Dataset resolution: {src.res}\")\n",
    "    logging.info(f\"Dataset transform: {src.transform}\")\n",
    "    \n",
    "    # Set up CRS transformers\n",
    "    data_crs = src.crs\n",
    "    working_crs = CRS.from_string(config.working_crs)\n",
    "    \n",
    "    # Create transformers for converting between CRS\n",
    "    to_working_crs = Transformer.from_crs(data_crs, working_crs, always_xy=True)\n",
    "    from_working_crs = Transformer.from_crs(working_crs, data_crs, always_xy=True)\n",
    "    \n",
    "    # Convert dataset bounds to working CRS for validation\n",
    "    bounds = src.bounds\n",
    "    ds_left, ds_bottom = to_working_crs.transform(bounds.left, bounds.bottom)\n",
    "    ds_right, ds_top = to_working_crs.transform(bounds.right, bounds.top)\n",
    "    \n",
    "    \n",
    "    logging.info(\"\\nBounding box validation:\")\n",
    "    logging.info(f\"Dataset bounds (lon/lat): {ds_left:.4f}, {ds_bottom:.4f}, {ds_right:.4f}, {ds_top:.4f}\")\n",
    "    logging.info(f\"Selected bbox (lon/lat): {config.bbox_west}, {config.bbox_south}, {config.bbox_east}, {config.bbox_north}\")    \n",
    "    samples_x = src.shape[1] / config.downsample_ratio / config.image_size\n",
    "    samples_y = src.shape[0] / config.downsample_ratio / config.image_size\n",
    "\n",
    "    logging.info(f\"Maximum number of sampled images from full dataset: {samples_x * samples_y:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Class Counting and Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping class counts computation from raster; loading from file\n"
     ]
    }
   ],
   "source": [
    "# Function to compute class counts in a block\n",
    "def compute_block_counts(data):\n",
    "    unique, counts = np.unique(data, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "# Calculate available memory\n",
    "available_memory = psutil.virtual_memory().available\n",
    "dtype_size = np.dtype('uint8').itemsize\n",
    "max_elements = available_memory // (2 * dtype_size)  # Use half of available memory\n",
    "\n",
    "if not config.recompute_counts:\n",
    "    logging.info(f\"Skipping class counts computation from raster; loading from file\")\n",
    "else:\n",
    "    with rasterio.open(config.nlcd_path) as src:\n",
    "        # Convert bbox to pixel coordinates\n",
    "        bbox_left, bbox_bottom = from_working_crs.transform(config.bbox_west, config.bbox_south)\n",
    "        bbox_right, bbox_top = from_working_crs.transform(config.bbox_east, config.bbox_north)\n",
    "        \n",
    "        # Get pixel bounds\n",
    "        row_start, col_start = src.index(bbox_left, bbox_top)\n",
    "        row_end, col_end = src.index(bbox_right, bbox_bottom)\n",
    "        \n",
    "        # Ensure correct order\n",
    "        row_start, row_end = min(row_start, row_end), max(row_start, row_end)\n",
    "        col_start, col_end = min(col_start, col_end), max(col_start, col_end)\n",
    "        \n",
    "        # Calculate block size for the bbox region\n",
    "        bbox_height = row_end - row_start\n",
    "        bbox_width = col_end - col_start\n",
    "        total_pixels = bbox_height * bbox_width\n",
    "        n_blocks = max(1, total_pixels // max_elements)\n",
    "        block_height = bbox_height // n_blocks\n",
    "        \n",
    "        # Initialize counts dictionary\n",
    "        total_counts = {}\n",
    "        \n",
    "        # Process data in blocks within the bbox\n",
    "        for i in tqdm(range(row_start, row_end, block_height), desc='Computing class counts'):\n",
    "            # Read a block of data\n",
    "            window = rasterio.windows.Window(\n",
    "                col_start, i, \n",
    "                col_end - col_start,\n",
    "                min(block_height, row_end - i)\n",
    "            )\n",
    "            data = src.read(1, window=window)\n",
    "            \n",
    "            # Update counts\n",
    "            block_counts = compute_block_counts(data)\n",
    "            for k, v in block_counts.items():\n",
    "                total_counts[k] = total_counts.get(k, 0) + v\n",
    "\n",
    "    logging.info(f\"Unique values present in the bbox: {len(total_counts)}: {total_counts.keys()}\")\n",
    "\n",
    "    # Drop the counts which are in class 0 (Unknown)\n",
    "    _ = total_counts.pop(config.nlcd_original_unknown_class, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping class counts computation from raster; loading from file\n",
      "[INFO] Prepare lookup table for plotting with shape (100, 3)\n"
     ]
    }
   ],
   "source": [
    "if config.recompute_counts:\n",
    "\n",
    "    # Convert to DataFrame for better visualization\n",
    "    classes_df = pd.DataFrame([\n",
    "        {'class_value': k, 'count': v, 'name': config.nlcd_to_name.get(k, 'Unknown')} \n",
    "        for k, v in total_counts.items()\n",
    "    ])\n",
    "    classes_df['percentage'] = classes_df['count'] / classes_df['count'].sum() * 100\n",
    "    classes_df = classes_df.sort_values('count', ascending=False)\n",
    "\n",
    "    # Rename the index (currently unnamed) to \"class\"\n",
    "    classes_df.index.name = 'class'\n",
    "    classes_df = classes_df.sort_index()\n",
    "\n",
    "    # Load the mapping from original class codes to RGB for plotting and add to the dataframe\n",
    "    # We will use these for plotting later\n",
    "    classes_df['RGB'] = classes_df['class_value'].map(config.nlcd_to_rgb)\n",
    "    classes_df.to_parquet(Path(config.data_dir) / 'class_distribution.parquet')\n",
    "    present_classes = sorted(total_counts.keys())\n",
    "\n",
    "\n",
    "else:\n",
    "    logging.info(f\"Skipping class counts computation from raster; loading from file\")\n",
    "    classes_df = pd.read_parquet(Path(config.data_dir) / 'class_distribution.parquet')\n",
    "    total_counts = classes_df.set_index('class_value')['count'].to_dict()\n",
    "    classes_df['RGB'] = classes_df['class_value'].map(config.nlcd_to_rgb)\n",
    "\n",
    "\n",
    "palette_series = classes_df['RGB']\n",
    "lut = np.zeros((100, 3))\n",
    "for code, color in config.nlcd_to_rgb.items():\n",
    "    lut[code] = color\n",
    "\n",
    "logging.info(f\"Prepare lookup table for plotting with shape {lut.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel counts by class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Grid Creation for Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/m2ssd/data/Dropbox/research/lc-gpt/.venv/lib/python3.10/site-packages/scipy/stats/_qmc.py:958: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  sample = self._random(n, workers=workers)\n",
      "[INFO] Generated 125 test cells out of 2500 total cells and saved to grid_gdf\n",
      "[INFO] Created 2,500 records\n",
      "[INFO] Saved geodataframe for grid of train/test cells to /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/split_64_ratio2.gpkg\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import qmc  # Built into scipy, no extra installation needed\n",
    "\n",
    "# Set up grid for train/test split\n",
    "with rasterio.open(config.nlcd_path) as src:\n",
    "    # Create grid cells in working CRS using bbox\n",
    "    x_edges = np.linspace(config.bbox_west, config.bbox_east, config.n_grid_unit + 1)\n",
    "    y_edges = np.linspace(config.bbox_south, config.bbox_north, config.n_grid_unit + 1)\n",
    "    \n",
    "    # Create grid cell polygons\n",
    "    grid_cells = []\n",
    "    for i in range(len(x_edges)-1):\n",
    "        for j in range(len(y_edges)-1):\n",
    "            # Create polygon in working CRS\n",
    "            polygon = {\n",
    "                'geometry': {\n",
    "                    'type': 'Polygon',\n",
    "                    'coordinates': [[\n",
    "                        [x_edges[i], y_edges[j]],\n",
    "                        [x_edges[i+1], y_edges[j]],\n",
    "                        [x_edges[i+1], y_edges[j+1]],\n",
    "                        [x_edges[i], y_edges[j+1]],\n",
    "                        [x_edges[i], y_edges[j]]\n",
    "                    ]]\n",
    "                },\n",
    "                'properties': {'id': len(grid_cells)}\n",
    "            }\n",
    "            grid_cells.append(polygon)\n",
    "    \n",
    "    # Create GeoDataFrame in working CRS\n",
    "    grid_gdf = gpd.GeoDataFrame.from_features(grid_cells, crs=working_crs)\n",
    "    \n",
    "    # Set up Sobol sequence generator\n",
    "    n_cells = len(grid_gdf)\n",
    "    n_test = int(n_cells * config.area_fraction_test)\n",
    "    \n",
    "    # Generate Sobol sequence and scale to unique grid indices\n",
    "    sobol_points = qmc.Sobol(d=1, seed=config.random_seed).random(n=n_test)\n",
    "    sobol_indices = (sobol_points.flatten() * (n_cells - 1)).astype(int)\n",
    "    sobol_indices = np.unique(sobol_indices)\n",
    "    \n",
    "    # If we got fewer unique indices than needed, add random ones\n",
    "    if len(sobol_indices) < n_test:\n",
    "        additional_indices = np.random.choice(\n",
    "            np.setdiff1d(np.arange(n_cells), sobol_indices),\n",
    "            size=n_test - len(sobol_indices),\n",
    "            replace=False\n",
    "        )\n",
    "        sobol_indices = np.concatenate([sobol_indices, additional_indices])\n",
    "    \n",
    "    # Assign splits\n",
    "    grid_gdf['split'] = 'train'\n",
    "    grid_gdf.loc[sobol_indices, 'split'] = 'test'\n",
    "    logging.info(f\"Generated {n_test} test cells out of {n_cells} total cells and saved to grid_gdf\")\n",
    "\n",
    "# Save the grid to a GeoPackage file\n",
    "grid_gdf.to_file(config.split_save_path, driver='GPKG')\n",
    "logging.info(f\"Saved geodataframe for grid of train/test cells to {config.split_save_path}\")\n",
    "\n",
    "if config.show_plots:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    grid_gdf[grid_gdf['split'] == 'train'].plot(ax=ax, color='c', alpha=0.3)\n",
    "    grid_gdf[grid_gdf['split'] == 'test'].plot(ax=ax, color='m', alpha=0.3)\n",
    "\n",
    "    for x in x_edges:\n",
    "        ax.axvline(x, color='black', linestyle='--', alpha=0.5)\n",
    "    for y in y_edges:\n",
    "        ax.axhline(y, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Manually create legend elements\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor='c', alpha=0.3, label='Train unit'),\n",
    "        mpatches.Patch(facecolor='m', alpha=0.3, label='Test unit')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "    ax.set_title('Train/Test Grid Split')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sampling Land Cover Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Processing cells using 15 processes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee4d90b22664b6b86a24d1fb60ca7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing grid cells:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5306 train images and 263 test images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_cell(cell_data, args):\n",
    "    \"\"\"Process a single cell of data.\n",
    "    \n",
    "    Args:\n",
    "        cell_data: tuple of (cell, src_bounds, src_res, transforms)\n",
    "        args: dict containing configuration parameters\n",
    "    \"\"\"\n",
    "    cell, src_bounds, src_res, transforms = cell_data\n",
    "    to_working_crs, from_working_crs = transforms\n",
    "    \n",
    "    # Unpack configuration\n",
    "    config = args['config']\n",
    "    full_size_pixels = config.image_size * config.downsample_ratio\n",
    "    \n",
    "    bounds = cell.geometry.bounds\n",
    "    split = cell['split']\n",
    "    \n",
    "    # Convert bounds to pixel coordinates\n",
    "    bbox_left, bbox_bottom = from_working_crs.transform(bounds[0], bounds[1])\n",
    "    bbox_right, bbox_top = from_working_crs.transform(bounds[2], bounds[3])\n",
    "    \n",
    "    with rasterio.open(config.nlcd_path) as src:\n",
    "        row_start, col_start = src.index(bbox_left, bbox_top)\n",
    "        row_end, col_end = src.index(bbox_right, bbox_bottom)\n",
    "        \n",
    "        # Ensure correct order\n",
    "        row_start, row_end = min(row_start, row_end), max(row_start, row_end)\n",
    "        col_start, col_end = min(col_start, col_end), max(col_start, col_end)\n",
    "        \n",
    "        # Read the entire cell into memory\n",
    "        cell_data = src.read(1, window=rasterio.windows.Window(\n",
    "            col_start, row_start, \n",
    "            col_end - col_start, \n",
    "            row_end - row_start\n",
    "        ))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    done = False\n",
    "    for i in range(0, cell_data.shape[0] - full_size_pixels + 1, full_size_pixels // 2 ):\n",
    "        if done:\n",
    "            break\n",
    "        for j in range(0, cell_data.shape[1] - full_size_pixels + 1, full_size_pixels // 2):\n",
    "            patch = cell_data[i:i + full_size_pixels, j:j + full_size_pixels]\n",
    "            \n",
    "            # Reject if any pixels are unknown\n",
    "            if np.any(patch == config.nlcd_original_unknown_class):\n",
    "                continue\n",
    "            \n",
    "            # Check water fraction\n",
    "            reject = False\n",
    "            for c in config.nlcd_original_classes_for_reject:\n",
    "                class_fraction = np.mean(patch == c)\n",
    "                if class_fraction > config.max_fraction_reject_class:\n",
    "                    reject = True\n",
    "                    break\n",
    "            if reject:\n",
    "                continue\n",
    "            \n",
    "            # Downsample the patch\n",
    "            downsampled = downsample_patch(patch, config.downsample_ratio)\n",
    "            \n",
    "        \n",
    "            x_ul, y_ul = to_working_crs.transform(*src.xy(row_start + i, col_start + j))\n",
    "            x_lr, y_lr = to_working_crs.transform(*src.xy(\n",
    "                row_start + i + full_size_pixels, \n",
    "                col_start + j + full_size_pixels\n",
    "            ))\n",
    "            \n",
    "            bbox = box(x_ul, y_ul, x_lr, y_lr)\n",
    "            results.append((downsampled, bbox, split))\n",
    "            if config.n_samples_per_cell and len(results) >= config.n_samples_per_cell:\n",
    "                done = True\n",
    "                break\n",
    "    \n",
    "    return results\n",
    "\n",
    "def sample_images_parallel(grid_gdf, config, n_processes=None) -> tuple:\n",
    "    \"\"\"Sample and process images in parallel for either train or test set.\n",
    "    \n",
    "    Args:\n",
    "        grid_gdf: GeoDataFrame containing grid cells\n",
    "        config: Configuration object\n",
    "        n_processes: Number of processes to use (defaults to CPU count - 1)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_images, train_gdf, test_images, test_gdf)\n",
    "    \"\"\"\n",
    "    if n_processes is None:\n",
    "        n_processes = max(1, mp.cpu_count() - 1)\n",
    "    \n",
    "    # Get source metadata once\n",
    "    with rasterio.open(config.nlcd_path) as src:\n",
    "        src_bounds = src.bounds\n",
    "        src_res = src.res\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    transforms = (to_working_crs, from_working_crs)  # Assuming these are defined\n",
    "    cell_data = [(cell, src_bounds, src_res, transforms) for _, cell in grid_gdf.iterrows()]\n",
    "    \n",
    "    # Prepare static arguments\n",
    "    process_args = {\n",
    "        'config': config,\n",
    "    }\n",
    "    \n",
    "    # Create process pool and process cells in parallel\n",
    "    logging.info(f\"Processing cells using {n_processes} processes...\")\n",
    "    with mp.Pool(n_processes) as pool:\n",
    "        process_func = partial(process_cell, args=process_args)\n",
    "        results = list(tqdm(\n",
    "            pool.imap(process_func, cell_data),\n",
    "            total=len(cell_data),\n",
    "            desc=\"Processing grid cells\"\n",
    "        ))\n",
    "    \n",
    "    # Flatten results and separate train/test\n",
    "    train_images = []\n",
    "    test_images = []\n",
    "    train_bboxes = []\n",
    "    test_bboxes = []\n",
    "    \n",
    "    for cell_results in results:\n",
    "        for remapped, bbox, split in cell_results:\n",
    "            if split == 'train':\n",
    "                train_images.append(remapped)\n",
    "                train_bboxes.append(bbox)\n",
    "            else:\n",
    "                test_images.append(remapped)\n",
    "                test_bboxes.append(bbox)\n",
    "    \n",
    "    # Create GeoDataFrames for train and test bounding boxes\n",
    "    working_crs = grid_gdf.crs  # Get CRS from input GeoDataFrame\n",
    "    train_gdf = gpd.GeoDataFrame(geometry=train_bboxes, crs=working_crs)\n",
    "    test_gdf = gpd.GeoDataFrame(geometry=test_bboxes, crs=working_crs)\n",
    "    \n",
    "    return (np.array(train_images, dtype=np.uint8), train_gdf, np.array(test_images, dtype=np.uint8), test_gdf)\n",
    "\n",
    "\n",
    "train_images, train_gdf, test_images, test_gdf = sample_images_parallel(\n",
    "    grid_gdf,\n",
    "    config,\n",
    ")\n",
    "\n",
    "print(f\"Created {len(train_images)} train images and {len(test_images)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] All images are free of null / NaN values.\n",
      "[INFO] All images are the correct size.\n"
     ]
    }
   ],
   "source": [
    "# Make sure all images are in the valid integer range with no NaNs\n",
    "assert np.all(np.isfinite(train_images))\n",
    "assert np.all(np.isfinite(test_images))\n",
    "logging.info(\"All images are free of null / NaN values.\")\n",
    "\n",
    "# Check that the images are the correct size\n",
    "assert train_images.shape[1:] == (config.image_size, config.image_size), f\"Train shape: {train_images.shape} should be {(config.image_size, config.image_size)}\"\n",
    "assert test_images.shape[1:] == (config.image_size, config.image_size), f\"Test shape: {test_images.shape}, should be {(config.image_size, config.image_size)}\"\n",
    "logging.info(\"All images are the correct size.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.show_plots:\n",
    "    train_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping display of sample NLCD images. Set `show_plots` to True to display.\n"
     ]
    }
   ],
   "source": [
    "n_images = 3\n",
    "\n",
    "seen_classes = set()\n",
    "\n",
    "if config.show_plots:\n",
    "    fig, ax = plt.subplots(2, n_images, figsize=(8, 6))\n",
    "\n",
    "    for i in range(n_images):\n",
    "        for j, (images, geom, title) in enumerate(zip([train_images, test_images], [train_gdf.iloc[i].geometry, test_gdf.iloc[i].geometry], [\"Train\", \"Test\"])):\n",
    "            ax[j, i].imshow(lut[images[i]])\n",
    "            ax[j, i].set_title(f\"{title} Image {i+1}\")\n",
    "            ax[j, i].axis('off')\n",
    "\n",
    "            lat, lon = geom.centroid.xy\n",
    "            ax[j, i].text(1.5, 3, f\"{lat[0]:.3f}, {lon[0]:.3f}\", color='black', fontsize=8,\n",
    "                          bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "            seen_classes.update(np.unique(images[i]))\n",
    "\n",
    "    legend_handles = [mpatches.Patch(color=classes_df.loc[idx, \"RGB\"], label=classes_df.loc[idx, \"name\"]) for idx in seen_classes]\n",
    "\n",
    "    fig.legend(handles=legend_handles, loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.15))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.info(\"Skipping display of sample NLCD images. Set `show_plots` to True to display.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Class distribution across sampled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] \n",
      "Final class distribution (original class ID: percentage):\n",
      "[INFO] \n",
      "Training set:\n",
      "[INFO] 11 (Open Water): 4.84%\n",
      "[INFO] 12 (Perennial Ice/Snow): 0.00%\n",
      "[INFO] 21 (Developed, Open Space): 3.56%\n",
      "[INFO] 22 (Developed, Low Intensity): 1.89%\n",
      "[INFO] 23 (Developed, Medium Intensity): 0.93%\n",
      "[INFO] 24 (Developed, High Intensity): 0.28%\n",
      "[INFO] 31 (Barren Land (Rock/Sand/Clay)): 1.16%\n",
      "[INFO] 41 (Deciduous Forest): 10.18%\n",
      "[INFO] 42 (Evergreen Forest): 10.73%\n",
      "[INFO] 43 (Mixed Forest): 3.31%\n",
      "[INFO] 52 (Shrub/Scrub): 21.18%\n",
      "[INFO] 71 (Grassland/Herbaceous): 13.07%\n",
      "[INFO] 81 (Pasture/Hay): 6.14%\n",
      "[INFO] 82 (Cultivated Crops): 16.15%\n",
      "[INFO] 90 (Woody Wetlands): 5.01%\n",
      "[INFO] 95 (Emergent Herbaceous Wetlands): 1.58%\n"
     ]
    }
   ],
   "source": [
    "def compute_class_distribution(images):\n",
    "    unique, counts = np.unique(images, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    return {cls: count/total for cls, count in zip(unique, counts)}\n",
    "\n",
    "train_dist = compute_class_distribution(train_images)\n",
    "test_dist = compute_class_distribution(test_images)\n",
    "\n",
    "logging.info(\"\\nFinal class distribution (original class ID: percentage):\")\n",
    "logging.info(\"\\nTraining set:\")\n",
    "for cls_id, pct in train_dist.items():\n",
    "    logging.info(f\"{cls_id} ({config.nlcd_to_name[cls_id]}): {pct*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Downloading and matching with DEM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data using `elevation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping DEM download. Set `download_dem` to True to download.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if config.download_dem:\n",
    "    n_dem_downloads, bounds = 625, (config.bbox_west, config.bbox_south, config.bbox_east, config.bbox_north)  # should be a square number\n",
    "    # Define the bounding box for continental USA (approximate)\n",
    "    # For testing, use a sample pair of values like below:\n",
    "    # n_dem_downloads, bounds = 4, (-100.0, 28.0, -99.0, 29.0)  # should be a square number\n",
    "\n",
    "    os.makedirs(config.dem_dir, exist_ok=True)\n",
    "\n",
    "    # Calculate the number of splits in each dimension\n",
    "    n_splits = int(n_dem_downloads ** 0.5)\n",
    "\n",
    "    # Remove all files from the DEM directory\n",
    "    for file in config.dem_dir.glob('*.tif'):\n",
    "        file.unlink()\n",
    "\n",
    "    # Split bounds into a grid and download DEM data\n",
    "    with tqdm(total=n_dem_downloads, desc=\"Downloading DEM data\") as pbar:\n",
    "        for i in range(n_splits):\n",
    "            for j in range(n_splits):\n",
    "                west = bounds[0] + (bounds[2] - bounds[0]) * i / n_splits\n",
    "                east = bounds[0] + (bounds[2] - bounds[0]) * (i + 1) / n_splits\n",
    "                south = bounds[1] + (bounds[3] - bounds[1]) * j / n_splits\n",
    "                north = bounds[1] + (bounds[3] - bounds[1]) * (j + 1) / n_splits\n",
    "\n",
    "                assert west < east, f\"West {west} should be less than east {east}\"\n",
    "                assert south < north, f\"South {south} should be less than north {north}\"\n",
    "                \n",
    "                dem_save_path = config.dem_dir / f'conus_dem_{i}_{j}.tif'\n",
    "                elevation.clip(bounds=(west, south, east, north), output=dem_save_path, product=config.dem_product)\n",
    "\n",
    "                # Check the statistics on the DEM\n",
    "                with rasterio.open(dem_save_path) as src:\n",
    "                    dem_data = src.read(1)\n",
    "                    dem_nodata = src.nodata\n",
    "                    dem_stats = {\n",
    "                        'min': dem_data.min(),\n",
    "                        'max': dem_data.max(),\n",
    "                        'mean': dem_data.mean(),\n",
    "                        'nodata': dem_nodata,\n",
    "                        'nodata_fraction': np.mean(dem_data == dem_nodata)\n",
    "                    }\n",
    "                    logging.info(f\"DEM statistics for {dem_save_path}: {dem_stats}\")\n",
    "\n",
    "                pbar.update(1)\n",
    "else:\n",
    "    logging.info(\"Skipping DEM download. Set `download_dem` to True to download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge into single contiguous DEM raster file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from rasterio import merge\n",
    "\n",
    "if config.merge_dem:\n",
    "    # Create a list of all the GeoTIFF files\n",
    "    search_pattern = os.path.join(config.dem_dir, \"conus_dem_*.tif\")\n",
    "    dem_files = glob.glob(search_pattern)\n",
    "\n",
    "    src_files_to_mosaic = []\n",
    "    for file in dem_files:\n",
    "        src = rasterio.open(file)\n",
    "        src_files_to_mosaic.append(src)\n",
    "\n",
    "    mosaic, out_trans = merge.merge(src_files_to_mosaic)\n",
    "\n",
    "    # Copy the metadata from one of the input files\n",
    "    out_meta = src_files_to_mosaic[0].meta.copy()\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": mosaic.shape[1],\n",
    "        \"width\": mosaic.shape[2],\n",
    "        \"transform\": out_trans\n",
    "    })\n",
    "\n",
    "    with rasterio.open(config.merged_dem_path, \"w\", **out_meta) as dest:\n",
    "        dest.write(mosaic)\n",
    "        logging.info(f\"Merged DEM saved to {config.merged_dem_path}\")\n",
    "\n",
    "    logging.info(f\"Proportion of missing data in merged DEM: {np.mean(mosaic < config.dem_nodata_threshold):.2%}\")\n",
    "\n",
    "    for src in src_files_to_mosaic:\n",
    "        src.close()\n",
    "\n",
    "    # Delete variables to save on memory\n",
    "    del mosaic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check merged file metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset CRS: EPSG:4326\n",
      "[INFO] Dataset bounds: BoundingBox(left=-119.00013888888888, bottom=22.000138888888905, right=-64.00013888888893, top=49.00013888888889)\n",
      "[INFO] Dataset shape: (97200, 198000)\n",
      "[INFO] Dataset resolution: (0.0002777777777777776, 0.0002777777777777776)\n",
      "[INFO] Dataset transform: | 0.00, 0.00,-119.00|\n",
      "| 0.00,-0.00, 49.00|\n",
      "| 0.00, 0.00, 1.00|\n",
      "[INFO] Missing data value: -32768.0\n",
      "[INFO] Data type: ('int16',)\n"
     ]
    }
   ],
   "source": [
    "# Print basic information about the merged GeoTIFF file\n",
    "with rasterio.open(config.merged_dem_path) as merged_src:\n",
    "    logging.info(f\"Dataset CRS: {merged_src.crs}\")\n",
    "    logging.info(f\"Dataset bounds: {merged_src.bounds}\")\n",
    "    logging.info(f\"Dataset shape: {merged_src.shape}\")\n",
    "    logging.info(f\"Dataset resolution: {merged_src.res}\")\n",
    "    logging.info(f\"Dataset transform: {merged_src.transform}\")\n",
    "    logging.info(f\"Missing data value: {merged_src.nodata}\")\n",
    "    logging.info(f\"Data type: {merged_src.dtypes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show merged file as elevation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping display of merged DEM data. Set `show_plots` to True to display.\n"
     ]
    }
   ],
   "source": [
    "# Load the image and run imshow\n",
    "with rasterio.open(config.merged_dem_path) as src:\n",
    "        downsample_stride = 100\n",
    "        dem_data = src.read(1,\n",
    "            out_shape=(\n",
    "                src.count,\n",
    "                int(src.height / downsample_stride),\n",
    "                int(src.width / downsample_stride)\n",
    "            ),\n",
    "        resampling=rasterio.enums.Resampling.nearest\n",
    "    )\n",
    "if config.show_plots:\n",
    "    \n",
    "    # Calculate slope\n",
    "    x, y = np.gradient(dem_data, src.res[0], src.res[1])\n",
    "    slope = np.sqrt(x**2 + y**2)\n",
    "    log_slope = np.log10(slope + 1)  # Adding 1 to avoid log(0)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # Plot elevation\n",
    "    im1 = axes[0].imshow(dem_data, cmap='terrain', extent=(bounds[0], bounds[2], bounds[1], bounds[3]), vmin=0)\n",
    "    axes[0].set_title('Merged DEM Data')\n",
    "    axes[0].set_xlabel('Longitude')\n",
    "    axes[0].set_ylabel('Latitude')\n",
    "    cbar1 = fig.colorbar(im1, ax=axes[0], orientation='vertical', label='Elevation (meters)')\n",
    "    \n",
    "    # Plot log10 slope\n",
    "    im2 = axes[1].imshow(log_slope, cmap='viridis', extent=(bounds[0], bounds[2], bounds[1], bounds[3]))\n",
    "    axes[1].set_title('Log10 Slope')\n",
    "    axes[1].set_xlabel('Longitude')\n",
    "    axes[1].set_ylabel('Latitude')\n",
    "    cbar2 = fig.colorbar(im2, ax=axes[1], orientation='vertical', label='Log10 Slope')\n",
    "    \n",
    "    # Set the ticks to match the bounds\n",
    "    for ax in axes:\n",
    "        ax.set_xticks(np.linspace(bounds[0], bounds[2], num=3))\n",
    "        ax.set_yticks(np.linspace(bounds[1], bounds[3], num=3))\n",
    "        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1f}'))\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1f}'))\n",
    "        ax.grid(True, linestyle='--', alpha=0.8, color='k')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.info(\"Skipping display of merged DEM data. Set `show_plots` to True to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32767"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Join elevation data with land cover data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DEM images for 5306 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0b6826bb7446fca315a36e987dcf4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting DEM images:   0%|          | 0/5306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/m2ssd/data/Dropbox/research/lc-gpt/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/mnt/m2ssd/data/Dropbox/research/lc-gpt/.venv/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_1164799/3713914763.py:25: RuntimeWarning: invalid value encountered in multiply\n",
      "  dem_data = np.empty((config.image_size, config.image_size)) * np.nan\n",
      "[INFO] Number of images dropped due to nodata proportion exceeding threshold: 77 / 5306\n",
      "[INFO] Number of images with interpolation of missing values: 16 / 5306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DEM images for 263 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92545d98dd1e47ff8b42d0ea005fe0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting DEM images:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Number of images dropped due to nodata proportion exceeding threshold: 6 / 263\n",
      "[INFO] Number of images with interpolation of missing values: 0 / 263\n",
      "/tmp/ipykernel_1164799/3713914763.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  train_dem_images = train_dem_images / train_dem_images.max(axis=(1, 2), keepdims=True)\n",
      "/tmp/ipykernel_1164799/3713914763.py:61: RuntimeWarning: invalid value encountered in cast\n",
      "  train_dem_images = (train_dem_images * 255).astype(np.uint8)\n",
      "/tmp/ipykernel_1164799/3713914763.py:62: RuntimeWarning: invalid value encountered in cast\n",
      "  test_dem_images = (test_dem_images * 255).astype(np.uint8)\n"
     ]
    }
   ],
   "source": [
    "def extract_dem_images(gdf: gpd.GeoDataFrame, dem_src: rasterio.DatasetReader) -> np.ndarray[np.float32]:\n",
    "    dem_images = []\n",
    "    nodata_count = 0\n",
    "    interpolate_count = 0\n",
    "    print(f\"Extracting DEM images for {len(gdf)} samples\")\n",
    "    for _, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extracting DEM images\"):\n",
    "        \n",
    "        window = rasterio.windows.from_bounds(*row.geometry.bounds, transform=dem_src.transform) \n",
    "        \n",
    "        dem_data = dem_src.read(1, window=window)\n",
    "\n",
    "        # If the data is NaN or it's above the threshold, we set it to NaN\n",
    "        is_nodata = np.logical_or(dem_data == dem_src.nodata, dem_data>config.dem_elev_max)\n",
    "        nodata_fraction = np.mean(is_nodata)\n",
    "        dem_data = dem_data.astype(np.float32)\n",
    "\n",
    "        # If the read failed, the shape will be empty so we raise an alarm\n",
    "        # If any failure cases occur, we want the resulting DEM array to be all NaNs\n",
    "        # and have all dims with nonzero size.\n",
    "        if len(dem_data.shape) == 0:\n",
    "            dem_data  = np.empty((config.image_size, config.image_size)) * np.nan\n",
    "            logging.debug(f\"Failed to read window for row {row} with window {window}\")\n",
    "            nodata_count += 1\n",
    "        elif any([dim == 0 for dim in dem_data.shape]):\n",
    "            dem_data = np.empty((config.image_size, config.image_size)) * np.nan\n",
    "            logging.debug(f\"Window read for row {row} with bbox {row.geometry.bounds} has a zero dimension with shape {dem_data.shape}\")\n",
    "            nodata_count += 1\n",
    "        elif nodata_fraction > config.dem_nodata_threshold:            \n",
    "            dem_data *= np.nan\n",
    "            nodata_count += 1\n",
    "        elif np.any(is_nodata):\n",
    "            # Interpolate NaN values using a spatially informed method\n",
    "            dem_data = cv2.inpaint(dem_data, is_nodata.astype(np.uint8), inpaintRadius=3, flags=cv2.INPAINT_TELEA)\n",
    "            interpolate_count += 1\n",
    "\n",
    "        assert np.all(np.isnan(dem_data)) or np.nanmax(dem_data) < config.dem_elev_max, f\"DEM data contains values above nodata threshold: {dem_data.max()}\"\n",
    "\n",
    "        # Resize using cv2 to the desired image size\n",
    "        if not np.any(np.isnan(dem_data)):\n",
    "            dem_data = cv2.resize(dem_data, (config.image_size, config.image_size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        dem_images.append(dem_data)\n",
    "\n",
    "    logging.info(f\"Number of images dropped due to nodata proportion exceeding threshold: {nodata_count} / {len(gdf)}\")\n",
    "    logging.info(f\"Number of images with interpolation of missing values: {interpolate_count} / {len(gdf)}\")\n",
    "    return np.array(dem_images).astype(np.float32)\n",
    "\n",
    "with rasterio.open(config.merged_dem_path) as dem_src:\n",
    "    train_dem_images = extract_dem_images(train_gdf, dem_src)\n",
    "    test_dem_images = extract_dem_images(test_gdf, dem_src)\n",
    "\n",
    "# Offset all images to have a minimum of zero\n",
    "train_dem_images -= train_dem_images.min(axis=(1, 2), keepdims=True)\n",
    "test_dem_images -= test_dem_images.min(axis=(1, 2), keepdims=True)\n",
    "\n",
    "train_dem_images = train_dem_images / train_dem_images.max(axis=(1, 2), keepdims=True)\n",
    "test_dem_images  = test_dem_images / test_dem_images.max(axis=(1, 2), keepdims=True)\n",
    "\n",
    "\n",
    "# Case to uint8\n",
    "train_dem_images = (train_dem_images * 255).astype(np.uint8)\n",
    "test_dem_images = (test_dem_images * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of DEM values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([951636., 174731., 141340., 195397., 138147., 252975., 149563.,\n",
       "        243078., 168625., 256670., 262144., 170577., 272243., 181793.,\n",
       "        277727., 183168., 292496., 187762., 281175., 188956., 303167.,\n",
       "        308925., 199281., 293756., 226927., 302731., 206027., 318076.,\n",
       "        206692., 323331., 305466., 216875., 308346., 242166., 323241.,\n",
       "        221368., 315419., 221056., 314616., 209163., 331089., 323650.,\n",
       "        211923., 326383., 219338., 313354., 214117., 310719., 204279.,\n",
       "        379724., 296274., 201260., 301065., 204343., 292124., 205881.,\n",
       "        306719., 195252., 283505., 186673., 296956., 273607., 191792.,\n",
       "        271070., 177305., 258787., 201652., 242728., 165945., 245588.,\n",
       "        236553., 164700., 233107., 154835., 253277., 137032., 213179.,\n",
       "        140837., 191151., 124019., 193186., 178529., 112510., 173205.,\n",
       "        102963., 148261.,  96269., 133331.,  88531., 114671., 108597.,\n",
       "         61018.,  84993.,  46961.,  71310.,  36182.,  49430.,  23839.,\n",
       "         28097.,  62869.]),\n",
       " array([  0.  ,   2.55,   5.1 ,   7.65,  10.2 ,  12.75,  15.3 ,  17.85,\n",
       "         20.4 ,  22.95,  25.5 ,  28.05,  30.6 ,  33.15,  35.7 ,  38.25,\n",
       "         40.8 ,  43.35,  45.9 ,  48.45,  51.  ,  53.55,  56.1 ,  58.65,\n",
       "         61.2 ,  63.75,  66.3 ,  68.85,  71.4 ,  73.95,  76.5 ,  79.05,\n",
       "         81.6 ,  84.15,  86.7 ,  89.25,  91.8 ,  94.35,  96.9 ,  99.45,\n",
       "        102.  , 104.55, 107.1 , 109.65, 112.2 , 114.75, 117.3 , 119.85,\n",
       "        122.4 , 124.95, 127.5 , 130.05, 132.6 , 135.15, 137.7 , 140.25,\n",
       "        142.8 , 145.35, 147.9 , 150.45, 153.  , 155.55, 158.1 , 160.65,\n",
       "        163.2 , 165.75, 168.3 , 170.85, 173.4 , 175.95, 178.5 , 181.05,\n",
       "        183.6 , 186.15, 188.7 , 191.25, 193.8 , 196.35, 198.9 , 201.45,\n",
       "        204.  , 206.55, 209.1 , 211.65, 214.2 , 216.75, 219.3 , 221.85,\n",
       "        224.4 , 226.95, 229.5 , 232.05, 234.6 , 237.15, 239.7 , 242.25,\n",
       "        244.8 , 247.35, 249.9 , 252.45, 255.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_dem_images.flatten(), bins=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show DEM images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping DEM image visualization; set show_plots to True to display images.\n"
     ]
    }
   ],
   "source": [
    "# Plot several train and test images using elevation colormap\n",
    "if config.show_plots:\n",
    "    n_images = 3\n",
    "    fig, ax = plt.subplots(2, n_images, figsize=(8, 5))\n",
    "\n",
    "    # Pick random sample of train and test images to show\n",
    "    sampled_train_indices = np.random.choice(len(train_dem_images), n_images, replace=False)\n",
    "    sampled_test_indices = np.random.choice(len(test_dem_images), n_images, replace=False)\n",
    "\n",
    "    for i in range(n_images):\n",
    "        for j, (images, gdf, title, sampled_indices) in enumerate(zip(\n",
    "                [train_dem_images, test_dem_images], \n",
    "                [train_gdf, test_gdf], \n",
    "                [\"Train DEM\", \"Test DEM\"],\n",
    "                [sampled_train_indices, sampled_test_indices])):\n",
    "            \n",
    "            im = ax[j, i].imshow(images[sampled_indices[i]], cmap='terrain')\n",
    "            ax[j, i].set_title(f\"{title} Image {i+1}\")\n",
    "            ax[j, i].axis('off')\n",
    "            centroid = gdf.iloc[sampled_indices[i]].geometry.centroid\n",
    "            ax[j, i].text(1.5, 3, f\"{centroid.y:.3f}, {centroid.x:.3f}\",color='black', \n",
    "                          bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "            # Add gridlines and lat/long overlay\n",
    "            ax[j, i].grid(True, linestyle='--', alpha=0.8, color='k')\n",
    "            ax[j, i].set_xticks(np.linspace(0, config.image_size, num=3))\n",
    "            ax[j, i].set_yticks(np.linspace(0, config.image_size, num=3))\n",
    "            ax[j, i].set_xticklabels(np.linspace(centroid.x - config.image_size // 2, centroid.x + config.image_size // 2, num=3).round(2))\n",
    "            ax[j, i].set_yticklabels(np.linspace(centroid.y - config.image_size // 2, centroid.y + config.image_size // 2, num=3).round(2))\n",
    "            \n",
    "    # Add a common colorbar on the right-hand side\n",
    "    cbar_ax = fig.add_axes([1.0, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax, orientation='vertical', label='Elevation (meters)')\n",
    "\n",
    "    plt.tight_layout(rect=[0.1, 0, 1, 1])\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.info(\"Skipping DEM image visualization; set show_plots to True to display images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping visualization of NLCD and DEM data; set show_plots=True to enable.\n"
     ]
    }
   ],
   "source": [
    "# Create visualization of NLCD and DEM data\n",
    "n_images = 3\n",
    "seen_classes = set()\n",
    "if config.show_plots:\n",
    "    fig, axes = plt.subplots(2, n_images, figsize=(n_images*2.5, 8))\n",
    "\n",
    "    # Pick random sample of train images to show\n",
    "    \n",
    "    sampled_indices = np.random.choice(len(train_images), n_images, replace=False)\n",
    "\n",
    "    # Plot first n_images from training set\n",
    "    for i, sample_idx in enumerate(sampled_indices):\n",
    "\n",
    "        # Get DEM data for this image\n",
    "        dem = train_dem_images[sample_idx]\n",
    "        dem_min = dem.min()\n",
    "        dem_relative = dem - dem_min\n",
    "        \n",
    "        # Calculate contours (relative to minimum elevation)\n",
    "        levels = np.linspace(0, dem_relative.max(), 10)\n",
    "        \n",
    "        # Plot NLCD with contours\n",
    "        axes[0, i].imshow(lut[train_images[sample_idx]])\n",
    "        cs = axes[0, i].contour(dem_relative, levels=levels, colors='k', alpha=0.7, linewidths=0.5)\n",
    "        axes[0, i].clabel(cs, inline=True, fontsize=8, fmt='%.0f')\n",
    "        axes[0, i].set_title(f'Training Image {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Add lat/lon labels to image\n",
    "        centroid = train_gdf.iloc[sample_idx].geometry.centroid\n",
    "        axes[0, i].text(1.5, 3, f\"{centroid.y:.3f}, {centroid.x:.3f}\", color='black',\n",
    "                        bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "        axes[0, i].grid(True, linestyle='--', alpha=0.8, color='k')\n",
    "\n",
    "        # Plot DEM\n",
    "        im = axes[1, i].imshow(dem, cmap='terrain')\n",
    "        axes[1, i].set_title(f'Elevation Image {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "        seen_classes.update(np.unique(train_images[sample_idx]))\n",
    "        \n",
    "    # Add colorbar for elevation below the subplots\n",
    "    cbar_ax = fig.add_axes([0.15, 0.12, 0.7, 0.02])\n",
    "    fig.colorbar(im, cax=cbar_ax, orientation='horizontal', label='Elevation, relative to minimum (m)')\n",
    "\n",
    "    # Add legend for NLCD classes above the subplots\n",
    "    legend_handles = [mpatches.Patch(color=classes_df.loc[idx, \"RGB\"], \n",
    "                                     label=classes_df.loc[idx, \"name\"]) \n",
    "                      for idx in seen_classes]\n",
    "    fig.legend(handles=legend_handles, loc='upper center', \n",
    "               bbox_to_anchor=(0.5, 0.99), ncol=4)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.95])\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.info(\"Skipping visualization of NLCD and DEM data; set show_plots=True to enable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Create tokenized arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding verified successfully! There are 20601 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "At this stage, we want to ge the unique DxD patches from both `train_images` and `test_images` which have shape (Ntrain, H, W) and (Ntest, H, W) respectively with integer datatype.\n",
    "D is the token downsampling ratio here, so a ratio of D means that each patch is DxD and the shape of the tokenized image is (H // D, W // D). At the end, we\n",
    "need the `train_images_tokenized`, `test_images_tokenized` and the `decode_table`(shape (K, D, D) where K is the number of unique tokens)\n",
    "'''\n",
    "\n",
    "D = config.tokenizer_downsample_ratio\n",
    "\n",
    "# Extract all DxD patches\n",
    "def extract_patches(images, D):\n",
    "    N, H, W = images.shape\n",
    "    patches = images.reshape(N, H//D, D, W//D, D).transpose(0,1,3,2,4).reshape(-1, D, D)\n",
    "    return patches\n",
    "\n",
    "# Get unique patches and create decode table\n",
    "all_patches = np.vstack([extract_patches(train_images, D), extract_patches(test_images, D)])\n",
    "decode_table, inverse = np.unique(all_patches.reshape(len(all_patches), -1), \n",
    "                                  axis=0, return_inverse=True)\n",
    "decode_table = decode_table.reshape(-1, D, D)\n",
    "\n",
    "# Tokenize images\n",
    "n_train_patches = (train_images.shape[0] * train_images.shape[1] * train_images.shape[2]) // (D * D)\n",
    "train_tokens = inverse[:n_train_patches]\n",
    "test_tokens = inverse[n_train_patches:]\n",
    "\n",
    "# Reshape to tokenized images\n",
    "train_images_tokenized = train_tokens.reshape(train_images.shape[0], train_images.shape[1]//D, train_images.shape[2]//D)\n",
    "test_images_tokenized = test_tokens.reshape(test_images.shape[0], test_images.shape[1]//D, test_images.shape[2]//D)\n",
    "\n",
    "# Decode and verify first 3 images\n",
    "def decode_images(tokenized, decode_table, D):\n",
    "    N, th, tw = tokenized.shape\n",
    "    decoded = decode_table[tokenized.flatten()].reshape(N, th, tw, D, D)\n",
    "    return decoded.transpose(0,1,3,2,4).reshape(N, th*D, tw*D)\n",
    "\n",
    "train_decoded = decode_images(train_images_tokenized[:3], decode_table, D)\n",
    "test_decoded = decode_images(test_images_tokenized[:3], decode_table, D)\n",
    "\n",
    "assert np.array_equal(train_decoded, train_images[:3]), \"Train decoding mismatch!\"\n",
    "assert np.array_equal(test_decoded, test_images[:3]), \"Test decoding mismatch!\"\n",
    "print(f\"Decoding verified successfully! There are {decode_table.shape[0]} unique tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Concatenate data and save to disk + s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Removed 0 images with missing DEM data from training set.\n",
      "[INFO] Removed 0 images with missing DEM data from test set.\n",
      "[INFO] Created 5,306 records\n",
      "[INFO] Training sample location GeoDataFrame saved to /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/train_64_ratio2.gpkg (Size: 1.14 MB)\n",
      "[INFO] Created 263 records\n",
      "[INFO] Test sample location GeoDataFrame saved to /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/test_64_ratio2.gpkg (Size: 0.15 MB)\n",
      "/tmp/ipykernel_1164799/87776154.py:33: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  [train_gdf_final.centroid.x.values,\n",
      "/tmp/ipykernel_1164799/87776154.py:34: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_gdf_final.centroid.y.values,],\n",
      "/tmp/ipykernel_1164799/87776154.py:38: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  [test_gdf_final.centroid.x.values,\n",
      "/tmp/ipykernel_1164799/87776154.py:39: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  test_gdf_final.centroid.y.values],\n",
      "[INFO] Saved training and test data to /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/data_size64_ratio2.npz (Size: 23.77 MB)\n",
      "[INFO] Uploading /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/data_size64_ratio2.npz to S3 bucket lc-inpaint...\n",
      "[INFO] Uploaded /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/data_size64_ratio2.npz to S3 bucket lc-inpaint as data_size64_ratio2.npz\n",
      "[INFO] Uploading /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/train_64_ratio2.gpkg to S3 bucket lc-inpaint...\n",
      "[INFO] Uploaded /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/train_64_ratio2.gpkg to S3 bucket lc-inpaint as train_64_ratio2.gpkg\n",
      "[INFO] Uploading /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/test_64_ratio2.gpkg to S3 bucket lc-inpaint...\n",
      "[INFO] Uploaded /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/test_64_ratio2.gpkg to S3 bucket lc-inpaint as test_64_ratio2.gpkg\n"
     ]
    }
   ],
   "source": [
    "if save_data:\n",
    "    is_image_bad_train = np.any(np.isnan(train_dem_images), axis=(1, 2))\n",
    "    is_image_kept_train = ~is_image_bad_train\n",
    "\n",
    "    train_gdf_final = train_gdf[is_image_kept_train]\n",
    "    logging.info(f\"Removed {is_image_bad_train.sum()} images with missing DEM data from training set.\")\n",
    "\n",
    "    is_image_bad_test = np.any(np.isnan(test_dem_images), axis=(1, 2))\n",
    "    is_image_kept_test = ~is_image_bad_test\n",
    "    test_gdf_final = test_gdf[is_image_kept_test]\n",
    "    logging.info(f\"Removed {is_image_bad_test.sum()} images with missing DEM data from test set.\")\n",
    "\n",
    "    train_gdf_final.to_crs('EPSG:4326', inplace=True)\n",
    "    test_gdf_final.to_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "    # Take arrays of shape (N, H, W) and stack them along the channel axis\n",
    "    # which needs to be created for both data sets\n",
    "    train_combined = np.stack([train_images[is_image_kept_train], train_dem_images[is_image_kept_train]], axis=1)\n",
    "    test_combined = np.stack([test_images[is_image_kept_test],  test_dem_images[is_image_kept_test]], axis=1)\n",
    "\n",
    "    train_gdf_path = config.output_path_train_gpkg\n",
    "    train_gdf_final.to_file(train_gdf_path, driver='GPKG')\n",
    "    train_gpkg_size = os.path.getsize(train_gdf_path)\n",
    "    logging.info(f\"Training sample location GeoDataFrame saved to {train_gdf_path} (Size: {train_gpkg_size / (1024 * 1024):.2f} MB)\")\n",
    "\n",
    "    test_gdf_path = config.output_path_test_gpkg\n",
    "    test_gdf_final.to_file(test_gdf_path, driver='GPKG')\n",
    "    test_gpkg_size = os.path.getsize(test_gdf_path)\n",
    "    logging.info(f\"Test sample location GeoDataFrame saved to {test_gdf_path} (Size: {test_gpkg_size / (1024 * 1024):.2f} MB)\")\n",
    "\n",
    "    # Save the lat-long coordinates of the training and test samples\n",
    "    train_coords = np.stack(\n",
    "        [train_gdf_final.centroid.x.values,\n",
    "        train_gdf_final.centroid.y.values,],\n",
    "    axis=1)\n",
    "\n",
    "    test_coords = np.stack(\n",
    "        [test_gdf_final.centroid.x.values,\n",
    "        test_gdf_final.centroid.y.values],\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "    np.savez_compressed(config.output_path, train_data=train_combined, test_data=test_combined, train_coords=train_coords, test_coords=test_coords,\n",
    "    train_data_tokenized=train_images_tokenized, test_data_tokenized=test_images_tokenized, decode_table=decode_table)\n",
    "    logging.info(f\"Saved training and test data to {config.output_path} (Size: {os.path.getsize(config.output_path) / (1024 * 1024):.2f} MB)\")\n",
    "    if config.upload_to_s3:\n",
    "        import boto3\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket_name = BUCKET_NAME\n",
    "        files_to_upload = [\n",
    "            config.output_path,\n",
    "            train_gdf_path,\n",
    "            test_gdf_path, \n",
    "        ]\n",
    "\n",
    "        for file in files_to_upload:\n",
    "            logging.info(f\"Uploading {file} to S3 bucket {bucket_name}...\")\n",
    "            try:\n",
    "                s3.upload_file(str(file), bucket_name, file.name)\n",
    "                logging.info(f\"Uploaded {file} to S3 bucket {bucket_name} as {file.name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to upload {file} to S3 bucket {bucket_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show large, random sample of LULC images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lulc_sample = False\n",
    "\n",
    "if plot_lulc_sample:\n",
    "    fig, axes = plt.subplots(10, 16, figsize=(48, 30))\n",
    "    axes = axes.flatten()\n",
    "    for i in range(16*10):\n",
    "        ax = axes[i]\n",
    "        random_index = np.random.randint(0, train_combined.shape[0])\n",
    "        image = train_combined[random_index, 0].astype(np.uint8)\n",
    "        ax.imshow(lut[image])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Create animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'train_images_final' in locals():\n",
    "    train_images_final = np.load(config.output_path)['train_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_animation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_animation:\n",
    "    class TerrainAnimator:\n",
    "        def __init__(self, train_images, train_dem_images, lut, n_rows=4, n_cols=8):\n",
    "            self.train_images = train_images\n",
    "            self.train_dem_images = train_dem_images\n",
    "            self.lut = lut\n",
    "            self.n_rows = n_rows\n",
    "            self.n_cols = n_cols\n",
    "            self.exaggeration = 1\n",
    "            \n",
    "            # Pre-calculate mesh grid\n",
    "            self.h, self.w = train_images[0].shape\n",
    "            x = np.arange(self.w)\n",
    "            y = np.arange(self.h)\n",
    "            self.X, self.Y = np.meshgrid(x, y)\n",
    "            \n",
    "            # Initialize figure\n",
    "            self.setup_figure()\n",
    "            \n",
    "        def setup_figure(self):\n",
    "            plt.rcParams['figure.dpi'] = 300\n",
    "            self.fig, self.axes = plt.subplots(\n",
    "                self.n_rows, \n",
    "                self.n_cols, \n",
    "                figsize=(self.n_cols*1.4, self.n_rows*1.6),  # Reduced figure size\n",
    "                subplot_kw={'projection': '3d'},\n",
    "                constrained_layout=True  # Use constrained layout\n",
    "            )\n",
    "            self.fig.set_facecolor('black')\n",
    "            self.fig.patch.set_alpha(1.0)\n",
    "            # Reduce margins\n",
    "            plt.subplots_adjust(left=0.02, right=0.98, bottom=0.02, top=0.98)\n",
    "            \n",
    "            # Select random indices once\n",
    "            self.indices = np.random.choice(\n",
    "                len(self.train_images), \n",
    "                self.n_rows * self.n_cols, \n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "        def process_elevation(self, elevation):\n",
    "            \"\"\"Pre-process elevation data with Gaussian smoothing\"\"\"\n",
    "            return cv2.GaussianBlur(elevation, (3, 3), 0)\n",
    "            \n",
    "        def create_surface(self, ax, idx):\n",
    "            \"\"\"Create a single surface plot\"\"\"\n",
    "            land_cover = self.train_images[idx]\n",
    "            elevation = self.process_elevation(self.train_dem_images[idx])\n",
    "            \n",
    "            surf = ax.plot_surface(\n",
    "                self.X, self.Y,\n",
    "                elevation * self.exaggeration,\n",
    "                facecolors=self.lut[land_cover],\n",
    "                shade=False,\n",
    "                antialiased=False,\n",
    "                rstride=1,\n",
    "                cstride=1\n",
    "            )\n",
    "\n",
    "            ax.set_facecolor('black')\n",
    "            \n",
    "            # Configure view\n",
    "            ax.view_init(elev=30, azim=45)\n",
    "            ax.set_box_aspect([1, 1, 0.5])\n",
    "            \n",
    "            # Remove unnecessary elements\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_zticks([])\n",
    "            ax.grid(False)\n",
    "            ax.axis('off')\n",
    "\n",
    "            ele_max = elevation.max()\n",
    "\n",
    "            if ele_max < 100:\n",
    "                zlim = 150\n",
    "            elif ele_max < 200:\n",
    "                zlim = 250\n",
    "            else:\n",
    "                zlim = max(300, ele_max * 3)\n",
    "\n",
    "            ax.set_zlim(0, zlim)\n",
    "            \n",
    "            return surf\n",
    "            \n",
    "        def setup_plots(self):\n",
    "            \"\"\"Initialize all surface plots in parallel\"\"\"\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                self.surfaces = list(executor.map(\n",
    "                    lambda args: self.create_surface(*args),\n",
    "                    zip(self.axes.flatten(), self.indices)\n",
    "                ))\n",
    "            \n",
    "            plt.subplots_adjust(hspace=-0.6, wspace=-0.2)  # Increased overlap between subplots\n",
    "            \n",
    "        def update(self, frame):\n",
    "            \"\"\"Animation update function\"\"\"\n",
    "            for ax in self.axes.flatten():\n",
    "                ax.view_init(elev=30, azim=frame)\n",
    "            return self.surfaces\n",
    "            \n",
    "        def create_animation(self, frames=360, fps=30, out_path=config.output_path_animation):\n",
    "            \"\"\"Create and save the animation\"\"\"\n",
    "            self.setup_plots()\n",
    "            \n",
    "            anim = FuncAnimation(\n",
    "                self.fig,\n",
    "                self.update,\n",
    "                frames=frames,\n",
    "                interval=1000/fps,\n",
    "                blit=True\n",
    "            )\n",
    "            \n",
    "            # Save with optimized settings\n",
    "            anim.save(\n",
    "                out_path,\n",
    "                writer='pillow',\n",
    "                fps=fps,\n",
    "                savefig_kwargs={'facecolor': 'black'},\n",
    "                progress_callback=lambda i, n: print(f'Saving frame {i}/{n}', end='\\r')\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    # Usage\n",
    "    train_images_final[:,0]\n",
    "    animator = TerrainAnimator(\n",
    "        train_images_final[:,0], train_images_final[:,1], lut,\n",
    "        n_rows=4, n_cols=4\n",
    "    )\n",
    "    animator.create_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"terrain_rotation.gif\" width=\"1500\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                      Type              Data/Info\n",
      "---------------------------------------------------------\n",
      "BUCKET_NAME                   str               lc-inpaint\n",
      "CRS                           type              <class 'pyproj.crs.crs.CRS'>\n",
      "D                             int               2\n",
      "DataPrepConfig                type              <class '__main__.DataPrepConfig'>\n",
      "FuncAnimation                 type              <class 'matplotlib.animation.FuncAnimation'>\n",
      "HTML                          type              <class 'IPython.core.display.HTML'>\n",
      "Path                          type              <class 'pathlib.Path'>\n",
      "Resampling                    EnumMeta          <enum 'Resampling'>\n",
      "ThreadPoolExecutor            type              <class 'concurrent.future<...>read.ThreadPoolExecutor'>\n",
      "Transformer                   type              <class 'pyproj.transformer.Transformer'>\n",
      "Tuple                         _TupleType        typing.Tuple\n",
      "all_patches                   ndarray           5702656x2x2: 22810624 elems, type `uint8`, 22810624 bytes (21.75390625 Mb)\n",
      "available_memory              int               77299519488\n",
      "boto3                         module            <module 'boto3' from '/mn<...>kages/boto3/__init__.py'>\n",
      "bounds                        BoundingBox       BoundingBox(left=-2493045<...>2342655.0, top=3310005.0)\n",
      "box                           function          <function box at 0x7ff6b8b5f2e0>\n",
      "bucket_name                   str               lc-inpaint\n",
      "calculate_default_transform   function          <function calculate_defau<...>nsform at 0x7ff68510b6d0>\n",
      "check_overlap                 function          <function check_overlap at 0x7ff685127a30>\n",
      "classes_df                    DataFrame                class_value       <...>  (0.439, 0.639, 0.729)  \n",
      "cls_id                        uint8             95\n",
      "code                          int               95\n",
      "color                         tuple             n=3\n",
      "compute_block_counts          function          <function compute_block_counts at 0x7ff685127d90>\n",
      "compute_class_distribution    function          <function compute_class_d<...>bution at 0x7ff67c799cf0>\n",
      "config                        DataPrepConfig    DataPrepConfig(bbox_west=<...>tio=2, upload_to_s3=True)\n",
      "current_path                  PosixPath         /mnt/m2ssd/data/Dropbox/research/lc-gpt/scripts\n",
      "cv2                           module            <module 'cv2' from '/mnt/<...>ackages/cv2/__init__.py'>\n",
      "data_crs                      CRS               PROJCS[\"Albers_Conical_Eq<...>],AXIS[\"Northing\",NORTH]]\n",
      "dataclass                     function          <function dataclass at 0x7ff723150e50>\n",
      "decode_images                 function          <function decode_images at 0x7ff67c10a440>\n",
      "decode_table                  ndarray           20601x2x2: 82404 elems, type `uint8`, 82404 bytes\n",
      "dem_data                      ndarray           972x1980: 1924560 elems, type `int16`, 3849120 bytes (3.670806884765625 Mb)\n",
      "dem_src                       DatasetReader     <closed DatasetReader nam<...>_conus_dem.tif' mode='r'>\n",
      "display                       function          <function display at 0x7ff722a13e20>\n",
      "downsample_patch              function          <function downsample_patch at 0x7ff685127e20>\n",
      "downsample_ratio              int               2\n",
      "downsample_stride             int               100\n",
      "ds_bottom                     float             21.742307778016638\n",
      "ds_left                       float             -119.78610533604079\n",
      "ds_right                      float             -63.672191850655295\n",
      "ds_top                        float             49.177063191389536\n",
      "dtype_size                    int               1\n",
      "elevation                     module            <module 'elevation' from <...>s/elevation/__init__.py'>\n",
      "extract_dem_images            function          <function extract_dem_images at 0x7ff667f0e560>\n",
      "extract_patches               function          <function extract_patches at 0x7ff67c1a7be0>\n",
      "file                          PosixPath         /mnt/m2ssd/data/Dropbox/r<...>/data/test_64_ratio2.gpkg\n",
      "files_to_upload               list              n=3\n",
      "from_working_crs              Transformer       proj=pipeline step proj=u<...>5 x_0=0 y_0=0 ellps=WGS84\n",
      "geometry_mask                 function          <function geometry_mask at 0x7ff68510bac0>\n",
      "glob                          module            <module 'glob' from '/usr<...>/lib/python3.10/glob.py'>\n",
      "gpd                           module            <module 'geopandas' from <...>s/geopandas/__init__.py'>\n",
      "grid_cells                    list              n=2500\n",
      "grid_gdf                      GeoDataFrame                               <...>\\n[2500 rows x 3 columns]\n",
      "i                             int               49\n",
      "image_size                    int               64\n",
      "inverse                       ndarray           5702656: 5702656 elems, type `int64`, 45621248 bytes (43.5078125 Mb)\n",
      "is_image_bad_test             ndarray           263: 263 elems, type `bool`, 263 bytes\n",
      "is_image_bad_train            ndarray           5306: 5306 elems, type `bool`, 5306 bytes\n",
      "is_image_kept_test            ndarray           263: 263 elems, type `bool`, 263 bytes\n",
      "is_image_kept_train           ndarray           5306: 5306 elems, type `bool`, 5306 bytes\n",
      "is_within_bbox                function          <function is_within_bbox at 0x7ff6851271c0>\n",
      "j                             int               49\n",
      "load_dotenv                   function          <function load_dotenv at 0x7ff68510a440>\n",
      "logging                       module            <module 'logging' from '/<...>.10/logging/__init__.py'>\n",
      "lut                           ndarray           100x3: 300 elems, type `float64`, 2400 bytes\n",
      "make_animation                bool              False\n",
      "mapping                       function          <function mapping at 0x7ff6b8b5f400>\n",
      "mask                          function          <function mask at 0x7ff68510ab90>\n",
      "max_elements                  int               38649759744\n",
      "merge                         module            <module 'rasterio.merge' <...>kages/rasterio/merge.py'>\n",
      "merged_src                    DatasetReader     <closed DatasetReader nam<...>_conus_dem.tif' mode='r'>\n",
      "mode                          function          <function mode at 0x7ff685834820>\n",
      "mp                            module            <module 'multiprocessing'<...>iprocessing/__init__.py'>\n",
      "mpatches                      module            <module 'matplotlib.patch<...>s/matplotlib/patches.py'>\n",
      "n_cells                       int               2500\n",
      "n_images                      int               3\n",
      "n_test                        int               125\n",
      "n_train_patches               int               5433344\n",
      "np                            module            <module 'numpy' from '/mn<...>kages/numpy/__init__.py'>\n",
      "os                            module            <module 'os' from '/usr/lib/python3.10/os.py'>\n",
      "palette_series                Series            class\\n0      (0.278, 0.4<...>nName: RGB, dtype: object\n",
      "parent_path                   PosixPath         /mnt/m2ssd/data/Dropbox/research/lc-gpt\n",
      "partial                       type              <class 'functools.partial'>\n",
      "pct                           float64           0.01577730031450245\n",
      "pd                            module            <module 'pandas' from '/m<...>ages/pandas/__init__.py'>\n",
      "pickle                        module            <module 'pickle' from '/u<...>ib/python3.10/pickle.py'>\n",
      "plot_lulc_sample              bool              False\n",
      "plt                           module            <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "polygon                       dict              n=2\n",
      "process_cell                  function          <function process_cell at 0x7ff67c799d80>\n",
      "psutil                        module            <module 'psutil' from '/m<...>ages/psutil/__init__.py'>\n",
      "qmc                           module            <module 'scipy.stats.qmc'<...>ages/scipy/stats/qmc.py'>\n",
      "rasterio                      module            <module 'rasterio' from '<...>es/rasterio/__init__.py'>\n",
      "reproject                     function          <function reproject at 0x7ff68510b520>\n",
      "s3                            S3                <botocore.client.S3 object at 0x7ff67c3ae020>\n",
      "sample_images_parallel        function          <function sample_images_p<...>rallel at 0x7ff67c79af80>\n",
      "samples_x                     float             1259.296875\n",
      "samples_y                     float             815.8125\n",
      "save_data                     bool              True\n",
      "seen_classes                  set               set()\n",
      "sobol_indices                 ndarray           125: 125 elems, type `int64`, 1000 bytes\n",
      "sobol_points                  ndarray           125x1: 125 elems, type `float64`, 1000 bytes\n",
      "src                           DatasetReader     <closed DatasetReader nam<...>_conus_dem.tif' mode='r'>\n",
      "subprocess                    module            <module 'subprocess' from<...>ython3.10/subprocess.py'>\n",
      "tempfile                      module            <module 'tempfile' from '<...>/python3.10/tempfile.py'>\n",
      "test_combined                 ndarray           263x2x64x64: 2154496 elems, type `uint8`, 2154496 bytes (2.0546875 Mb)\n",
      "test_coords                   ndarray           263x2: 526 elems, type `float64`, 4208 bytes\n",
      "test_decoded                  ndarray           3x64x64: 12288 elems, type `uint8`, 12288 bytes\n",
      "test_dem_images               ndarray           263x64x64: 1077248 elems, type `uint8`, 1077248 bytes (1.02734375 Mb)\n",
      "test_dist                     dict              n=16\n",
      "test_gdf                      GeoDataFrame                               <...>n\\n[263 rows x 1 columns]\n",
      "test_gdf_final                GeoDataFrame                               <...>n\\n[263 rows x 1 columns]\n",
      "test_gdf_path                 PosixPath         /mnt/m2ssd/data/Dropbox/r<...>/data/test_64_ratio2.gpkg\n",
      "test_gpkg_size                int               155648\n",
      "test_images                   ndarray           263x64x64: 1077248 elems, type `uint8`, 1077248 bytes (1.02734375 Mb)\n",
      "test_images_tokenized         ndarray           263x32x32: 269312 elems, type `int64`, 2154496 bytes (2.0546875 Mb)\n",
      "test_tokens                   ndarray           269312: 269312 elems, type `int64`, 2154496 bytes (2.0546875 Mb)\n",
      "to_working_crs                Transformer       proj=pipeline step inv pr<...>vert xy_in=rad xy_out=deg\n",
      "total_counts                  dict              n=16\n",
      "tqdm                          type              <class 'tqdm.notebook.tqdm_notebook'>\n",
      "train_combined                ndarray           5306x2x64x64: 43466752 elems, type `uint8`, 43466752 bytes (41.453125 Mb)\n",
      "train_coords                  ndarray           5306x2: 10612 elems, type `float64`, 84896 bytes\n",
      "train_decoded                 ndarray           3x64x64: 12288 elems, type `uint8`, 12288 bytes\n",
      "train_dem_images              ndarray           5306x64x64: 21733376 elems, type `uint8`, 21733376 bytes (20.7265625 Mb)\n",
      "train_dist                    dict              n=16\n",
      "train_gdf                     GeoDataFrame                               <...>\\n[5306 rows x 1 columns]\n",
      "train_gdf_final               GeoDataFrame                               <...>\\n[5306 rows x 1 columns]\n",
      "train_gdf_path                PosixPath         /mnt/m2ssd/data/Dropbox/r<...>data/train_64_ratio2.gpkg\n",
      "train_gpkg_size               int               1191936\n",
      "train_images                  ndarray           5306x64x64: 21733376 elems, type `uint8`, 21733376 bytes (20.7265625 Mb)\n",
      "train_images_final            ndarray           5306x2x64x64: 43466752 elems, type `uint8`, 43466752 bytes (41.453125 Mb)\n",
      "train_images_tokenized        ndarray           5306x32x32: 5433344 elems, type `int64`, 43466752 bytes (41.453125 Mb)\n",
      "train_tokens                  ndarray           5433344: 5433344 elems, type `int64`, 43466752 bytes (41.453125 Mb)\n",
      "transform_bounds              function          <function transform_bounds at 0x7ff68510b400>\n",
      "working_crs                   CRS               EPSG:4326\n",
      "x_edges                       ndarray           51: 51 elems, type `float64`, 408 bytes\n",
      "y_edges                       ndarray           51: 51 elems, type `float64`, 408 bytes\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 12. Base Footprint Extraction\n",
    "\n",
    "Extract NLCD data around base borders with configurable buffer radius.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
