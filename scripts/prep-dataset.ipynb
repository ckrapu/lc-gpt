{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook contains a data cleaning pipeline for preparing a dataset for land cover data overlaid on digital elevation data. \n",
    "\n",
    "The expected outputs are:\n",
    "- Train and test files for image data, with shape (N, 2, H, W) with the first channel for LULC and second channel for DEM\n",
    "- Train and test geopackage files containing the bounding boxes for each image\n",
    "\n",
    "The main steps in this notebook are:\n",
    "- Splitting up the domain into adjacent grid cells and extracting land cover data\n",
    "- Assigning portions of the domain into train and test splits using Sobol sequences\n",
    "- Downloading and merging DEM files for the same domain\n",
    "- Extracting DEM data for each land cover image\n",
    "- Saving these data to disk\n",
    "- Creating an animation of the final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geopandas : 1.1.0\n",
      "elevation : 1.1.3\n",
      "numpy     : 1.26.4\n",
      "pyproj    : 3.7.1\n",
      "psutil    : 7.0.0\n",
      "shapely   : 2.1.1\n",
      "matplotlib: 3.9.2\n",
      "dotenv    : 0.9.9\n",
      "logging   : 0.5.1.2\n",
      "rasterio  : 1.3.11\n",
      "scipy     : 1.14.1\n",
      "pandas    : 2.2.3\n",
      "tqdm      : 4.66.5\n",
      "IPython   : 8.37.0\n",
      "cv2       : 4.10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import elevation\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import rasterio\n",
    "import pickle\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "from pyproj import CRS, Transformer\n",
    "from scipy.stats import mode\n",
    "from shapely.geometry import box\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple\n",
    "from dotenv import load_dotenv\n",
    "from shapely.geometry import mapping\n",
    "from rasterio.mask import mask\n",
    "from rasterio.warp import transform_bounds, calculate_default_transform, reproject, Resampling\n",
    "from rasterio.features import geometry_mask\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "import logging\n",
    "\n",
    "BUCKET_NAME = \"lc-inpaint\"\n",
    "logging.getLogger('boto3').setLevel(logging.WARNING)\n",
    "logging.getLogger('botocore').setLevel(logging.WARNING)\n",
    "logging.getLogger('s3transfer').setLevel(logging.WARNING)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -iv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setting up the config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Setting project data directory to /mnt/m2ssd/data/Dropbox/research/lc-gpt/data\n"
     ]
    }
   ],
   "source": [
    "current_path = Path.cwd()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataPrepConfig:\n",
    "    logging_level = logging.INFO\n",
    "\n",
    "    # Geographic bounds (WGS84 coordinates)\n",
    "    bbox_west: float = -119.0\n",
    "    bbox_east: float = -64.0\n",
    "    bbox_south: float = 22.0\n",
    "    bbox_north: float = 49.0\n",
    "\n",
    "    # Sampling parameters\n",
    "    image_size: int = 32  # Size of output images\n",
    "    meters_per_raw_pixel: int = 240  # Meters per pixel in the raw data\n",
    "    downsample_ratio: int = meters_per_raw_pixel // 30  # Downsampling factor\n",
    "    max_fraction_reject_class: float = 0.9  # Maximum fraction of pixels allowed in a reject-eligible class\n",
    "    area_fraction_test: float = 0.05  # Fraction of area to reserve for testing\n",
    "    n_grid_unit: int = 50  # discrete units in each dimension for gridding the domain into discrete units\n",
    "    n_samples_per_cell: int = 10  # Maximum of samples to extract per cell; only use for testing since it does not randomize over position\n",
    "\n",
    "    # Data paths\n",
    "    data_dir: Path = current_path / 'data'\n",
    "    dem_dir = Path(data_dir) / 'dem'\n",
    "\n",
    "    nlcd_path: Path = data_dir / 'nlcd_2021_land_cover_l48_20230630.img' # Make sure you have this file before you start\n",
    "    output_path: Path  = data_dir / f'data_size{image_size}_ratio{downsample_ratio}.npz'\n",
    "    output_path_test_gpkg: Path = data_dir / f\"test_{image_size}_ratio{downsample_ratio}.gpkg\"\n",
    "    output_path_train_gpkg: Path = data_dir / f\"train_{image_size}_ratio{downsample_ratio}.gpkg\"\n",
    "    split_save_path: Path = data_dir / f\"split_{image_size}_ratio{downsample_ratio}.gpkg\"\n",
    "    merged_dem_path: Path = dem_dir / f\"merged_conus_dem.tif\"\n",
    "    output_path_animation: Path = data_dir / f\"animation_{image_size}_ratio{downsample_ratio}.gif\"\n",
    "    \n",
    "    # Processing parameters\n",
    "    random_seed: int = 827  # Random seed for reproducibility\n",
    "    recompute_counts: bool = False  # Whether to recompute class counts\n",
    "    show_plots: bool = False  # Whether to display plots\n",
    "    \n",
    "    # CRS parameters\n",
    "    working_crs: str = 'EPSG:4326'  # CRS for geographic operations (WGS84)         \n",
    "    \n",
    "    nlcd_original_classes_for_reject = {11}\n",
    "    nlcd_original_unknown_class = 0\n",
    "\n",
    "    # Parameters for DEM processing\n",
    "    dem_nodata_threshold: float = 0.25\n",
    "    dem_product: str = 'SRTM1' # Choices are 'SRTM1' or 'SRTM3', lower resolution\n",
    "    download_dem: bool = False \n",
    "    merge_dem: bool = False\n",
    "    dem_elev_max: float = 4430.0 # Threshold for nodata values in DEM. Highest point in CONUS is 4421 m.\n",
    "\n",
    "    upload_to_s3: bool = True\n",
    "\n",
    "    # Mapping from raw NLCD classes to RGB colors for visualization\n",
    "    nlcd_to_rgb  = {\n",
    "            11: (0.278, 0.420, 0.627),\n",
    "            12: (0.820, 0.867, 0.976),\n",
    "            21: (0.867, 0.788, 0.788),\n",
    "            22: (0.847, 0.576, 0.510),\n",
    "            23: (0.929, 0.0, 0.0),\n",
    "            24: (0.667, 0.0, 0.0),\n",
    "            31: (0.698, 0.678, 0.639),\n",
    "            41: (0.408, 0.667, 0.388),\n",
    "            42: (0.110, 0.388, 0.188),\n",
    "            43: (0.710, 0.788, 0.557),\n",
    "            51: (0.647, 0.549, 0.188),\n",
    "            52: (0.800, 0.729, 0.486),\n",
    "            71: (0.886, 0.886, 0.757),\n",
    "            72: (0.788, 0.788, 0.467),\n",
    "            73: (0.600, 0.757, 0.278),\n",
    "            74: (0.467, 0.678, 0.576),\n",
    "            81: (0.859, 0.847, 0.239),\n",
    "            82: (0.667, 0.439, 0.157),\n",
    "            90: (0.729, 0.847, 0.918),\n",
    "            95: (0.439, 0.639, 0.729),  \n",
    "        }\n",
    "    nlcd_to_name = {\n",
    "        11: \"Open Water\",\n",
    "        12: \"Perennial Ice/Snow\",\n",
    "        21: \"Developed, Open Space\",\n",
    "        22: \"Developed, Low Intensity\",\n",
    "        23: \"Developed, Medium Intensity\",\n",
    "        24: \"Developed, High Intensity\",\n",
    "        31: \"Barren Land (Rock/Sand/Clay)\",\n",
    "        41: \"Deciduous Forest\",\n",
    "        42: \"Evergreen Forest\",\n",
    "        43: \"Mixed Forest\",\n",
    "        51: \"Dwarf Scrub\",\n",
    "        52: \"Shrub/Scrub\",\n",
    "        71: \"Grassland/Herbaceous\",\n",
    "        72: \"Sedge/Herbaceous\",\n",
    "        73: \"Lichens\",\n",
    "        74: \"Moss\",\n",
    "        81: \"Pasture/Hay\",\n",
    "        82: \"Cultivated Crops\",\n",
    "        90: \"Woody Wetlands\",\n",
    "        95: \"Emergent Herbaceous Wetlands\"\n",
    "    }\n",
    "    \n",
    "    class_mapping = {\n",
    "        k: i for i, k in enumerate(nlcd_to_name.keys())\n",
    "    }\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Validate bbox coordinates\n",
    "        if not (self.bbox_west < self.bbox_east):\n",
    "            raise ValueError(f\"Invalid bbox coordinates: bbox_west ({self.bbox_west}) should be less than bbox_east ({self.bbox_east})\")\n",
    "        if not (self.bbox_south < self.bbox_north):\n",
    "            raise ValueError(f\"Invalid bbox coordinates: bbox_south ({self.bbox_south}) should be less than bbox_north ({self.bbox_north})\")\n",
    "        \n",
    "        # Validate file paths\n",
    "        if not self.nlcd_path.is_file():\n",
    "            raise FileNotFoundError(f\"NLCD file not found at {self.nlcd_path}\")\n",
    "        if not self.data_dir.is_dir():\n",
    "            raise FileNotFoundError(f\"Data directory not found at {self.data_dir}\")\n",
    "            \n",
    "\n",
    "config = DataPrepConfig()\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='[%(levelname)s] %(message)s',\n",
    "    level=config.logging_level,\n",
    ")\n",
    "\n",
    "np.random.seed(827)\n",
    "\n",
    "logging.info(f\"Setting project data directory to {config.data_dir}\")\n",
    "\n",
    "def downsample_patch(patch: np.ndarray, ratio: int, force_divisible=False) -> np.ndarray:\n",
    "    \"\"\"Downsample a patch by taking the mode of each ratio x ratio window.\"\"\"\n",
    "    if ratio == 1:\n",
    "        return patch\n",
    "    \n",
    "    if force_divisible:\n",
    "        h, w = patch.shape\n",
    "        new_h = (h // ratio) * ratio\n",
    "        new_w = (w // ratio) * ratio\n",
    "        patch = patch[:new_h, :new_w]\n",
    "    \n",
    "    # Reshape into blocks of size ratio x ratio\n",
    "    h, w = patch.shape\n",
    "    new_h, new_w = h // ratio, w // ratio\n",
    "    \n",
    "    reshaped = patch.reshape(new_h, ratio, new_w, ratio)\n",
    "    \n",
    "    # Move the two ratio axes adjacent so each block becomes one dimension\n",
    "    # resulting shape: (new_h * new_w, ratio * ratio)\n",
    "    reshaped = reshaped.swapaxes(1, 2).reshape(new_h * new_w, ratio * ratio)\n",
    "    \n",
    "    # mode(..., axis=1) finds the most frequent value in each row\n",
    "    block_modes, _ = mode(reshaped, axis=1)\n",
    "    \n",
    "    # Reshape back to (new_h, new_w)\n",
    "    downsampled = block_modes.reshape(new_h, new_w)\n",
    "    \n",
    "    return downsampled\n",
    "\n",
    "def get_pixel_bounds(src, x: float, y: float, size_pixels: int) -> Tuple[slice, slice]:\n",
    "    \"\"\"Convert geographic coordinates to pixel bounds for image extraction.\"\"\"\n",
    "    # Convert from working CRS to data CRS\n",
    "    x_data, y_data = from_working_crs.transform(x, y)\n",
    "    \n",
    "    # Convert to pixel coordinates\n",
    "    row, col = src.index(x_data, y_data)\n",
    "    \n",
    "    # Calculate pixel bounds\n",
    "    half_size = size_pixels // 2\n",
    "    row_start = row - half_size\n",
    "    row_end = row + half_size\n",
    "    col_start = col - half_size\n",
    "    col_end = col + half_size\n",
    "    \n",
    "    return (slice(row_start, row_end), slice(col_start, col_end))\n",
    "\n",
    "def check_overlap(point_coords: Tuple[float, float], image_size_meters: float, \n",
    "                 grid_gdf: gpd.GeoDataFrame, split: str) -> bool:\n",
    "    \"\"\"Check if an image centered at point_coords overlaps with the specified split area.\"\"\"\n",
    "    x, y = point_coords\n",
    "    half_size = image_size_meters / 2\n",
    "    \n",
    "    # Create a box representing the image extent in working CRS\n",
    "    image_box = box(x - half_size, y - half_size,\n",
    "                   x + half_size, y + half_size)\n",
    "    \n",
    "    # Check intersection with grid cells of the opposite split\n",
    "    opposite_split = 'test' if split == 'train' else 'train'\n",
    "    opposite_cells = grid_gdf[grid_gdf['split'] == opposite_split]\n",
    "    \n",
    "    return not any(image_box.intersects(cell) for cell in opposite_cells.geometry)\n",
    "\n",
    "def is_within_bbox(x: float, y: float) -> bool:\n",
    "    \"\"\"Check if a point is within the specified bbox.\"\"\"\n",
    "    return (config.bbox_west <= x <= config.bbox_east and\n",
    "            config.bbox_south <= y <= config.bbox_north)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Information and CRS Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset CRS: PROJCS[\"Albers_Conical_Equal_Area\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"meters\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n",
      "[INFO] Dataset bounds: BoundingBox(left=-2493045.0, bottom=177285.0, right=2342655.0, top=3310005.0)\n",
      "[INFO] Dataset shape: (104424, 161190)\n",
      "[INFO] Dataset resolution: (30.0, 30.0)\n",
      "[INFO] Dataset transform: | 30.00, 0.00,-2493045.00|\n",
      "| 0.00,-30.00, 3310005.00|\n",
      "| 0.00, 0.00, 1.00|\n",
      "[INFO] \n",
      "Bounding box validation:\n",
      "[INFO] Dataset bounds (lon/lat): -119.7861, 21.7423, -63.6722, 49.1771\n",
      "[INFO] Selected bbox (lon/lat): -119.0, 22.0, -64.0, 49.0\n",
      "[INFO] Maximum number of sampled images from full dataset: 256838\n"
     ]
    }
   ],
   "source": [
    "# Open the NLCD dataset and print basic information\n",
    "with rasterio.open(config.nlcd_path) as src:\n",
    "    logging.info(f\"Dataset CRS: {src.crs}\")\n",
    "    logging.info(f\"Dataset bounds: {src.bounds}\")\n",
    "    logging.info(f\"Dataset shape: {src.shape}\")\n",
    "    logging.info(f\"Dataset resolution: {src.res}\")\n",
    "    logging.info(f\"Dataset transform: {src.transform}\")\n",
    "    \n",
    "    # Set up CRS transformers\n",
    "    data_crs = src.crs\n",
    "    working_crs = CRS.from_string(config.working_crs)\n",
    "    \n",
    "    # Create transformers for converting between CRS\n",
    "    to_working_crs = Transformer.from_crs(data_crs, working_crs, always_xy=True)\n",
    "    from_working_crs = Transformer.from_crs(working_crs, data_crs, always_xy=True)\n",
    "    \n",
    "    # Convert dataset bounds to working CRS for validation\n",
    "    bounds = src.bounds\n",
    "    ds_left, ds_bottom = to_working_crs.transform(bounds.left, bounds.bottom)\n",
    "    ds_right, ds_top = to_working_crs.transform(bounds.right, bounds.top)\n",
    "    \n",
    "    \n",
    "    logging.info(\"\\nBounding box validation:\")\n",
    "    logging.info(f\"Dataset bounds (lon/lat): {ds_left:.4f}, {ds_bottom:.4f}, {ds_right:.4f}, {ds_top:.4f}\")\n",
    "    logging.info(f\"Selected bbox (lon/lat): {config.bbox_west}, {config.bbox_south}, {config.bbox_east}, {config.bbox_north}\")    \n",
    "    samples_x = src.shape[1] / config.downsample_ratio / config.image_size\n",
    "    samples_y = src.shape[0] / config.downsample_ratio / config.image_size\n",
    "\n",
    "    logging.info(f\"Maximum number of sampled images from full dataset: {samples_x * samples_y:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Class Counting and Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping class counts computation from raster; loading from file\n"
     ]
    }
   ],
   "source": [
    "# Function to compute class counts in a block\n",
    "def compute_block_counts(data):\n",
    "    unique, counts = np.unique(data, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "# Calculate available memory\n",
    "available_memory = psutil.virtual_memory().available\n",
    "dtype_size = np.dtype('uint8').itemsize\n",
    "max_elements = available_memory // (2 * dtype_size)  # Use half of available memory\n",
    "\n",
    "if not config.recompute_counts:\n",
    "    logging.info(f\"Skipping class counts computation from raster; loading from file\")\n",
    "else:\n",
    "    with rasterio.open(config.nlcd_path) as src:\n",
    "        # Convert bbox to pixel coordinates\n",
    "        bbox_left, bbox_bottom = from_working_crs.transform(config.bbox_west, config.bbox_south)\n",
    "        bbox_right, bbox_top = from_working_crs.transform(config.bbox_east, config.bbox_north)\n",
    "        \n",
    "        # Get pixel bounds\n",
    "        row_start, col_start = src.index(bbox_left, bbox_top)\n",
    "        row_end, col_end = src.index(bbox_right, bbox_bottom)\n",
    "        \n",
    "        # Ensure correct order\n",
    "        row_start, row_end = min(row_start, row_end), max(row_start, row_end)\n",
    "        col_start, col_end = min(col_start, col_end), max(col_start, col_end)\n",
    "        \n",
    "        # Calculate block size for the bbox region\n",
    "        bbox_height = row_end - row_start\n",
    "        bbox_width = col_end - col_start\n",
    "        total_pixels = bbox_height * bbox_width\n",
    "        n_blocks = max(1, total_pixels // max_elements)\n",
    "        block_height = bbox_height // n_blocks\n",
    "        \n",
    "        # Initialize counts dictionary\n",
    "        total_counts = {}\n",
    "        \n",
    "        # Process data in blocks within the bbox\n",
    "        for i in tqdm(range(row_start, row_end, block_height), desc='Computing class counts'):\n",
    "            # Read a block of data\n",
    "            window = rasterio.windows.Window(\n",
    "                col_start, i, \n",
    "                col_end - col_start,\n",
    "                min(block_height, row_end - i)\n",
    "            )\n",
    "            data = src.read(1, window=window)\n",
    "            \n",
    "            # Update counts\n",
    "            block_counts = compute_block_counts(data)\n",
    "            for k, v in block_counts.items():\n",
    "                total_counts[k] = total_counts.get(k, 0) + v\n",
    "\n",
    "    logging.info(f\"Unique values present in the bbox: {len(total_counts)}: {total_counts.keys()}\")\n",
    "\n",
    "    # Drop the counts which are in class 0 (Unknown)\n",
    "    _ = total_counts.pop(config.nlcd_original_unknown_class, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping class counts computation from raster; loading from file\n",
      "[INFO] Prepare lookup table for plotting with shape (17, 3)\n",
      "[INFO] \n",
      "Class mapping:\n",
      "[INFO] {11: 0, 12: 1, 21: 2, 22: 3, 23: 4, 24: 5, 31: 6, 41: 7, 42: 8, 43: 9, 51: 10, 52: 11, 71: 12, 72: 13, 73: 14, 74: 15, 81: 16, 82: 17, 90: 18, 95: 19}\n"
     ]
    }
   ],
   "source": [
    "if config.recompute_counts:\n",
    "\n",
    "    # Convert to DataFrame for better visualization\n",
    "    classes_df = pd.DataFrame([\n",
    "        {'class_value': k, 'count': v, 'name': config.nlcd_to_name.get(k, 'Unknown')} \n",
    "        for k, v in total_counts.items()\n",
    "    ])\n",
    "    classes_df['percentage'] = classes_df['count'] / classes_df['count'].sum() * 100\n",
    "    classes_df = classes_df.sort_values('count', ascending=False)\n",
    "\n",
    "    # Rename the index (currently unnamed) to \"class\"\n",
    "    classes_df.index.name = 'class'\n",
    "    classes_df = classes_df.sort_index()\n",
    "\n",
    "    # Load the mapping from original class codes to RGB for plotting and add to the dataframe\n",
    "    # We will use these for plotting later\n",
    "    classes_df['RGB'] = classes_df['class_value'].map(config.nlcd_to_rgb)\n",
    "    classes_df.to_parquet(Path(config.data_dir) / 'class_distribution.parquet')\n",
    "    present_classes = sorted(total_counts.keys())\n",
    "\n",
    "\n",
    "else:\n",
    "    logging.info(f\"Skipping class counts computation from raster; loading from file\")\n",
    "    classes_df = pd.read_parquet(Path(config.data_dir) / 'class_distribution.parquet')\n",
    "    total_counts = classes_df.set_index('class_value')['count'].to_dict()\n",
    "    classes_df['RGB'] = classes_df['class_value'].map(config.nlcd_to_rgb)\n",
    "\n",
    "reverse_mapping = {idx: old_val for old_val, idx in config.class_mapping.items()}\n",
    "\n",
    "palette_series = classes_df['RGB']\n",
    "lut = np.array(palette_series.tolist())\n",
    "\n",
    "# Make first row of lut be 0,0,0 for mask\n",
    "lut = np.insert(lut, 0, [0.0, 0.0, 0.0], axis=0)\n",
    "\n",
    "logging.info(f\"Prepare lookup table for plotting with shape {lut.shape}\")\n",
    "\n",
    "# Print mapping from original classes to zero-based indices\n",
    "logging.info(\"\\nClass mapping:\")\n",
    "logging.info(config.class_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel counts by class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Grid Creation for Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/m2ssd/data/Dropbox/research/lc-gpt/.venv/lib/python3.10/site-packages/scipy/stats/_qmc.py:958: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  sample = self._random(n, workers=workers)\n",
      "[INFO] Generated 125 test cells out of 2500 total cells and saved to grid_gdf\n",
      "[INFO] Created 2,500 records\n",
      "[INFO] Saved geodataframe for grid of train/test cells to /mnt/m2ssd/data/Dropbox/research/lc-gpt/data/split_32_ratio8.gpkg\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import qmc  # Built into scipy, no extra installation needed\n",
    "\n",
    "# Set up grid for train/test split\n",
    "with rasterio.open(config.nlcd_path) as src:\n",
    "    # Create grid cells in working CRS using bbox\n",
    "    x_edges = np.linspace(config.bbox_west, config.bbox_east, config.n_grid_unit + 1)\n",
    "    y_edges = np.linspace(config.bbox_south, config.bbox_north, config.n_grid_unit + 1)\n",
    "    \n",
    "    # Create grid cell polygons\n",
    "    grid_cells = []\n",
    "    for i in range(len(x_edges)-1):\n",
    "        for j in range(len(y_edges)-1):\n",
    "            # Create polygon in working CRS\n",
    "            polygon = {\n",
    "                'geometry': {\n",
    "                    'type': 'Polygon',\n",
    "                    'coordinates': [[\n",
    "                        [x_edges[i], y_edges[j]],\n",
    "                        [x_edges[i+1], y_edges[j]],\n",
    "                        [x_edges[i+1], y_edges[j+1]],\n",
    "                        [x_edges[i], y_edges[j+1]],\n",
    "                        [x_edges[i], y_edges[j]]\n",
    "                    ]]\n",
    "                },\n",
    "                'properties': {'id': len(grid_cells)}\n",
    "            }\n",
    "            grid_cells.append(polygon)\n",
    "    \n",
    "    # Create GeoDataFrame in working CRS\n",
    "    grid_gdf = gpd.GeoDataFrame.from_features(grid_cells, crs=working_crs)\n",
    "    \n",
    "    # Set up Sobol sequence generator\n",
    "    n_cells = len(grid_gdf)\n",
    "    n_test = int(n_cells * config.area_fraction_test)\n",
    "    \n",
    "    # Generate Sobol sequence and scale to unique grid indices\n",
    "    sobol_points = qmc.Sobol(d=1, seed=config.random_seed).random(n=n_test)\n",
    "    sobol_indices = (sobol_points.flatten() * (n_cells - 1)).astype(int)\n",
    "    sobol_indices = np.unique(sobol_indices)\n",
    "    \n",
    "    # If we got fewer unique indices than needed, add random ones\n",
    "    if len(sobol_indices) < n_test:\n",
    "        additional_indices = np.random.choice(\n",
    "            np.setdiff1d(np.arange(n_cells), sobol_indices),\n",
    "            size=n_test - len(sobol_indices),\n",
    "            replace=False\n",
    "        )\n",
    "        sobol_indices = np.concatenate([sobol_indices, additional_indices])\n",
    "    \n",
    "    # Assign splits\n",
    "    grid_gdf['split'] = 'train'\n",
    "    grid_gdf.loc[sobol_indices, 'split'] = 'test'\n",
    "    logging.info(f\"Generated {n_test} test cells out of {n_cells} total cells and saved to grid_gdf\")\n",
    "\n",
    "# Save the grid to a GeoPackage file\n",
    "grid_gdf.to_file(config.split_save_path, driver='GPKG')\n",
    "logging.info(f\"Saved geodataframe for grid of train/test cells to {config.split_save_path}\")\n",
    "\n",
    "if config.show_plots:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    grid_gdf[grid_gdf['split'] == 'train'].plot(ax=ax, color='c', alpha=0.3)\n",
    "    grid_gdf[grid_gdf['split'] == 'test'].plot(ax=ax, color='m', alpha=0.3)\n",
    "\n",
    "    for x in x_edges:\n",
    "        ax.axvline(x, color='black', linestyle='--', alpha=0.5)\n",
    "    for y in y_edges:\n",
    "        ax.axhline(y, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Manually create legend elements\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor='c', alpha=0.3, label='Train unit'),\n",
    "        mpatches.Patch(facecolor='m', alpha=0.3, label='Test unit')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "    ax.set_title('Train/Test Grid Split')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sampling Land Cover Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Processing cells using 15 processes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa81a5b033747749eba4ac195001634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing grid cells:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 13145 train images and 650 test images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_cell(cell_data, args):\n",
    "    \"\"\"Process a single cell of data.\n",
    "    \n",
    "    Args:\n",
    "        cell_data: tuple of (cell, src_bounds, src_res, transforms)\n",
    "        args: dict containing configuration parameters\n",
    "    \"\"\"\n",
    "    cell, src_bounds, src_res, transforms = cell_data\n",
    "    to_working_crs, from_working_crs = transforms\n",
    "    \n",
    "    # Unpack configuration\n",
    "    config = args['config']\n",
    "    class_mapping = args['class_mapping']\n",
    "    full_size_pixels = config.image_size * config.downsample_ratio\n",
    "    \n",
    "    bounds = cell.geometry.bounds\n",
    "    split = cell['split']\n",
    "    \n",
    "    # Convert bounds to pixel coordinates\n",
    "    bbox_left, bbox_bottom = from_working_crs.transform(bounds[0], bounds[1])\n",
    "    bbox_right, bbox_top = from_working_crs.transform(bounds[2], bounds[3])\n",
    "    \n",
    "    with rasterio.open(config.nlcd_path) as src:\n",
    "        row_start, col_start = src.index(bbox_left, bbox_top)\n",
    "        row_end, col_end = src.index(bbox_right, bbox_bottom)\n",
    "        \n",
    "        # Ensure correct order\n",
    "        row_start, row_end = min(row_start, row_end), max(row_start, row_end)\n",
    "        col_start, col_end = min(col_start, col_end), max(col_start, col_end)\n",
    "        \n",
    "        # Read the entire cell into memory\n",
    "        cell_data = src.read(1, window=rasterio.windows.Window(\n",
    "            col_start, row_start, \n",
    "            col_end - col_start, \n",
    "            row_end - row_start\n",
    "        ))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    done = False\n",
    "    for i in range(0, cell_data.shape[0] - full_size_pixels + 1, full_size_pixels // 2 ):\n",
    "        if done:\n",
    "            break\n",
    "        for j in range(0, cell_data.shape[1] - full_size_pixels + 1, full_size_pixels // 2):\n",
    "            patch = cell_data[i:i + full_size_pixels, j:j + full_size_pixels]\n",
    "            \n",
    "            # Reject if any pixels are unknown\n",
    "            if np.any(patch == config.nlcd_original_unknown_class):\n",
    "                continue\n",
    "            \n",
    "            # Check water fraction\n",
    "            reject = False\n",
    "            for c in config.nlcd_original_classes_for_reject:\n",
    "                class_fraction = np.mean(patch == c)\n",
    "                if class_fraction > config.max_fraction_reject_class:\n",
    "                    reject = True\n",
    "                    break\n",
    "            if reject:\n",
    "                continue\n",
    "            \n",
    "            # Downsample the patch\n",
    "            downsampled = downsample_patch(patch, config.downsample_ratio)\n",
    "            remapped = downsampled.copy()\n",
    "            # Remap classes to zero-based indices\n",
    "            for old_val, new_val in class_mapping.items():\n",
    "                remapped[downsampled == old_val] = new_val\n",
    "            \n",
    "                x_ul, y_ul = to_working_crs.transform(*src.xy(row_start + i, col_start + j))\n",
    "                x_lr, y_lr = to_working_crs.transform(*src.xy(\n",
    "                    row_start + i + full_size_pixels, \n",
    "                    col_start + j + full_size_pixels\n",
    "                ))\n",
    "            \n",
    "            bbox = box(x_ul, y_ul, x_lr, y_lr)\n",
    "            results.append((remapped, bbox, split))\n",
    "            if config.n_samples_per_cell and len(results) >= config.n_samples_per_cell:\n",
    "                done = True\n",
    "                break\n",
    "    \n",
    "    return results\n",
    "\n",
    "def sample_images_parallel(grid_gdf, config, n_processes=None) -> tuple:\n",
    "    \"\"\"Sample and process images in parallel for either train or test set.\n",
    "    \n",
    "    Args:\n",
    "        grid_gdf: GeoDataFrame containing grid cells\n",
    "        config: Configuration object\n",
    "        n_processes: Number of processes to use (defaults to CPU count - 1)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_images, train_gdf, test_images, test_gdf)\n",
    "    \"\"\"\n",
    "    if n_processes is None:\n",
    "        n_processes = max(1, mp.cpu_count() - 1)\n",
    "    \n",
    "    # Get source metadata once\n",
    "    with rasterio.open(config.nlcd_path) as src:\n",
    "        src_bounds = src.bounds\n",
    "        src_res = src.res\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    transforms = (to_working_crs, from_working_crs)  # Assuming these are defined\n",
    "    cell_data = [(cell, src_bounds, src_res, transforms) for _, cell in grid_gdf.iterrows()]\n",
    "    \n",
    "    # Prepare static arguments\n",
    "    process_args = {\n",
    "        'config': config,\n",
    "        'class_mapping': config.class_mapping\n",
    "    }\n",
    "    \n",
    "    # Create process pool and process cells in parallel\n",
    "    logging.info(f\"Processing cells using {n_processes} processes...\")\n",
    "    with mp.Pool(n_processes) as pool:\n",
    "        process_func = partial(process_cell, args=process_args)\n",
    "        results = list(tqdm(\n",
    "            pool.imap(process_func, cell_data),\n",
    "            total=len(cell_data),\n",
    "            desc=\"Processing grid cells\"\n",
    "        ))\n",
    "    \n",
    "    # Flatten results and separate train/test\n",
    "    train_images = []\n",
    "    test_images = []\n",
    "    train_bboxes = []\n",
    "    test_bboxes = []\n",
    "    \n",
    "    for cell_results in results:\n",
    "        for remapped, bbox, split in cell_results:\n",
    "            if split == 'train':\n",
    "                train_images.append(remapped)\n",
    "                train_bboxes.append(bbox)\n",
    "            else:\n",
    "                test_images.append(remapped)\n",
    "                test_bboxes.append(bbox)\n",
    "    \n",
    "    # Create GeoDataFrames for train and test bounding boxes\n",
    "    working_crs = grid_gdf.crs  # Get CRS from input GeoDataFrame\n",
    "    train_gdf = gpd.GeoDataFrame(geometry=train_bboxes, crs=working_crs)\n",
    "    test_gdf = gpd.GeoDataFrame(geometry=test_bboxes, crs=working_crs)\n",
    "    \n",
    "    return (np.array(train_images, dtype=np.uint8), train_gdf, np.array(test_images, dtype=np.uint8), test_gdf)\n",
    "\n",
    "\n",
    "train_images, train_gdf, test_images, test_gdf = sample_images_parallel(\n",
    "    grid_gdf,\n",
    "    config,\n",
    ")\n",
    "\n",
    "print(f\"Created {len(train_images)} train images and {len(test_images)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] All images are free of null / NaN values.\n",
      "[INFO] All images are in the correct range of values.\n",
      "[INFO] All images are the correct size.\n"
     ]
    }
   ],
   "source": [
    "# Make sure all images are in the valid integer range with no NaNs\n",
    "assert np.all(np.isfinite(train_images))\n",
    "assert np.all(np.isfinite(test_images))\n",
    "logging.info(\"All images are free of null / NaN values.\")\n",
    "\n",
    "# Make sure in right range of values\n",
    "assert np.all((train_images >= 0) & (train_images <= len(config.class_mapping))), f\"Unqiue train values: {np.unique(train_images)}\"\n",
    "assert np.all((test_images >= 0) & (test_images <= len(config.class_mapping))), f\"Unique test values: {np.unique(test_images)}\"\n",
    "logging.info(\"All images are in the correct range of values.\")\n",
    "\n",
    "# Check that the images are the correct size\n",
    "assert train_images.shape[1:] == (config.image_size, config.image_size), f\"Train shape: {train_images.shape} should be {(config.image_size, config.image_size)}\"\n",
    "assert test_images.shape[1:] == (config.image_size, config.image_size), f\"Test shape: {test_images.shape}, should be {(config.image_size, config.image_size)}\"\n",
    "logging.info(\"All images are the correct size.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.show_plots:\n",
    "    train_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping display of sample NLCD images. Set `show_plots` to True to display.\n"
     ]
    }
   ],
   "source": [
    "n_images = 3\n",
    "\n",
    "seen_classes = set()\n",
    "\n",
    "if config.show_plots:\n",
    "    fig, ax = plt.subplots(2, n_images, figsize=(8, 6))\n",
    "\n",
    "    for i in range(n_images):\n",
    "        for j, (images, geom, title) in enumerate(zip([train_images, test_images], [train_gdf.iloc[i].geometry, test_gdf.iloc[i].geometry], [\"Train\", \"Test\"])):\n",
    "            ax[j, i].imshow(lut[images[i]])\n",
    "            ax[j, i].set_title(f\"{title} Image {i+1}\")\n",
    "            ax[j, i].axis('off')\n",
    "\n",
    "            lat, lon = geom.centroid.xy\n",
    "            ax[j, i].text(1.5, 3, f\"{lat[0]:.3f}, {lon[0]:.3f}\", color='black', fontsize=8,\n",
    "                          bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "            seen_classes.update(np.unique(images[i]))\n",
    "\n",
    "    legend_handles = [mpatches.Patch(color=classes_df.loc[idx, \"RGB\"], label=classes_df.loc[idx, \"name\"]) for idx in seen_classes]\n",
    "\n",
    "    fig.legend(handles=legend_handles, loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.15))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.info(\"Skipping display of sample NLCD images. Set `show_plots` to True to display.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Class distribution across sampled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] \n",
      "Final class distribution (original class ID: percentage):\n",
      "[INFO] \n",
      "Training set:\n",
      "[INFO] 11 (Open Water): 4.11%\n",
      "[INFO] 12 (Perennial Ice/Snow): 0.00%\n",
      "[INFO] 21 (Developed, Open Space): 1.08%\n",
      "[INFO] 22 (Developed, Low Intensity): 1.23%\n",
      "[INFO] 23 (Developed, Medium Intensity): 1.01%\n",
      "[INFO] 24 (Developed, High Intensity): 0.35%\n",
      "[INFO] 31 (Barren Land (Rock/Sand/Clay)): 0.82%\n",
      "[INFO] 41 (Deciduous Forest): 10.68%\n",
      "[INFO] 42 (Evergreen Forest): 10.52%\n",
      "[INFO] 43 (Mixed Forest): 3.22%\n",
      "[INFO] 52 (Shrub/Scrub): 21.46%\n",
      "[INFO] 71 (Grassland/Herbaceous): 13.23%\n",
      "[INFO] 81 (Pasture/Hay): 6.85%\n",
      "[INFO] 82 (Cultivated Crops): 18.46%\n",
      "[INFO] 90 (Woody Wetlands): 5.45%\n",
      "[INFO] 95 (Emergent Herbaceous Wetlands): 1.53%\n"
     ]
    }
   ],
   "source": [
    "def compute_class_distribution(images):\n",
    "    unique, counts = np.unique(images, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    return {reverse_mapping[cls]: count/total for cls, count in zip(unique, counts)}\n",
    "\n",
    "train_dist = compute_class_distribution(train_images)\n",
    "test_dist = compute_class_distribution(test_images)\n",
    "\n",
    "logging.info(\"\\nFinal class distribution (original class ID: percentage):\")\n",
    "logging.info(\"\\nTraining set:\")\n",
    "for cls_id, pct in train_dist.items():\n",
    "    logging.info(f\"{cls_id} ({config.nlcd_to_name[cls_id]}): {pct*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Downloading and matching with DEM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data using `elevation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping DEM download. Set `download_dem` to True to download.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if config.download_dem:\n",
    "    n_dem_downloads, bounds = 625, (config.bbox_west, config.bbox_south, config.bbox_east, config.bbox_north)  # should be a square number\n",
    "    # Define the bounding box for continental USA (approximate)\n",
    "    # For testing, use a sample pair of values like below:\n",
    "    # n_dem_downloads, bounds = 4, (-100.0, 28.0, -99.0, 29.0)  # should be a square number\n",
    "\n",
    "    os.makedirs(config.dem_dir, exist_ok=True)\n",
    "\n",
    "    # Calculate the number of splits in each dimension\n",
    "    n_splits = int(n_dem_downloads ** 0.5)\n",
    "\n",
    "    # Remove all files from the DEM directory\n",
    "    for file in config.dem_dir.glob('*.tif'):\n",
    "        file.unlink()\n",
    "\n",
    "    # Split bounds into a grid and download DEM data\n",
    "    with tqdm(total=n_dem_downloads, desc=\"Downloading DEM data\") as pbar:\n",
    "        for i in range(n_splits):\n",
    "            for j in range(n_splits):\n",
    "                west = bounds[0] + (bounds[2] - bounds[0]) * i / n_splits\n",
    "                east = bounds[0] + (bounds[2] - bounds[0]) * (i + 1) / n_splits\n",
    "                south = bounds[1] + (bounds[3] - bounds[1]) * j / n_splits\n",
    "                north = bounds[1] + (bounds[3] - bounds[1]) * (j + 1) / n_splits\n",
    "\n",
    "                assert west < east, f\"West {west} should be less than east {east}\"\n",
    "                assert south < north, f\"South {south} should be less than north {north}\"\n",
    "                \n",
    "                dem_save_path = config.dem_dir / f'conus_dem_{i}_{j}.tif'\n",
    "                elevation.clip(bounds=(west, south, east, north), output=dem_save_path, product=config.dem_product)\n",
    "\n",
    "                # Check the statistics on the DEM\n",
    "                with rasterio.open(dem_save_path) as src:\n",
    "                    dem_data = src.read(1)\n",
    "                    dem_nodata = src.nodata\n",
    "                    dem_stats = {\n",
    "                        'min': dem_data.min(),\n",
    "                        'max': dem_data.max(),\n",
    "                        'mean': dem_data.mean(),\n",
    "                        'nodata': dem_nodata,\n",
    "                        'nodata_fraction': np.mean(dem_data == dem_nodata)\n",
    "                    }\n",
    "                    logging.info(f\"DEM statistics for {dem_save_path}: {dem_stats}\")\n",
    "\n",
    "                pbar.update(1)\n",
    "else:\n",
    "    logging.info(\"Skipping DEM download. Set `download_dem` to True to download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge into single contiguous DEM raster file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from rasterio import merge\n",
    "\n",
    "if config.merge_dem:\n",
    "    # Create a list of all the GeoTIFF files\n",
    "    search_pattern = os.path.join(config.dem_dir, \"conus_dem_*.tif\")\n",
    "    dem_files = glob.glob(search_pattern)\n",
    "\n",
    "    src_files_to_mosaic = []\n",
    "    for file in dem_files:\n",
    "        src = rasterio.open(file)\n",
    "        src_files_to_mosaic.append(src)\n",
    "\n",
    "    mosaic, out_trans = merge.merge(src_files_to_mosaic)\n",
    "\n",
    "    # Copy the metadata from one of the input files\n",
    "    out_meta = src_files_to_mosaic[0].meta.copy()\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": mosaic.shape[1],\n",
    "        \"width\": mosaic.shape[2],\n",
    "        \"transform\": out_trans\n",
    "    })\n",
    "\n",
    "    with rasterio.open(config.merged_dem_path, \"w\", **out_meta) as dest:\n",
    "        dest.write(mosaic)\n",
    "        logging.info(f\"Merged DEM saved to {config.merged_dem_path}\")\n",
    "\n",
    "    logging.info(f\"Proportion of missing data in merged DEM: {np.mean(mosaic < config.dem_nodata_threshold):.2%}\")\n",
    "\n",
    "    for src in src_files_to_mosaic:\n",
    "        src.close()\n",
    "\n",
    "    # Delete variables to save on memory\n",
    "    del mosaic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check merged file metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset CRS: EPSG:4326\n",
      "[INFO] Dataset bounds: BoundingBox(left=-119.00013888888888, bottom=22.000138888888905, right=-64.00013888888893, top=49.00013888888889)\n",
      "[INFO] Dataset shape: (97200, 198000)\n",
      "[INFO] Dataset resolution: (0.0002777777777777776, 0.0002777777777777776)\n",
      "[INFO] Dataset transform: | 0.00, 0.00,-119.00|\n",
      "| 0.00,-0.00, 49.00|\n",
      "| 0.00, 0.00, 1.00|\n",
      "[INFO] Missing data value: -32768.0\n",
      "[INFO] Data type: ('int16',)\n"
     ]
    }
   ],
   "source": [
    "# Print basic information about the merged GeoTIFF file\n",
    "with rasterio.open(config.merged_dem_path) as merged_src:\n",
    "    logging.info(f\"Dataset CRS: {merged_src.crs}\")\n",
    "    logging.info(f\"Dataset bounds: {merged_src.bounds}\")\n",
    "    logging.info(f\"Dataset shape: {merged_src.shape}\")\n",
    "    logging.info(f\"Dataset resolution: {merged_src.res}\")\n",
    "    logging.info(f\"Dataset transform: {merged_src.transform}\")\n",
    "    logging.info(f\"Missing data value: {merged_src.nodata}\")\n",
    "    logging.info(f\"Data type: {merged_src.dtypes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show merged file as elevation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping display of merged DEM data. Set `show_plots` to True to display.\n"
     ]
    }
   ],
   "source": [
    "# Load the image and run imshow\n",
    "with rasterio.open(config.merged_dem_path) as src:\n",
    "        downsample_stride = 100\n",
    "        dem_data = src.read(1,\n",
    "            out_shape=(\n",
    "                src.count,\n",
    "                int(src.height / downsample_stride),\n",
    "                int(src.width / downsample_stride)\n",
    "            ),\n",
    "        resampling=rasterio.enums.Resampling.nearest\n",
    "    )\n",
    "if config.show_plots:\n",
    "    \n",
    "    # Calculate slope\n",
    "    x, y = np.gradient(dem_data, src.res[0], src.res[1])\n",
    "    slope = np.sqrt(x**2 + y**2)\n",
    "    log_slope = np.log10(slope + 1)  # Adding 1 to avoid log(0)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # Plot elevation\n",
    "    im1 = axes[0].imshow(dem_data, cmap='terrain', extent=(bounds[0], bounds[2], bounds[1], bounds[3]), vmin=0)\n",
    "    axes[0].set_title('Merged DEM Data')\n",
    "    axes[0].set_xlabel('Longitude')\n",
    "    axes[0].set_ylabel('Latitude')\n",
    "    cbar1 = fig.colorbar(im1, ax=axes[0], orientation='vertical', label='Elevation (meters)')\n",
    "    \n",
    "    # Plot log10 slope\n",
    "    im2 = axes[1].imshow(log_slope, cmap='viridis', extent=(bounds[0], bounds[2], bounds[1], bounds[3]))\n",
    "    axes[1].set_title('Log10 Slope')\n",
    "    axes[1].set_xlabel('Longitude')\n",
    "    axes[1].set_ylabel('Latitude')\n",
    "    cbar2 = fig.colorbar(im2, ax=axes[1], orientation='vertical', label='Log10 Slope')\n",
    "    \n",
    "    # Set the ticks to match the bounds\n",
    "    for ax in axes:\n",
    "        ax.set_xticks(np.linspace(bounds[0], bounds[2], num=3))\n",
    "        ax.set_yticks(np.linspace(bounds[1], bounds[3], num=3))\n",
    "        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1f}'))\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1f}'))\n",
    "        ax.grid(True, linestyle='--', alpha=0.8, color='k')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.info(\"Skipping display of merged DEM data. Set `show_plots` to True to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32767"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Join elevation data with land cover data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DEM images for 13145 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6412f29124324f0f9878720333077a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting DEM images:   0%|          | 0/13145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Number of images dropped due to nodata proportion exceeding threshold: 0 / 13145\n",
      "[INFO] Number of images with interpolation of missing values: 36 / 13145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DEM images for 650 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0851ab353c491e929c17ad38e29cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting DEM images:   0%|          | 0/650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Number of images dropped due to nodata proportion exceeding threshold: 0 / 650\n",
      "[INFO] Number of images with interpolation of missing values: 0 / 650\n",
      "/tmp/ipykernel_321072/3713914763.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  train_dem_images = train_dem_images / train_dem_images.max(axis=(1, 2), keepdims=True)\n",
      "/tmp/ipykernel_321072/3713914763.py:61: RuntimeWarning: invalid value encountered in cast\n",
      "  train_dem_images = (train_dem_images * 255).astype(np.uint8)\n"
     ]
    }
   ],
   "source": [
    "def extract_dem_images(gdf: gpd.GeoDataFrame, dem_src: rasterio.DatasetReader) -> np.ndarray[np.float32]:\n",
    "    dem_images = []\n",
    "    nodata_count = 0\n",
    "    interpolate_count = 0\n",
    "    print(f\"Extracting DEM images for {len(gdf)} samples\")\n",
    "    for _, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extracting DEM images\"):\n",
    "        \n",
    "        window = rasterio.windows.from_bounds(*row.geometry.bounds, transform=dem_src.transform) \n",
    "        \n",
    "        dem_data = dem_src.read(1, window=window)\n",
    "\n",
    "        # If the data is NaN or it's above the threshold, we set it to NaN\n",
    "        is_nodata = np.logical_or(dem_data == dem_src.nodata, dem_data>config.dem_elev_max)\n",
    "        nodata_fraction = np.mean(is_nodata)\n",
    "        dem_data = dem_data.astype(np.float32)\n",
    "\n",
    "        # If the read failed, the shape will be empty so we raise an alarm\n",
    "        # If any failure cases occur, we want the resulting DEM array to be all NaNs\n",
    "        # and have all dims with nonzero size.\n",
    "        if len(dem_data.shape) == 0:\n",
    "            dem_data  = np.empty((config.image_size, config.image_size)) * np.nan\n",
    "            logging.debug(f\"Failed to read window for row {row} with window {window}\")\n",
    "            nodata_count += 1\n",
    "        elif any([dim == 0 for dim in dem_data.shape]):\n",
    "            dem_data = np.empty((config.image_size, config.image_size)) * np.nan\n",
    "            logging.debug(f\"Window read for row {row} with bbox {row.geometry.bounds} has a zero dimension with shape {dem_data.shape}\")\n",
    "            nodata_count += 1\n",
    "        elif nodata_fraction > config.dem_nodata_threshold:            \n",
    "            dem_data *= np.nan\n",
    "            nodata_count += 1\n",
    "        elif np.any(is_nodata):\n",
    "            # Interpolate NaN values using a spatially informed method\n",
    "            dem_data = cv2.inpaint(dem_data, is_nodata.astype(np.uint8), inpaintRadius=3, flags=cv2.INPAINT_TELEA)\n",
    "            interpolate_count += 1\n",
    "\n",
    "        assert np.all(np.isnan(dem_data)) or np.nanmax(dem_data) < config.dem_elev_max, f\"DEM data contains values above nodata threshold: {dem_data.max()}\"\n",
    "\n",
    "        # Resize using cv2 to the desired image size\n",
    "        if not np.any(np.isnan(dem_data)):\n",
    "            dem_data = cv2.resize(dem_data, (config.image_size, config.image_size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        dem_images.append(dem_data)\n",
    "\n",
    "    logging.info(f\"Number of images dropped due to nodata proportion exceeding threshold: {nodata_count} / {len(gdf)}\")\n",
    "    logging.info(f\"Number of images with interpolation of missing values: {interpolate_count} / {len(gdf)}\")\n",
    "    return np.array(dem_images).astype(np.float32)\n",
    "\n",
    "with rasterio.open(config.merged_dem_path) as dem_src:\n",
    "    train_dem_images = extract_dem_images(train_gdf, dem_src)\n",
    "    test_dem_images = extract_dem_images(test_gdf, dem_src)\n",
    "\n",
    "# Offset all images to have a minimum of zero\n",
    "train_dem_images -= train_dem_images.min(axis=(1, 2), keepdims=True)\n",
    "test_dem_images -= test_dem_images.min(axis=(1, 2), keepdims=True)\n",
    "\n",
    "train_dem_images = train_dem_images / train_dem_images.max(axis=(1, 2), keepdims=True)\n",
    "test_dem_images  = test_dem_images / test_dem_images.max(axis=(1, 2), keepdims=True)\n",
    "\n",
    "\n",
    "# Case to uint8\n",
    "train_dem_images = (train_dem_images * 255).astype(np.uint8)\n",
    "test_dem_images = (test_dem_images * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of DEM values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([339258., 122238.,  88313., 136603.,  92362., 143417.,  97893.,\n",
       "        155545., 101983., 161272., 165100., 110732., 169996., 114820.,\n",
       "        179791., 118749., 179145., 119683., 185379., 122409., 193247.,\n",
       "        187919., 127699., 192850., 137451., 189819., 131954., 200153.,\n",
       "        134669., 200952., 203693., 136856., 198652., 149885., 205766.,\n",
       "        137058., 204840., 140285., 204971., 134339., 210568., 206249.,\n",
       "        141785., 204343., 139098., 205318., 133961., 201176., 134338.,\n",
       "        220428., 193501., 129855., 192512., 131207., 194327., 130912.,\n",
       "        195288., 123661., 185836., 121150., 188568., 179535., 120229.,\n",
       "        177524., 113232., 164628., 119335., 159904., 104776., 157308.,\n",
       "        149155., 104420., 143556.,  94954., 141854.,  88598., 129974.,\n",
       "         86139., 119605.,  77220., 115180., 108704.,  67276.,  98996.,\n",
       "         62535.,  89997.,  55695.,  78966.,  49212.,  68149.,  60548.,\n",
       "         36337.,  48455.,  30388.,  39943.,  21266.,  26788.,  14140.,\n",
       "         15443.,  38689.]),\n",
       " array([  0.  ,   2.55,   5.1 ,   7.65,  10.2 ,  12.75,  15.3 ,  17.85,\n",
       "         20.4 ,  22.95,  25.5 ,  28.05,  30.6 ,  33.15,  35.7 ,  38.25,\n",
       "         40.8 ,  43.35,  45.9 ,  48.45,  51.  ,  53.55,  56.1 ,  58.65,\n",
       "         61.2 ,  63.75,  66.3 ,  68.85,  71.4 ,  73.95,  76.5 ,  79.05,\n",
       "         81.6 ,  84.15,  86.7 ,  89.25,  91.8 ,  94.35,  96.9 ,  99.45,\n",
       "        102.  , 104.55, 107.1 , 109.65, 112.2 , 114.75, 117.3 , 119.85,\n",
       "        122.4 , 124.95, 127.5 , 130.05, 132.6 , 135.15, 137.7 , 140.25,\n",
       "        142.8 , 145.35, 147.9 , 150.45, 153.  , 155.55, 158.1 , 160.65,\n",
       "        163.2 , 165.75, 168.3 , 170.85, 173.4 , 175.95, 178.5 , 181.05,\n",
       "        183.6 , 186.15, 188.7 , 191.25, 193.8 , 196.35, 198.9 , 201.45,\n",
       "        204.  , 206.55, 209.1 , 211.65, 214.2 , 216.75, 219.3 , 221.85,\n",
       "        224.4 , 226.95, 229.5 , 232.05, 234.6 , 237.15, 239.7 , 242.25,\n",
       "        244.8 , 247.35, 249.9 , 252.45, 255.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_dem_images.flatten(), bins=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show DEM images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping DEM image visualization; set show_plots to True to display images.\n"
     ]
    }
   ],
   "source": [
    "# Plot several train and test images using elevation colormap\n",
    "if config.show_plots:\n",
    "    n_images = 3\n",
    "    fig, ax = plt.subplots(2, n_images, figsize=(8, 5))\n",
    "\n",
    "    # Pick random sample of train and test images to show\n",
    "    sampled_train_indices = np.random.choice(len(train_dem_images), n_images, replace=False)\n",
    "    sampled_test_indices = np.random.choice(len(test_dem_images), n_images, replace=False)\n",
    "\n",
    "    for i in range(n_images):\n",
    "        for j, (images, gdf, title, sampled_indices) in enumerate(zip(\n",
    "                [train_dem_images, test_dem_images], \n",
    "                [train_gdf, test_gdf], \n",
    "                [\"Train DEM\", \"Test DEM\"],\n",
    "                [sampled_train_indices, sampled_test_indices])):\n",
    "            \n",
    "            im = ax[j, i].imshow(images[sampled_indices[i]], cmap='terrain')\n",
    "            ax[j, i].set_title(f\"{title} Image {i+1}\")\n",
    "            ax[j, i].axis('off')\n",
    "            centroid = gdf.iloc[sampled_indices[i]].geometry.centroid\n",
    "            ax[j, i].text(1.5, 3, f\"{centroid.y:.3f}, {centroid.x:.3f}\",color='black', \n",
    "                          bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "            # Add gridlines and lat/long overlay\n",
    "            ax[j, i].grid(True, linestyle='--', alpha=0.8, color='k')\n",
    "            ax[j, i].set_xticks(np.linspace(0, config.image_size, num=3))\n",
    "            ax[j, i].set_yticks(np.linspace(0, config.image_size, num=3))\n",
    "            ax[j, i].set_xticklabels(np.linspace(centroid.x - config.image_size // 2, centroid.x + config.image_size // 2, num=3).round(2))\n",
    "            ax[j, i].set_yticklabels(np.linspace(centroid.y - config.image_size // 2, centroid.y + config.image_size // 2, num=3).round(2))\n",
    "            \n",
    "    # Add a common colorbar on the right-hand side\n",
    "    cbar_ax = fig.add_axes([1.0, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax, orientation='vertical', label='Elevation (meters)')\n",
    "\n",
    "    plt.tight_layout(rect=[0.1, 0, 1, 1])\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.info(\"Skipping DEM image visualization; set show_plots to True to display images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping visualization of NLCD and DEM data; set show_plots=True to enable.\n"
     ]
    }
   ],
   "source": [
    "# Create visualization of NLCD and DEM data\n",
    "n_images = 3\n",
    "seen_classes = set()\n",
    "if config.show_plots:\n",
    "    fig, axes = plt.subplots(2, n_images, figsize=(n_images*2.5, 8))\n",
    "\n",
    "    # Pick random sample of train images to show\n",
    "    \n",
    "    sampled_indices = np.random.choice(len(train_images), n_images, replace=False)\n",
    "\n",
    "    # Plot first n_images from training set\n",
    "    for i, sample_idx in enumerate(sampled_indices):\n",
    "\n",
    "        # Get DEM data for this image\n",
    "        dem = train_dem_images[sample_idx]\n",
    "        dem_min = dem.min()\n",
    "        dem_relative = dem - dem_min\n",
    "        \n",
    "        # Calculate contours (relative to minimum elevation)\n",
    "        levels = np.linspace(0, dem_relative.max(), 10)\n",
    "        \n",
    "        # Plot NLCD with contours\n",
    "        axes[0, i].imshow(lut[train_images[sample_idx]])\n",
    "        cs = axes[0, i].contour(dem_relative, levels=levels, colors='k', alpha=0.7, linewidths=0.5)\n",
    "        axes[0, i].clabel(cs, inline=True, fontsize=8, fmt='%.0f')\n",
    "        axes[0, i].set_title(f'Training Image {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Add lat/lon labels to image\n",
    "        centroid = train_gdf.iloc[sample_idx].geometry.centroid\n",
    "        axes[0, i].text(1.5, 3, f\"{centroid.y:.3f}, {centroid.x:.3f}\", color='black',\n",
    "                        bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "        axes[0, i].grid(True, linestyle='--', alpha=0.8, color='k')\n",
    "\n",
    "        # Plot DEM\n",
    "        im = axes[1, i].imshow(dem, cmap='terrain')\n",
    "        axes[1, i].set_title(f'Elevation Image {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "        seen_classes.update(np.unique(train_images[sample_idx]))\n",
    "        \n",
    "    # Add colorbar for elevation below the subplots\n",
    "    cbar_ax = fig.add_axes([0.15, 0.12, 0.7, 0.02])\n",
    "    fig.colorbar(im, cax=cbar_ax, orientation='horizontal', label='Elevation, relative to minimum (m)')\n",
    "\n",
    "    # Add legend for NLCD classes above the subplots\n",
    "    legend_handles = [mpatches.Patch(color=classes_df.loc[idx, \"RGB\"], \n",
    "                                     label=classes_df.loc[idx, \"name\"]) \n",
    "                      for idx in seen_classes]\n",
    "    fig.legend(handles=legend_handles, loc='upper center', \n",
    "               bbox_to_anchor=(0.5, 0.99), ncol=4)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.95])\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.info(\"Skipping visualization of NLCD and DEM data; set show_plots=True to enable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Concatenate data and save to disk + s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data:\n",
    "    is_image_bad_train = np.any(np.isnan(train_dem_images), axis=(1, 2))\n",
    "    is_image_kept_train = ~is_image_bad_train\n",
    "\n",
    "    train_gdf_final = train_gdf[is_image_kept_train]\n",
    "    logging.info(f\"Removed {is_image_bad_train.sum()} images with missing DEM data from training set.\")\n",
    "\n",
    "    is_image_bad_test = np.any(np.isnan(test_dem_images), axis=(1, 2))\n",
    "    is_image_kept_test = ~is_image_bad_test\n",
    "    test_gdf_final = test_gdf[is_image_kept_test]\n",
    "    logging.info(f\"Removed {is_image_bad_test.sum()} images with missing DEM data from test set.\")\n",
    "\n",
    "    train_gdf_final.to_crs('EPSG:4326', inplace=True)\n",
    "    test_gdf_final.to_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "    # Take arrays of shape (N, H, W) and stack them along the channel axis\n",
    "    # which needs to be created for both data sets\n",
    "    train_combined = np.stack([train_images[is_image_kept_train], train_dem_images[is_image_kept_train]], axis=1)\n",
    "    test_combined = np.stack([test_images[is_image_kept_test],  test_dem_images[is_image_kept_test]], axis=1)\n",
    "\n",
    "    train_gdf_path = config.output_path_train_gpkg\n",
    "    train_gdf_final.to_file(train_gdf_path, driver='GPKG')\n",
    "    train_gpkg_size = os.path.getsize(train_gdf_path)\n",
    "    logging.info(f\"Training sample location GeoDataFrame saved to {train_gdf_path} (Size: {train_gpkg_size / (1024 * 1024):.2f} MB)\")\n",
    "\n",
    "    test_gdf_path = config.output_path_test_gpkg\n",
    "    test_gdf_final.to_file(test_gdf_path, driver='GPKG')\n",
    "    test_gpkg_size = os.path.getsize(test_gdf_path)\n",
    "    logging.info(f\"Test sample location GeoDataFrame saved to {test_gdf_path} (Size: {test_gpkg_size / (1024 * 1024):.2f} MB)\")\n",
    "\n",
    "    # Save the lat-long coordinates of the training and test samples\n",
    "    train_coords = np.stack(\n",
    "        [train_gdf_final.centroid.x.values,\n",
    "        train_gdf_final.centroid.y.values,],\n",
    "    axis=1)\n",
    "\n",
    "    test_coords = np.stack(\n",
    "        [test_gdf_final.centroid.x.values,\n",
    "        test_gdf_final.centroid.y.values],\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "    np.savez_compressed(config.output_path, train_data=train_combined, test_data=test_combined, train_coords=train_coords, test_coords=test_coords)\n",
    "\n",
    "    if config.upload_to_s3:\n",
    "        import boto3\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket_name = BUCKET_NAME\n",
    "        files_to_upload = [\n",
    "            config.output_path,\n",
    "            train_gdf_path,\n",
    "            test_gdf_path, \n",
    "        ]\n",
    "\n",
    "        for file in files_to_upload:\n",
    "            logging.info(f\"Uploading {file} to S3 bucket {bucket_name}...\")\n",
    "            s3.upload_file(str(file), bucket_name, file.name)\n",
    "            logging.info(f\"Uploaded {file} to S3 bucket {bucket_name} as {file.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show large, random sample of LULC images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lulc_sample = False\n",
    "\n",
    "if plot_lulc_sample:\n",
    "    fig, axes = plt.subplots(10, 16, figsize=(48, 30))\n",
    "    axes = axes.flatten()\n",
    "    for i in range(16*10):\n",
    "        ax = axes[i]\n",
    "        random_index = np.random.randint(0, train_combined.shape[0])\n",
    "        image = train_combined[random_index, 0].astype(np.uint8)\n",
    "        ax.imshow(lut[image])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Create animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'train_images_final' in locals():\n",
    "    train_images_final = np.load(config.output_path)['train_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_animation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9446/965541235.py:31: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  plt.subplots_adjust(left=0.02, right=0.98, bottom=0.02, top=0.98)\n",
      "/tmp/ipykernel_9446/965541235.py:93: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  plt.subplots_adjust(hspace=-0.6, wspace=-0.2)  # Increased overlap between subplots\n",
      "[INFO] Animation.save using <class 'matplotlib.animation.PillowWriter'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving frame 359/360\r"
     ]
    }
   ],
   "source": [
    "if make_animation:\n",
    "    class TerrainAnimator:\n",
    "        def __init__(self, train_images, train_dem_images, lut, n_rows=4, n_cols=8):\n",
    "            self.train_images = train_images\n",
    "            self.train_dem_images = train_dem_images\n",
    "            self.lut = lut\n",
    "            self.n_rows = n_rows\n",
    "            self.n_cols = n_cols\n",
    "            self.exaggeration = 1\n",
    "            \n",
    "            # Pre-calculate mesh grid\n",
    "            self.h, self.w = train_images[0].shape\n",
    "            x = np.arange(self.w)\n",
    "            y = np.arange(self.h)\n",
    "            self.X, self.Y = np.meshgrid(x, y)\n",
    "            \n",
    "            # Initialize figure\n",
    "            self.setup_figure()\n",
    "            \n",
    "        def setup_figure(self):\n",
    "            plt.rcParams['figure.dpi'] = 300\n",
    "            self.fig, self.axes = plt.subplots(\n",
    "                self.n_rows, \n",
    "                self.n_cols, \n",
    "                figsize=(self.n_cols*1.4, self.n_rows*1.6),  # Reduced figure size\n",
    "                subplot_kw={'projection': '3d'},\n",
    "                constrained_layout=True  # Use constrained layout\n",
    "            )\n",
    "            self.fig.set_facecolor('black')\n",
    "            self.fig.patch.set_alpha(1.0)\n",
    "            # Reduce margins\n",
    "            plt.subplots_adjust(left=0.02, right=0.98, bottom=0.02, top=0.98)\n",
    "            \n",
    "            # Select random indices once\n",
    "            self.indices = np.random.choice(\n",
    "                len(self.train_images), \n",
    "                self.n_rows * self.n_cols, \n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "        def process_elevation(self, elevation):\n",
    "            \"\"\"Pre-process elevation data with Gaussian smoothing\"\"\"\n",
    "            return cv2.GaussianBlur(elevation, (3, 3), 0)\n",
    "            \n",
    "        def create_surface(self, ax, idx):\n",
    "            \"\"\"Create a single surface plot\"\"\"\n",
    "            land_cover = self.train_images[idx]\n",
    "            elevation = self.process_elevation(self.train_dem_images[idx])\n",
    "            \n",
    "            surf = ax.plot_surface(\n",
    "                self.X, self.Y,\n",
    "                elevation * self.exaggeration,\n",
    "                facecolors=self.lut[land_cover],\n",
    "                shade=False,\n",
    "                antialiased=False,\n",
    "                rstride=1,\n",
    "                cstride=1\n",
    "            )\n",
    "\n",
    "            ax.set_facecolor('black')\n",
    "            \n",
    "            # Configure view\n",
    "            ax.view_init(elev=30, azim=45)\n",
    "            ax.set_box_aspect([1, 1, 0.5])\n",
    "            \n",
    "            # Remove unnecessary elements\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_zticks([])\n",
    "            ax.grid(False)\n",
    "            ax.axis('off')\n",
    "\n",
    "            ele_max = elevation.max()\n",
    "\n",
    "            if ele_max < 100:\n",
    "                zlim = 150\n",
    "            elif ele_max < 200:\n",
    "                zlim = 250\n",
    "            else:\n",
    "                zlim = max(300, ele_max * 3)\n",
    "\n",
    "            ax.set_zlim(0, zlim)\n",
    "            \n",
    "            return surf\n",
    "            \n",
    "        def setup_plots(self):\n",
    "            \"\"\"Initialize all surface plots in parallel\"\"\"\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                self.surfaces = list(executor.map(\n",
    "                    lambda args: self.create_surface(*args),\n",
    "                    zip(self.axes.flatten(), self.indices)\n",
    "                ))\n",
    "            \n",
    "            plt.subplots_adjust(hspace=-0.6, wspace=-0.2)  # Increased overlap between subplots\n",
    "            \n",
    "        def update(self, frame):\n",
    "            \"\"\"Animation update function\"\"\"\n",
    "            for ax in self.axes.flatten():\n",
    "                ax.view_init(elev=30, azim=frame)\n",
    "            return self.surfaces\n",
    "            \n",
    "        def create_animation(self, frames=360, fps=30, out_path=config.output_path_animation):\n",
    "            \"\"\"Create and save the animation\"\"\"\n",
    "            self.setup_plots()\n",
    "            \n",
    "            anim = FuncAnimation(\n",
    "                self.fig,\n",
    "                self.update,\n",
    "                frames=frames,\n",
    "                interval=1000/fps,\n",
    "                blit=True\n",
    "            )\n",
    "            \n",
    "            # Save with optimized settings\n",
    "            anim.save(\n",
    "                out_path,\n",
    "                writer='pillow',\n",
    "                fps=fps,\n",
    "                savefig_kwargs={'facecolor': 'black'},\n",
    "                progress_callback=lambda i, n: print(f'Saving frame {i}/{n}', end='\\r')\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    # Usage\n",
    "    train_images_final[:,0]\n",
    "    animator = TerrainAnimator(\n",
    "        train_images_final[:,0], train_images_final[:,1], lut,\n",
    "        n_rows=4, n_cols=4\n",
    "    )\n",
    "    animator.create_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"terrain_rotation.gif\" width=\"1500\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                     Type               Data/Info\n",
      "---------------------------------------------------------\n",
      "BUCKET_NAME                  str                lc-inpaint\n",
      "CRS                          type               <class 'pyproj.crs.crs.CRS'>\n",
      "DataPrepConfig               type               <class '__main__.DataPrepConfig'>\n",
      "FuncAnimation                type               <class 'matplotlib.animation.FuncAnimation'>\n",
      "HTML                         type               <class 'IPython.core.display.HTML'>\n",
      "Path                         type               <class 'pathlib.Path'>\n",
      "TerrainAnimator              type               <class '__main__.TerrainAnimator'>\n",
      "ThreadPoolExecutor           type               <class 'concurrent.future<...>read.ThreadPoolExecutor'>\n",
      "Transformer                  type               <class 'pyproj.transformer.Transformer'>\n",
      "Tuple                        _TupleType         typing.Tuple\n",
      "animator                     TerrainAnimator    <__main__.TerrainAnimator<...>object at 0x74801ef431c0>\n",
      "available_memory             int                55705923584\n",
      "ax                           Axes               Axes(0.937751,0.005;0.0590625x0.0945)\n",
      "axes                         ndarray            160: 160 elems, type `object`, 1280 bytes\n",
      "bbox_bottom                  float              184067.27745031676\n",
      "bbox_height                  int                103318\n",
      "bbox_left                    float              -2405613.2719740258\n",
      "bbox_right                   float              2326232.831032607\n",
      "bbox_top                     float              3283614.798755772\n",
      "bbox_width                   int                157728\n",
      "block_counts                 dict               n=17\n",
      "block_height                 int                103318\n",
      "boto3                        module             <module 'boto3' from '/mn<...>kages/boto3/__init__.py'>\n",
      "bounds                       BoundingBox        BoundingBox(left=-2493045<...>2342655.0, top=3310005.0)\n",
      "box                          function           <function box at 0x748051f5be20>\n",
      "bucket_name                  str                lc-inpaint\n",
      "check_overlap                function           <function check_overlap at 0x74801a7abd00>\n",
      "class_mapping                dict               n=16\n",
      "classes_df                   DataFrame                 class_value       <...>  (0.439, 0.639, 0.729)  \n",
      "cls_id                       int                95\n",
      "col_end                      int                160642\n",
      "col_start                    int                2914\n",
      "compute_block_counts         function           <function compute_block_counts at 0x748099009b40>\n",
      "compute_class_distribution   function           <function compute_class_d<...>bution at 0x74809900a680>\n",
      "config                       DataPrepConfig     DataPrepConfig(bbox_west=<...>430.0, upload_to_s3=True)\n",
      "current_path                 PosixPath          /mnt/m2ssd/data/Dropbox/research/nlcd-inpaint\n",
      "cv2                          module             <module 'cv2' from '/mnt/<...>ackages/cv2/__init__.py'>\n",
      "data                         ndarray            103318x157728: 16296141504 elems, type `uint8`, 16296141504 bytes (15541.211608886719 Mb)\n",
      "data_crs                     CRS                PROJCS[\"Albers_Conical_Eq<...>],AXIS[\"Northing\",NORTH]]\n",
      "dataclass                    function           <function dataclass at 0x7480b55501f0>\n",
      "dem_data                     ndarray            972x1980: 1924560 elems, type `int16`, 3849120 bytes (3.670806884765625 Mb)\n",
      "dem_src                      DatasetReader      <closed DatasetReader nam<...>_conus_dem.tif' mode='r'>\n",
      "display                      function           <function display at 0x7480b4db7eb0>\n",
      "downsample_patch             function           <function downsample_patch at 0x74809900a950>\n",
      "downsample_stride            int                100\n",
      "ds_bottom                    float              21.742307778016638\n",
      "ds_left                      float              -119.78610533604079\n",
      "ds_right                     float              -63.672191850655295\n",
      "ds_top                       float              49.177063191389536\n",
      "dtype_size                   int                1\n",
      "elevation                    module             <module 'elevation' from <...>s/elevation/__init__.py'>\n",
      "extract_dem_images           function           <function extract_dem_images at 0x74808456f1c0>\n",
      "fig                          Figure             Figure(4800x3000)\n",
      "file                         PosixPath          /mnt/m2ssd/data/Dropbox/r<...>-inpaint/data/test_8.gpkg\n",
      "files_to_upload              list               n=3\n",
      "from_working_crs             Transformer        proj=pipeline step proj=u<...>5 x_0=0 y_0=0 ellps=WGS84\n",
      "get_pixel_bounds             function           <function get_pixel_bounds at 0x74801a7ab1c0>\n",
      "glob                         module             <module 'glob' from '/usr<...>/lib/python3.10/glob.py'>\n",
      "gpd                          module             <module 'geopandas' from <...>s/geopandas/__init__.py'>\n",
      "grid_cells                   list               n=2500\n",
      "grid_gdf                     GeoDataFrame                                <...>\\n[2500 rows x 3 columns]\n",
      "i                            int                159\n",
      "image                        ndarray            8x8: 64 elems, type `uint8`, 64 bytes\n",
      "is_image_bad_test            ndarray            6539: 6539 elems, type `bool`, 6539 bytes\n",
      "is_image_bad_train           ndarray            132353: 132353 elems, type `bool`, 132353 bytes (129.2509765625 kb)\n",
      "is_image_kept_test           ndarray            6539: 6539 elems, type `bool`, 6539 bytes\n",
      "is_image_kept_train          ndarray            132353: 132353 elems, type `bool`, 132353 bytes (129.2509765625 kb)\n",
      "is_within_bbox               function           <function is_within_bbox at 0x74801a7abb50>\n",
      "j                            int                49\n",
      "k                            uint8              95\n",
      "load_dotenv                  function           <function load_dotenv at 0x74801ba479a0>\n",
      "logging                      module             <module 'logging' from '/<...>.10/logging/__init__.py'>\n",
      "lut                          ndarray            17x3: 51 elems, type `float64`, 408 bytes\n",
      "max_elements                 int                27852961792\n",
      "merge                        module             <module 'rasterio.merge' <...>kages/rasterio/merge.py'>\n",
      "merged_src                   DatasetReader      <closed DatasetReader nam<...>_conus_dem.tif' mode='r'>\n",
      "mode                         function           <function mode at 0x74801bc0af80>\n",
      "mp                           module             <module 'multiprocessing'<...>iprocessing/__init__.py'>\n",
      "mpatches                     module             <module 'matplotlib.patch<...>s/matplotlib/patches.py'>\n",
      "n_blocks                     int                1\n",
      "n_cells                      int                2500\n",
      "n_images                     int                3\n",
      "n_test                       int                125\n",
      "np                           module             <module 'numpy' from '/mn<...>kages/numpy/__init__.py'>\n",
      "os                           module             <module 'os' from '/usr/lib/python3.10/os.py'>\n",
      "palette_series               Series             class\\n0      (0.278, 0.4<...>nName: RGB, dtype: object\n",
      "partial                      type               <class 'functools.partial'>\n",
      "pct                          float64            0.018897852712065462\n",
      "pd                           module             <module 'pandas' from '/m<...>ages/pandas/__init__.py'>\n",
      "plt                          module             <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "polygon                      dict               n=2\n",
      "present_classes              list               n=16\n",
      "process_cell                 function           <function process_cell at 0x74801a7abac0>\n",
      "psutil                       module             <module 'psutil' from '/m<...>ages/psutil/__init__.py'>\n",
      "qmc                          module             <module 'scipy.stats.qmc'<...>ages/scipy/stats/qmc.py'>\n",
      "random_index                 int                121555\n",
      "rasterio                     module             <module 'rasterio' from '<...>es/rasterio/__init__.py'>\n",
      "reverse_mapping              dict               n=16\n",
      "row_end                      int                104197\n",
      "row_start                    int                879\n",
      "s3                           S3                 <botocore.client.S3 object at 0x748081f3fcd0>\n",
      "sample_images_parallel       function           <function sample_images_p<...>rallel at 0x74801a7aba30>\n",
      "samples_x                    float              2518.59375\n",
      "samples_y                    float              1631.625\n",
      "seen_classes                 set                set()\n",
      "sobol_indices                ndarray            125: 125 elems, type `int64`, 1000 bytes\n",
      "sobol_points                 ndarray            125x1: 125 elems, type `float64`, 1000 bytes\n",
      "src                          DatasetReader      <closed DatasetReader nam<...>_conus_dem.tif' mode='r'>\n",
      "test_combined                ndarray            6539x2x8x8: 836992 elems, type `uint8`, 836992 bytes (817.375 kb)\n",
      "test_coords                  ndarray            6539x2: 13078 elems, type `float64`, 104624 bytes (102.171875 kb)\n",
      "test_dem_images              ndarray            6539x8x8: 418496 elems, type `uint8`, 418496 bytes (408.6875 kb)\n",
      "test_dist                    dict               n=15\n",
      "test_gdf                     GeoDataFrame                                <...>\\n[6539 rows x 1 columns]\n",
      "test_gdf_final               GeoDataFrame                                <...>\\n[6539 rows x 1 columns]\n",
      "test_gdf_path                PosixPath          /mnt/m2ssd/data/Dropbox/r<...>-inpaint/data/test_8.gpkg\n",
      "test_gpkg_size               int                72495104\n",
      "test_images                  ndarray            6539x8x8: 418496 elems, type `uint8`, 418496 bytes (408.6875 kb)\n",
      "to_working_crs               Transformer        proj=pipeline step inv pr<...>vert xy_in=rad xy_out=deg\n",
      "total_counts                 dict               n=16\n",
      "total_pixels                 int                16296141504\n",
      "tqdm                         type               <class 'tqdm.notebook.tqdm_notebook'>\n",
      "train_combined               ndarray            132353x2x8x8: 16941184 elems, type `uint8`, 16941184 bytes (16.1563720703125 Mb)\n",
      "train_coords                 ndarray            132353x2: 264706 elems, type `float64`, 2117648 bytes (2.0195465087890625 Mb)\n",
      "train_dem_images             ndarray            132353x8x8: 8470592 elems, type `uint8`, 8470592 bytes (8.07818603515625 Mb)\n",
      "train_dist                   dict               n=16\n",
      "train_gdf                    GeoDataFrame                                <...>[132353 rows x 1 columns]\n",
      "train_gdf_final              GeoDataFrame                                <...>[132353 rows x 1 columns]\n",
      "train_gdf_path               PosixPath          /mnt/m2ssd/data/Dropbox/r<...>inpaint/data/train_8.gpkg\n",
      "train_gpkg_size              int                1454284800\n",
      "train_images                 ndarray            132353x8x8: 8470592 elems, type `uint8`, 8470592 bytes (8.07818603515625 Mb)\n",
      "train_images_final           ndarray            132353x2x8x8: 16941184 elems, type `uint8`, 16941184 bytes (16.1563720703125 Mb)\n",
      "v                            int64              142573444\n",
      "window                       Window             Window(col_off=2914, row_<...>th=157728, height=103318)\n",
      "working_crs                  CRS                EPSG:4326\n",
      "x_edges                      ndarray            51: 51 elems, type `float64`, 408 bytes\n",
      "y_edges                      ndarray            51: 51 elems, type `float64`, 408 bytes\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 12. Base Footprint Extraction\n",
    "\n",
    "Extract NLCD data around base borders with configurable buffer radius.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
